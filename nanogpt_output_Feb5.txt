Using GPUs: [0]
Using OpenAI API with model gpt-4o-2024-08-06.

Generating idea 1/3
Iteration 1/3
{'Name': 'sparse_attention', 'Title': 'Sparse Attention: Enhancing Transformer Efficiency with Reduced Complexity', 'Experiment': 'Implement a sparse attention mechanism within the CausalSelfAttention class. Modify the attention calculation to only attend to a subset of tokens, such as using a fixed or learned sparsity pattern. Evaluate the training time and model performance compared to the baseline dense attention. The experiment will involve modifying the forward method of the CausalSelfAttention class and possibly introducing additional parameters to control sparsity.', 'Interestingness': 7, 'Feasibility': 5, 'Novelty': 5}
Iteration 2/3
{'Name': 'sparse_attention', 'Title': 'Sparse Attention: Enhancing Transformer Efficiency with Reduced Complexity', 'Experiment': 'Implement a sparse attention mechanism within the CausalSelfAttention class using fixed sparsity patterns, such as local attention. Modify the attention calculation to attend only to a limited number of tokens within a local window. Evaluate the training time and model performance compared to the baseline dense attention. The experiment will involve modifying the forward method of the CausalSelfAttention class to incorporate sparsity.', 'Interestingness': 7, 'Feasibility': 6, 'Novelty': 5}
Idea generation converged after 2 iterations.

Generating idea 2/3
Iteration 1/3
{'Name': 'adaptive_weight_initialization', 'Title': 'Adaptive Weight Initialization: Enhancing Convergence in Transformer Models', 'Experiment': 'Modify the _init_weights function in the GPT class to incorporate a recalibration mechanism that adjusts initialization parameters based on training progression metrics, such as loss or gradient norms. Evaluate the impact on convergence speed and model performance compared to the baseline with static initialization.', 'Interestingness': 7, 'Feasibility': 6, 'Novelty': 5}
Iteration 2/3
{'Name': 'adaptive_weight_initialization', 'Title': 'Adaptive Weight Initialization: Enhancing Convergence in Transformer Models', 'Experiment': 'Modify the _init_weights function in the GPT class to include a recalibration mechanism that uses a moving average of gradient norms to adjust initialization parameters at regular intervals. Evaluate the impact on convergence speed, stability, and overall model performance compared to the baseline with static initialization.', 'Interestingness': 7, 'Feasibility': 6, 'Novelty': 5}
Iteration 3/3
{'Name': 'adaptive_weight_initialization', 'Title': 'Adaptive Weight Initialization: Enhancing Convergence in Transformer Models', 'Experiment': 'Modify the _init_weights function in the GPT class to include a recalibration mechanism that uses a moving average of gradient norms to adjust initialization parameters at regular intervals. Evaluate the impact on convergence speed, stability, and overall model performance compared to the baseline with static initialization.', 'Interestingness': 7, 'Feasibility': 6, 'Novelty': 5}
Idea generation converged after 3 iterations.

Generating idea 3/3
Iteration 1/3
{'Name': 'saliency_based_attention', 'Title': 'Saliency-Based Attention: Dynamic Prioritization of Token Importance in Transformer Models', 'Experiment': 'Implement a saliency-based mechanism within the CausalSelfAttention class. Modify the forward method to calculate saliency scores for tokens and use these scores to modulate attention weights. Saliency can be computed as a function of token embeddings and context. Evaluate the impact on training efficiency and model performance compared to baseline attention.', 'Interestingness': 8, 'Feasibility': 5, 'Novelty': 7}
Iteration 2/3
{'Name': 'saliency_based_attention', 'Title': 'Saliency-Based Attention: Dynamic Prioritization of Token Importance in Transformer Models', 'Experiment': 'Implement a saliency-based mechanism within the CausalSelfAttention class. Modify the forward method to calculate saliency scores for tokens and use these scores to modulate attention weights. Saliency can be computed as a function of token embeddings and context. Evaluate the impact on training efficiency and model performance compared to baseline attention.', 'Interestingness': 8, 'Feasibility': 5, 'Novelty': 7}
Idea generation converged after 2 iterations.

Checking novelty of idea 0: adaptive_block_size
Starting novelty iteration 1 for idea adaptive_block_size
LLM response at iteration 1: THOUGHT:
The idea of dynamically adjusting the block size during training to improve efficiency and learning of long-range dependencies is intriguing. I need to investigate whether there have been previous works on adaptive block sizes or context window adjustments in language models. Searching for ...
Response Status Code: 200
Response Content: {"total": 392, "offset": 0, "next": 10, "data": [{"paperId": "d33825b65cc34b3004e67bcb8bdff780eab4b3e4", "title": "An Adaptive Mini-Block Fisher Method for Deep Neural Networks", "abstract": "Deep neural networks (DNNs) are currently predominantly trained using \ufb01rst-order methods. Some of these methods (e.g., Adam, AdaGrad, and RMSprop, and their variants) incorporate a small amount of curvature information by using a diagonal matrix to precondition the stochastic gradient. Recently, effect
Starting novelty iteration 2 for idea adaptive_block_size
LLM response at iteration 2: THOUGHT:
The search results do not directly address the concept of dynamically adjusting block sizes in neural networks for efficient training and improved long-range dependency learning. The closest related work seems to be on adaptive methods and block-based approaches, but none specifically discu...
Response Status Code: 200
Response Content: {"total": 10839, "offset": 0, "next": 10, "data": [{"paperId": "d4b99821ab8c1ee3271a72dc4163feb8d310c8a0", "title": "DBPS: Dynamic Block Size and Precision Scaling for Efficient DNN Training Supported by RISC-V ISA Extensions", "abstract": "Over the past decade, it has been found that deep neural networks (DNNs) perform better on visual perception and language understanding tasks as their size increases. However, this comes at the cost of high energy consumption and large memory requirement to t
Starting novelty iteration 3 for idea adaptive_block_size
LLM response at iteration 3: THOUGHT:
The results feature a relevant paper titled "DBPS: Dynamic Block Size and Precision Scaling for Efficient DNN Training," which proposes dynamic block size adjustments for efficient DNN training. This overlaps with the idea of dynamically adjusting block size during training for efficiency a...
Decision made: not novel after round 2

Checking novelty of idea 1: layerwise_learning_rates
Starting novelty iteration 1 for idea layerwise_learning_rates
LLM response at iteration 1: THOUGHT:
The idea of using layer-wise learning rates in training neural networks, especially transformer models, is not entirely new. However, the specific implementation and analysis of its impact on training dynamics, convergence speed, and final performance might have some novelty. To assess this...
Response Status Code: 200
Response Content: {"total": 1623, "offset": 0, "next": 10, "data": [{"paperId": "e09bfc955fbf66e0a042ca3f921108b823093b2e", "title": "Layer-Wise Learning Rate Optimization for Task-Dependent Fine-Tuning of Pre-Trained Models: An Evolutionary Approach", "abstract": "The superior performance of large-scale pre-trained models, such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT), has received increasing attention in both academic and industrial research 
Starting novelty iteration 2 for idea layerwise_learning_rates
LLM response at iteration 2: THOUGHT:
The search results indicate that there is substantial existing research on layer-wise learning rate adaptation, particularly in the context of fine-tuning pre-trained models, as seen in papers like "Layer-Wise Learning Rate Optimization for Task-Dependent Fine-Tuning." There are also studie...
Decision made: not novel after round 1

Checking novelty of idea 2: sparse_attention
Starting novelty iteration 1 for idea sparse_attention
LLM response at iteration 1: THOUGHT:
The idea involves implementing a sparse attention mechanism within the CausalSelfAttention class to enhance transformer efficiency. This concept is about altering the attention mechanism to focus on a limited number of tokens, which could potentially reduce computational complexity. There h...
Response Status Code: 200
Response Content: {"total": 44321, "offset": 0, "next": 10, "data": [{"paperId": "224d2385eace559649f293756f9934f1750b3055", "title": "Aquaculture Water Quality Classification with Sparse Attention Transformers: Leveraging Water and Environmental Parameters", "abstract": "For aquaculture operations to be successful, water quality is essential. Maintaining a healthy aquaculture environment depends on the correct and timely evaluation of water quality based on both water parameters and environmental variables. Usin
Starting novelty iteration 2 for idea sparse_attention
LLM response at iteration 2: THOUGHT:
The results reveal several papers exploring sparse attention mechanisms in transformers, addressing computational efficiency and complexity issues. Notably, "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers" introduces a sparse attention mechanism w...
Response Status Code: 200
Response Content: {"total": 550, "offset": 0, "next": 10, "data": [{"paperId": "87bcf3e1e47b341d57ed6478e7ff6e2ff9955a80", "title": "16.2 A 28nm 53.8TOPS/W 8b Sparse Transformer Accelerator with In-Memory Butterfly Zero Skipper for Unstructured-Pruned NN and CIM-Based Local-Attention-Reusable Engine", "abstract": "Transformer networks, from BERT, GPT to Alphafold, have demonstrated unprecedented advances in a variety of AI tasks. Fig. 16.2.1 shows the computing flow of self-attention - the fundamental operation i
Starting novelty iteration 3 for idea sparse_attention
LLM response at iteration 3: THOUGHT:
The search results further highlight various implementations of local and sparse attention mechanisms in transformers, such as "MSWA: Refining Local Attention with Multi-Scale Window Attention" and "16.2 A 28nm 53.8TOPS/W 8b Sparse Transformer Accelerator with In-Memory Butterfly Zero Skipp...
Decision made: not novel after round 2

Checking novelty of idea 3: adaptive_weight_initialization
Starting novelty iteration 1 for idea adaptive_weight_initialization
LLM response at iteration 1: THOUGHT:
The idea involves modifying the weight initialization process in transformer models by incorporating a recalibration mechanism based on gradient norms. This concept aims to improve convergence speed, stability, and model performance. To evaluate the novelty, I'll search for existing literat...
Response Status Code: 200
Response Content: {"total": 7083, "offset": 0, "next": 10, "data": [{"paperId": "ceee7ddc5b1f2322b4f7eab185526d98ab37eabc", "title": "Research on Assembly Scheduling of Complex Electromechanical Products Based on Improved Particle Swarm Optimization", "abstract": "The assembly shop production scheduling of complex electromechanical products is a typical discrete variable NP-hard problem. In order to solve the assembly job scheduling problem of a large transformer assembly shop, an improved particle swarm algorith
Starting novelty iteration 2 for idea adaptive_weight_initialization
LLM response at iteration 2: THOUGHT:
The search results from the previous query did not yield any directly relevant papers specifically focusing on adaptive weight initialization for transformers using a recalibration mechanism based on gradient norms. There were mentions of adaptive methods and weight initialization in other ...
Response Status Code: 200
Response Content: {"total": 21877, "offset": 0, "next": 10, "data": [{"paperId": "f191a63c89900955e7db6c9f8d5c89c6e2e4f06f", "title": "A mathematical framework for improved weight initialization of neural networks using Lagrange multipliers", "abstract": null, "venue": "Neural Networks", "year": 2023, "citationCount": 7, "citationStyles": {"bibtex": "@Article{Pater2023AMF,\n author = {I. Pater and M. Mitici},\n booktitle = {Neural Networks},\n journal = {Neural networks : the official journal of the International
Starting novelty iteration 3 for idea adaptive_weight_initialization
LLM response at iteration 3: THOUGHT:
The search results from the latest query also did not reveal any papers specifically focused on adaptive or dynamic weight initialization techniques for transformers using a recalibration mechanism based on gradient norms. While there are papers on weight initialization strategies and impro...
Decision made: novel after round 2

Checking novelty of idea 4: saliency_based_attention
Starting novelty iteration 1 for idea saliency_based_attention
LLM response at iteration 1: THOUGHT:
The idea is to develop a saliency-based mechanism to dynamically prioritize token importance in transformer models, specifically by modifying the CausalSelfAttention class. This approach involves calculating saliency scores for tokens and using them to modulate attention weights, thereby po...
Response Status Code: 200
Response Content: {"total": 487, "offset": 0, "next": 10, "data": [{"paperId": "eda6bcd39531f113a95d070ba198aa78ec017055", "title": "Transformer-based multi-level attention integration network for video saliency prediction", "abstract": null, "venue": "Multimedia tools and applications", "year": 2024, "citationCount": 1, "citationStyles": {"bibtex": "@Article{Tan2024TransformerbasedMA,\n author = {Rui Tan and Minghui Sun and Yanhua Liang},\n booktitle = {Multimedia tools and applications},\n journal = {Multimedia
Starting novelty iteration 2 for idea saliency_based_attention
LLM response at iteration 2: THOUGHT:
The list of results from the previous query primarily includes applications of transformers in various domains, such as video saliency prediction, time-series forecasting, and skin lesion segmentation, among others. None of these directly address the idea of integrating saliency scores into...
Response Status Code: 200
Response Content: {"total": 684, "offset": 0, "next": 10, "data": [{"paperId": "eda6bcd39531f113a95d070ba198aa78ec017055", "title": "Transformer-based multi-level attention integration network for video saliency prediction", "abstract": null, "venue": "Multimedia tools and applications", "year": 2024, "citationCount": 1, "citationStyles": {"bibtex": "@Article{Tan2024TransformerbasedMA,\n author = {Rui Tan and Minghui Sun and Yanhua Liang},\n booktitle = {Multimedia tools and applications},\n journal = {Multimedia
Starting novelty iteration 3 for idea saliency_based_attention
LLM response at iteration 3: THOUGHT:
The results of the last query do not provide any direct evidence of existing research focusing on integrating saliency scores into transformer attention for token prioritization. The papers mostly discuss conventional applications of transformers or attention mechanisms in different fields,...
Decision made: novel after round 2
Processing idea: adaptive_weight_initialization
2025-02-05 17:25:33
*Starting idea: 20250205_172532_adaptive_weight_initialization*
2025-02-05 17:25:35
*Starting Experiments*
To implement the adaptive weight initialization, we will modify the _init_weights function in the GPT class to include a
recalibration mechanism using a moving average of gradient norms. We will conduct a series of experiments to evaluate   
the impact of this change on convergence speed, stability, and overall model performance. Here's the plan for the       
experiments:                                                                                                            

 1 Run 1: Implement the adaptive weight initialization with a moving average window size of 10.                         
 2 Run 2: Use a moving average window size of 20 to see if a larger window size impacts performance.                    
 3 Run 3: Test with a smaller moving average window size of 5 to evaluate the sensitivity to window size.               
 4 Run 4: Implement a variant where the recalibration is applied less frequently, e.g., every 50 iterations.            
 5 Run 5: Implement a variant where the recalibration is applied more frequently, e.g., every 10 iterations.            

Let's start by implementing the changes for Run 1. We will modify the _init_weights function to include the adaptive    
mechanism with a moving average window size of 10.                                                                      

Here are the changes needed in experiment.py:                                                                           

 1 Add a mechanism to track the moving average of gradient norms.                                                       
 2 Adjust the weight initialization based on the moving average.                                                        

Let's implement these changes.                                                                                          

Tokens: 11k sent, 282 received. Cost: $0.03 message, $0.03 session.
To implement the adaptive weight initialization, we will modify the `_init_weights` function in the `GPT` class to include a recalibration mechanism using a moving average of gradient norms. We will conduct a series of experiments to evaluate the impact of this change on convergence speed, stability, and overall model performance. Here's the plan for the experiments:

1. **Run 1**: Implement the adaptive weight initialization with a moving average window size of 10.
2. **Run 2**: Use a moving average window size of 20 to see if a larger window size impacts performance.
3. **Run 3**: Test with a smaller moving average window size of 5 to evaluate the sensitivity to window size.
4. **Run 4**: Implement a variant where the recalibration is applied less frequently, e.g., every 50 iterations.
5. **Run 5**: Implement a variant where the recalibration is applied more frequently, e.g., every 10 iterations.

Let's start by implementing the changes for **Run 1**. We will modify the `_init_weights` function to include the adaptive mechanism with a moving average window size of 10. 

Here are the changes needed in `experiment.py`:

1. Add a mechanism to track the moving average of gradient norms.
2. Adjust the weight initialization based on the moving average.

Let's implement these changes.
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2874, val loss 4.2823
iter 0: loss 4.2654, time 45954.88ms
iter 10: loss 3.2458, time 108.79ms
iter 20: loss 2.7914, time 71.59ms
iter 30: loss 2.6355, time 71.96ms
iter 40: loss 2.5778, time 76.55ms
iter 50: loss 2.5275, time 67.93ms
iter 60: loss 2.5197, time 79.81ms
iter 70: loss 2.4956, time 84.49ms
iter 80: loss 2.4975, time 90.50ms
iter 90: loss 2.4681, time 85.51ms
iter 100: loss 2.4588, time 75.80ms
iter 110: loss 2.4571, time 75.98ms
iter 120: loss 2.4263, time 71.96ms
iter 130: loss 2.4225, time 76.24ms
iter 140: loss 2.4291, time 66.74ms
iter 150: loss 2.4223, time 72.58ms
iter 160: loss 2.3798, time 81.27ms
iter 170: loss 2.3557, time 76.15ms
iter 180: loss 2.3172, time 79.26ms
iter 190: loss 2.2513, time 75.78ms
iter 200: loss 2.2064, time 75.78ms
iter 210: loss 2.1524, time 80.77ms
iter 220: loss 2.1378, time 80.41ms
iter 230: loss 2.0695, time 71.16ms
iter 240: loss 2.0820, time 68.31ms
step 250: train loss 1.9693, val loss 2.0672
iter 250: loss 2.0337, time 21771.11ms
iter 260: loss 1.9789, time 73.12ms
iter 270: loss 1.9843, time 72.74ms
iter 280: loss 1.9815, time 79.13ms
iter 290: loss 1.9208, time 73.70ms
iter 300: loss 1.9052, time 73.09ms
iter 310: loss 1.8725, time 93.84ms
iter 320: loss 1.8559, time 79.88ms
iter 330: loss 1.8241, time 76.74ms
iter 340: loss 1.7936, time 76.01ms
iter 350: loss 1.8231, time 79.29ms
iter 360: loss 1.7729, time 77.32ms
iter 370: loss 1.7437, time 82.98ms
iter 380: loss 1.7338, time 76.21ms
iter 390: loss 1.7407, time 77.28ms
iter 400: loss 1.7710, time 77.49ms
iter 410: loss 1.7013, time 73.03ms
iter 420: loss 1.7190, time 61.77ms
iter 430: loss 1.6885, time 75.01ms
iter 440: loss 1.6594, time 72.07ms
iter 450: loss 1.6571, time 71.45ms
iter 460: loss 1.6054, time 75.67ms
iter 470: loss 1.6515, time 70.28ms
iter 480: loss 1.6284, time 74.48ms
iter 490: loss 1.6023, time 78.09ms
step 500: train loss 1.5262, val loss 1.7262
iter 500: loss 1.5998, time 21754.76ms
iter 510: loss 1.6164, time 64.02ms
iter 520: loss 1.5965, time 71.67ms
iter 530: loss 1.5566, time 76.53ms
iter 540: loss 1.6244, time 66.69ms
iter 550: loss 1.5617, time 80.02ms
iter 560: loss 1.5681, time 79.56ms
iter 570: loss 1.5693, time 78.21ms
iter 580: loss 1.5358, time 68.72ms
iter 590: loss 1.5032, time 95.33ms
iter 600: loss 1.5106, time 71.34ms
iter 610: loss 1.5514, time 74.10ms
iter 620: loss 1.5393, time 70.34ms
iter 630: loss 1.5102, time 70.04ms
iter 640: loss 1.4668, time 69.71ms
iter 650: loss 1.5062, time 70.85ms
iter 660: loss 1.5162, time 71.15ms
iter 670: loss 1.4487, time 72.30ms
iter 680: loss 1.5197, time 86.77ms
iter 690: loss 1.4679, time 75.33ms
iter 700: loss 1.4915, time 65.94ms
iter 710: loss 1.4614, time 59.97ms
iter 720: loss 1.4492, time 74.16ms
iter 730: loss 1.4235, time 70.29ms
iter 740: loss 1.4350, time 69.33ms
step 750: train loss 1.3656, val loss 1.5890
iter 750: loss 1.4314, time 21842.99ms
iter 760: loss 1.4481, time 65.50ms
iter 770: loss 1.4324, time 73.89ms
iter 780: loss 1.4207, time 76.21ms
iter 790: loss 1.4254, time 75.00ms
iter 800: loss 1.4400, time 78.99ms
iter 810: loss 1.4165, time 72.80ms
iter 820: loss 1.4085, time 77.77ms
iter 830: loss 1.3984, time 81.99ms
iter 840: loss 1.4062, time 78.21ms
iter 850: loss 1.3973, time 70.12ms
iter 860: loss 1.4010, time 69.25ms
iter 870: loss 1.3988, time 75.82ms
iter 880: loss 1.3816, time 75.50ms
iter 890: loss 1.3929, time 76.67ms
iter 900: loss 1.3791, time 74.70ms
iter 910: loss 1.3242, time 74.61ms
iter 920: loss 1.3675, time 86.75ms
iter 930: loss 1.3573, time 75.22ms
iter 940: loss 1.3485, time 77.32ms
iter 950: loss 1.3597, time 77.44ms
iter 960: loss 1.3672, time 73.73ms
iter 970: loss 1.3597, time 78.12ms
iter 980: loss 1.3556, time 73.45ms
iter 990: loss 1.3454, time 75.94ms
step 1000: train loss 1.2786, val loss 1.5237
iter 1000: loss 1.3462, time 21459.86ms
iter 1010: loss 1.3420, time 77.15ms
iter 1020: loss 1.3256, time 83.09ms
iter 1030: loss 1.3456, time 71.15ms
iter 1040: loss 1.3586, time 74.12ms
iter 1050: loss 1.2992, time 72.34ms
iter 1060: loss 1.3394, time 74.36ms
iter 1070: loss 1.3352, time 67.94ms
iter 1080: loss 1.3440, time 64.66ms
iter 1090: loss 1.3599, time 63.65ms
iter 1100: loss 1.3243, time 72.87ms
iter 1110: loss 1.3057, time 70.29ms
iter 1120: loss 1.3039, time 69.75ms
iter 1130: loss 1.2978, time 75.10ms
iter 1140: loss 1.2967, time 70.00ms
iter 1150: loss 1.3118, time 81.82ms
iter 1160: loss 1.3309, time 69.39ms
iter 1170: loss 1.3093, time 73.59ms
iter 1180: loss 1.3286, time 79.08ms
iter 1190: loss 1.2694, time 65.50ms
iter 1200: loss 1.2978, time 68.22ms
iter 1210: loss 1.2673, time 72.93ms
iter 1220: loss 1.3133, time 70.08ms
iter 1230: loss 1.3052, time 69.20ms
iter 1240: loss 1.3063, time 72.02ms
step 1250: train loss 1.2094, val loss 1.4922
iter 1250: loss 1.2817, time 21760.65ms
iter 1260: loss 1.2810, time 67.51ms
iter 1270: loss 1.2690, time 74.63ms
iter 1280: loss 1.2615, time 77.53ms
iter 1290: loss 1.2856, time 77.04ms
iter 1300: loss 1.3085, time 76.49ms
iter 1310: loss 1.2458, time 73.91ms
iter 1320: loss 1.3058, time 70.41ms
iter 1330: loss 1.2722, time 77.93ms
iter 1340: loss 1.3012, time 72.03ms
iter 1350: loss 1.2565, time 73.83ms
iter 1360: loss 1.2783, time 82.42ms
iter 1370: loss 1.2587, time 69.59ms
iter 1380: loss 1.2690, time 72.70ms
iter 1390: loss 1.2557, time 69.13ms
iter 1400: loss 1.2543, time 71.89ms
iter 1410: loss 1.2537, time 67.48ms
iter 1420: loss 1.2732, time 77.15ms
iter 1430: loss 1.2472, time 69.51ms
iter 1440: loss 1.2623, time 59.49ms
iter 1450: loss 1.2364, time 57.83ms
iter 1460: loss 1.2442, time 74.86ms
iter 1470: loss 1.2285, time 73.98ms
iter 1480: loss 1.2201, time 71.96ms
iter 1490: loss 1.2413, time 74.31ms
step 1500: train loss 1.1578, val loss 1.4812
iter 1500: loss 1.1907, time 21519.08ms
iter 1510: loss 1.2372, time 72.54ms
iter 1520: loss 1.2253, time 75.45ms
iter 1530: loss 1.2541, time 71.62ms
iter 1540: loss 1.1932, time 68.01ms
iter 1550: loss 1.2350, time 81.27ms
iter 1560: loss 1.2128, time 71.60ms
iter 1570: loss 1.2394, time 73.01ms
iter 1580: loss 1.2100, time 69.72ms
iter 1590: loss 1.1992, time 75.38ms
iter 1600: loss 1.2002, time 72.81ms
iter 1610: loss 1.2413, time 85.12ms
iter 1620: loss 1.1898, time 77.27ms
iter 1630: loss 1.2059, time 72.21ms
iter 1640: loss 1.2062, time 79.54ms
iter 1650: loss 1.1852, time 73.42ms
iter 1660: loss 1.2190, time 72.86ms
iter 1670: loss 1.2007, time 74.08ms
iter 1680: loss 1.2005, time 80.18ms
iter 1690: loss 1.2035, time 70.34ms
iter 1700: loss 1.1879, time 66.02ms
iter 1710: loss 1.1805, time 72.41ms
iter 1720: loss 1.1867, time 67.35ms
iter 1730: loss 1.2062, time 72.46ms
iter 1740: loss 1.1797, time 70.57ms
step 1750: train loss 1.1065, val loss 1.4689
iter 1750: loss 1.1890, time 21648.89ms
iter 1760: loss 1.1903, time 75.41ms
iter 1770: loss 1.2009, time 77.32ms
iter 1780: loss 1.1999, time 73.97ms
iter 1790: loss 1.2010, time 82.33ms
iter 1800: loss 1.1830, time 73.96ms
iter 1810: loss 1.1524, time 73.78ms
iter 1820: loss 1.1763, time 74.25ms
iter 1830: loss 1.1735, time 75.85ms
iter 1840: loss 1.1628, time 76.56ms
iter 1850: loss 1.1601, time 77.53ms
iter 1860: loss 1.1824, time 79.58ms
iter 1870: loss 1.1417, time 70.76ms
iter 1880: loss 1.1809, time 75.56ms
iter 1890: loss 1.1814, time 69.69ms
iter 1900: loss 1.1380, time 71.43ms
iter 1910: loss 1.1731, time 75.13ms
iter 1920: loss 1.1721, time 70.00ms
iter 1930: loss 1.1500, time 75.59ms
iter 1940: loss 1.1314, time 71.83ms
iter 1950: loss 1.1377, time 61.23ms
iter 1960: loss 1.1580, time 74.09ms
iter 1970: loss 1.1533, time 72.15ms
iter 1980: loss 1.1563, time 77.79ms
iter 1990: loss 1.1584, time 71.40ms
step 2000: train loss 1.0614, val loss 1.4776
iter 2000: loss 1.1288, time 21888.04ms
iter 2010: loss 1.1310, time 74.59ms
iter 2020: loss 1.1243, time 75.84ms
iter 2030: loss 1.1595, time 74.42ms
iter 2040: loss 1.1496, time 70.30ms
iter 2050: loss 1.1143, time 75.00ms
iter 2060: loss 1.1105, time 71.68ms
iter 2070: loss 1.1253, time 58.82ms
iter 2080: loss 1.1221, time 57.52ms
iter 2090: loss 1.1386, time 71.46ms
iter 2100: loss 1.1352, time 65.77ms
iter 2110: loss 1.1351, time 71.41ms
iter 2120: loss 1.1395, time 73.33ms
iter 2130: loss 1.1426, time 67.01ms
iter 2140: loss 1.1464, time 68.29ms
iter 2150: loss 1.1312, time 72.71ms
iter 2160: loss 1.1511, time 69.57ms
iter 2170: loss 1.1370, time 65.79ms
iter 2180: loss 1.1193, time 66.52ms
iter 2190: loss 1.1122, time 65.27ms
iter 2200: loss 1.1321, time 84.81ms
iter 2210: loss 1.1244, time 72.64ms
iter 2220: loss 1.1284, time 77.49ms
iter 2230: loss 1.1185, time 71.85ms
iter 2240: loss 1.1316, time 71.62ms
step 2250: train loss 1.0113, val loss 1.4855
iter 2250: loss 1.1107, time 21218.84ms
iter 2260: loss 1.1136, time 74.81ms
iter 2270: loss 1.1375, time 83.27ms
iter 2280: loss 1.0965, time 78.03ms
iter 2290: loss 1.1452, time 74.43ms
iter 2300: loss 1.1278, time 70.06ms
iter 2310: loss 1.1044, time 77.89ms
iter 2320: loss 1.0920, time 71.52ms
iter 2330: loss 1.1022, time 71.36ms
iter 2340: loss 1.1172, time 76.83ms
iter 2350: loss 1.1099, time 70.92ms
iter 2360: loss 1.1162, time 70.04ms
iter 2370: loss 1.0926, time 73.69ms
iter 2380: loss 1.0810, time 70.09ms
iter 2390: loss 1.0808, time 68.11ms
iter 2400: loss 1.0804, time 73.92ms
iter 2410: loss 1.0783, time 73.44ms
iter 2420: loss 1.0783, time 70.45ms
iter 2430: loss 1.0615, time 72.79ms
iter 2440: loss 1.0605, time 71.81ms
iter 2450: loss 1.0761, time 69.24ms
iter 2460: loss 1.0874, time 73.99ms
iter 2470: loss 1.0945, time 79.17ms
iter 2480: loss 1.0868, time 75.19ms
iter 2490: loss 1.0556, time 70.38ms
step 2500: train loss 0.9645, val loss 1.4934
iter 2500: loss 1.0851, time 21438.47ms
iter 2510: loss 1.0717, time 76.49ms
iter 2520: loss 1.0492, time 74.49ms
iter 2530: loss 1.0550, time 70.41ms
iter 2540: loss 1.0633, time 75.39ms
iter 2550: loss 1.0699, time 75.83ms
iter 2560: loss 1.0645, time 74.71ms
iter 2570: loss 1.0749, time 71.91ms
iter 2580: loss 1.0833, time 83.22ms
iter 2590: loss 1.0706, time 74.04ms
iter 2600: loss 1.0744, time 69.81ms
iter 2610: loss 1.0517, time 71.42ms
iter 2620: loss 1.0468, time 86.80ms
iter 2630: loss 1.0285, time 76.36ms
iter 2640: loss 1.0442, time 76.03ms
iter 2650: loss 1.0645, time 72.18ms
iter 2660: loss 1.0442, time 76.97ms
iter 2670: loss 1.0202, time 78.73ms
iter 2680: loss 1.0475, time 75.32ms
iter 2690: loss 1.0548, time 78.26ms
iter 2700: loss 1.0247, time 71.75ms
iter 2710: loss 1.0495, time 72.27ms
iter 2720: loss 1.0505, time 72.78ms
iter 2730: loss 1.0661, time 73.40ms
iter 2740: loss 1.0238, time 81.23ms
step 2750: train loss 0.9190, val loss 1.5150
iter 2750: loss 1.0394, time 21452.73ms
iter 2760: loss 1.0374, time 74.29ms
iter 2770: loss 1.0199, time 74.88ms
iter 2780: loss 1.0242, time 66.12ms
iter 2790: loss 1.0424, time 73.79ms
iter 2800: loss 1.0177, time 73.92ms
iter 2810: loss 1.0431, time 72.64ms
iter 2820: loss 1.0250, time 68.43ms
iter 2830: loss 1.0369, time 72.11ms
iter 2840: loss 0.9977, time 75.80ms
iter 2850: loss 1.0334, time 72.88ms
iter 2860: loss 1.0268, time 76.54ms
iter 2870: loss 1.0091, time 69.64ms
iter 2880: loss 1.0432, time 69.64ms
iter 2890: loss 1.0142, time 72.00ms
iter 2900: loss 0.9949, time 72.83ms
iter 2910: loss 1.0490, time 65.92ms
iter 2920: loss 1.0152, time 65.49ms
iter 2930: loss 1.0015, time 69.95ms
iter 2940: loss 0.9935, time 74.98ms
iter 2950: loss 1.0221, time 80.05ms
iter 2960: loss 1.0041, time 81.54ms
iter 2970: loss 0.9923, time 76.34ms
iter 2980: loss 0.9970, time 67.59ms
iter 2990: loss 0.9862, time 72.78ms
step 3000: train loss 0.8704, val loss 1.5282
iter 3000: loss 0.9891, time 21245.62ms
iter 3010: loss 0.9993, time 75.14ms
iter 3020: loss 1.0010, time 78.01ms
iter 3030: loss 1.0031, time 77.31ms
iter 3040: loss 1.0225, time 72.42ms
iter 3050: loss 0.9825, time 74.33ms
iter 3060: loss 0.9989, time 73.39ms
iter 3070: loss 1.0300, time 72.55ms
iter 3080: loss 0.9949, time 76.17ms
iter 3090: loss 0.9934, time 69.37ms
iter 3100: loss 1.0020, time 73.04ms
iter 3110: loss 0.9822, time 68.37ms
iter 3120: loss 1.0031, time 78.17ms
iter 3130: loss 0.9737, time 67.39ms
iter 3140: loss 0.9855, time 74.89ms
iter 3150: loss 1.0040, time 71.67ms
iter 3160: loss 1.0136, time 71.48ms
iter 3170: loss 0.9600, time 80.80ms
iter 3180: loss 0.9710, time 75.35ms
iter 3190: loss 0.9968, time 68.13ms
iter 3200: loss 0.9690, time 76.00ms
iter 3210: loss 0.9627, time 80.03ms
iter 3220: loss 0.9645, time 71.19ms
iter 3230: loss 0.9642, time 75.22ms
iter 3240: loss 0.9609, time 77.75ms
step 3250: train loss 0.8289, val loss 1.5533
iter 3250: loss 0.9699, time 21586.80ms
iter 3260: loss 0.9581, time 72.65ms
iter 3270: loss 0.9808, time 75.54ms
iter 3280: loss 0.9639, time 76.37ms
iter 3290: loss 0.9493, time 68.65ms
iter 3300: loss 0.9410, time 76.94ms
iter 3310: loss 0.9608, time 71.59ms
iter 3320: loss 0.9702, time 72.92ms
iter 3330: loss 0.9648, time 69.61ms
iter 3340: loss 0.9639, time 70.94ms
iter 3350: loss 0.9607, time 74.80ms
iter 3360: loss 0.9399, time 71.86ms
iter 3370: loss 0.9616, time 70.96ms
iter 3380: loss 0.9534, time 79.31ms
iter 3390: loss 0.9572, time 79.59ms
iter 3400: loss 0.9542, time 73.51ms
iter 3410: loss 0.9451, time 74.19ms
iter 3420: loss 0.9488, time 70.72ms
iter 3430: loss 0.9612, time 71.13ms
iter 3440: loss 0.9723, time 78.47ms
iter 3450: loss 0.9638, time 73.21ms
iter 3460: loss 0.9563, time 75.20ms
iter 3470: loss 0.9443, time 84.51ms
iter 3480: loss 0.9553, time 70.41ms
iter 3490: loss 0.9221, time 72.85ms
step 3500: train loss 0.7899, val loss 1.5708
iter 3500: loss 0.9111, time 21159.72ms
iter 3510: loss 0.9156, time 70.57ms
iter 3520: loss 0.9312, time 78.53ms
iter 3530: loss 0.9620, time 78.61ms
iter 3540: loss 0.9366, time 73.37ms
iter 3550: loss 0.9241, time 78.88ms
iter 3560: loss 0.9664, time 78.46ms
iter 3570: loss 0.9407, time 70.35ms
iter 3580: loss 0.9248, time 74.77ms
iter 3590: loss 0.9261, time 78.36ms
iter 3600: loss 0.9195, time 80.35ms
iter 3610: loss 0.9135, time 72.36ms
iter 3620: loss 0.9184, time 75.38ms
iter 3630: loss 0.9284, time 73.36ms
iter 3640: loss 0.9212, time 72.74ms
iter 3650: loss 0.9131, time 70.72ms
iter 3660: loss 0.9449, time 75.04ms
iter 3670: loss 0.9442, time 69.85ms
iter 3680: loss 0.9120, time 69.48ms
iter 3690: loss 0.9396, time 69.35ms
iter 3700: loss 0.8767, time 72.02ms
iter 3710: loss 0.8857, time 65.27ms
iter 3720: loss 0.9059, time 70.98ms
iter 3730: loss 0.9112, time 68.08ms
iter 3740: loss 0.9102, time 72.17ms
step 3750: train loss 0.7469, val loss 1.6059
iter 3750: loss 0.9032, time 21497.47ms
iter 3760: loss 0.9386, time 76.76ms
iter 3770: loss 0.9389, time 69.44ms
iter 3780: loss 0.9359, time 81.50ms
iter 3790: loss 0.9039, time 68.43ms
iter 3800: loss 0.9219, time 72.10ms
iter 3810: loss 0.9296, time 77.38ms
iter 3820: loss 0.8920, time 71.30ms
iter 3830: loss 0.9041, time 69.62ms
iter 3840: loss 0.8917, time 69.13ms
iter 3850: loss 0.8942, time 72.32ms
iter 3860: loss 0.8758, time 82.01ms
iter 3870: loss 0.8962, time 70.42ms
iter 3880: loss 0.9018, time 72.03ms
iter 3890: loss 0.8980, time 72.72ms
iter 3900: loss 0.8976, time 72.13ms
iter 3910: loss 0.8931, time 90.74ms
iter 3920: loss 0.8771, time 70.36ms
iter 3930: loss 0.8928, time 71.73ms
iter 3940: loss 0.8845, time 74.41ms
iter 3950: loss 0.8842, time 78.13ms
iter 3960: loss 0.9102, time 70.92ms
iter 3970: loss 0.8951, time 70.07ms
iter 3980: loss 0.9126, time 70.78ms
iter 3990: loss 0.8819, time 66.49ms
step 4000: train loss 0.7146, val loss 1.6206
iter 4000: loss 0.8647, time 21344.49ms
iter 4010: loss 0.8866, time 66.51ms
iter 4020: loss 0.8986, time 74.09ms
iter 4030: loss 0.8893, time 80.79ms
iter 4040: loss 0.8833, time 71.54ms
iter 4050: loss 0.8760, time 85.04ms
iter 4060: loss 0.8618, time 69.46ms
iter 4070: loss 0.8665, time 79.48ms
iter 4080: loss 0.8981, time 80.78ms
iter 4090: loss 0.8507, time 77.66ms
iter 4100: loss 0.9115, time 67.04ms
iter 4110: loss 0.8815, time 71.98ms
iter 4120: loss 0.8784, time 68.25ms
iter 4130: loss 0.8693, time 78.75ms
iter 4140: loss 0.8840, time 75.68ms
iter 4150: loss 0.8711, time 74.38ms
iter 4160: loss 0.8627, time 76.30ms
iter 4170: loss 0.8826, time 72.99ms
iter 4180: loss 0.8726, time 79.25ms
iter 4190: loss 0.8766, time 64.69ms
iter 4200: loss 0.8541, time 70.46ms
iter 4210: loss 0.8808, time 71.20ms
iter 4220: loss 0.8632, time 74.76ms
iter 4230: loss 0.8829, time 70.42ms
iter 4240: loss 0.8644, time 70.51ms
step 4250: train loss 0.6842, val loss 1.6461
iter 4250: loss 0.8696, time 21497.41ms
iter 4260: loss 0.8710, time 80.81ms
iter 4270: loss 0.8728, time 66.02ms
iter 4280: loss 0.8550, time 72.47ms
iter 4290: loss 0.8379, time 73.94ms
iter 4300: loss 0.8346, time 74.92ms
iter 4310: loss 0.8636, time 69.56ms
iter 4320: loss 0.8543, time 73.58ms
iter 4330: loss 0.8599, time 67.90ms
iter 4340: loss 0.8351, time 67.46ms
iter 4350: loss 0.8451, time 61.07ms
iter 4360: loss 0.8614, time 71.69ms
iter 4370: loss 0.8574, time 72.23ms
iter 4380: loss 0.8428, time 71.06ms
iter 4390: loss 0.8727, time 69.66ms
iter 4400: loss 0.8495, time 74.66ms
iter 4410: loss 0.8723, time 72.19ms
iter 4420: loss 0.8698, time 68.09ms
iter 4430: loss 0.8499, time 70.36ms
iter 4440: loss 0.8527, time 71.57ms
iter 4450: loss 0.8547, time 78.55ms
iter 4460: loss 0.8394, time 67.40ms
iter 4470: loss 0.8623, time 71.44ms
iter 4480: loss 0.8394, time 83.66ms
iter 4490: loss 0.8508, time 69.37ms
step 4500: train loss 0.6594, val loss 1.6713
iter 4500: loss 0.8640, time 21579.30ms
iter 4510: loss 0.8535, time 68.23ms
iter 4520: loss 0.8420, time 75.63ms
iter 4530: loss 0.8603, time 74.55ms
iter 4540: loss 0.8547, time 88.06ms
iter 4550: loss 0.8793, time 82.27ms
iter 4560: loss 0.8473, time 74.53ms
iter 4570: loss 0.8529, time 79.72ms
iter 4580: loss 0.8643, time 64.43ms
iter 4590: loss 0.8546, time 75.53ms
iter 4600: loss 0.8295, time 66.94ms
iter 4610: loss 0.8717, time 67.10ms
iter 4620: loss 0.8411, time 78.35ms
iter 4630: loss 0.8243, time 75.58ms
iter 4640: loss 0.8543, time 75.22ms
iter 4650: loss 0.8697, time 63.69ms
iter 4660: loss 0.8549, time 71.86ms
iter 4670: loss 0.8441, time 73.86ms
iter 4680: loss 0.8621, time 72.13ms
iter 4690: loss 0.8524, time 80.75ms
iter 4700: loss 0.8349, time 74.66ms
iter 4710: loss 0.8031, time 69.15ms
iter 4720: loss 0.8320, time 72.51ms
iter 4730: loss 0.8229, time 76.14ms
iter 4740: loss 0.8339, time 76.53ms
step 4750: train loss 0.6427, val loss 1.6806
iter 4750: loss 0.8056, time 21334.62ms
iter 4760: loss 0.8186, time 73.86ms
iter 4770: loss 0.8036, time 90.20ms
iter 4780: loss 0.8105, time 77.66ms
iter 4790: loss 0.8436, time 87.69ms
iter 4800: loss 0.8229, time 72.90ms
iter 4810: loss 0.8501, time 93.18ms
iter 4820: loss 0.8342, time 69.82ms
iter 4830: loss 0.8191, time 73.45ms
iter 4840: loss 0.8409, time 76.24ms
iter 4850: loss 0.8225, time 73.40ms
iter 4860: loss 0.8286, time 81.80ms
iter 4870: loss 0.8152, time 96.44ms
iter 4880: loss 0.8361, time 83.05ms
iter 4890: loss 0.8196, time 73.80ms
iter 4900: loss 0.8118, time 83.26ms
iter 4910: loss 0.8394, time 83.75ms
iter 4920: loss 0.8143, time 72.74ms
iter 4930: loss 0.8090, time 72.18ms
iter 4940: loss 0.8060, time 70.01ms
iter 4950: loss 0.8306, time 62.45ms
iter 4960: loss 0.8327, time 80.58ms
iter 4970: loss 0.7883, time 80.15ms
iter 4980: loss 0.7973, time 77.40ms
iter 4990: loss 0.8265, time 75.39ms
step 5000: train loss 0.6299, val loss 1.6949
iter 5000: loss 0.8265, time 21784.39ms
training done
Best validation loss: 1.468855381011963
Total train time: 14.21 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 beauty;
And yet we breathe upon the traitor's nose,
Which presently cannot be sick from the sea,
To fly to him suspect that he may purchase them.

KING RICHARD III:
And I, to him, I condemn him to-morrow.

JOHN OF GAUNT:
To tell him so, he wants with me uncle,
And that the friendliness of two seven years
Did sleep such foremost spirit! So show the walls
As I give my tardy bribes.

KING RICHARD II:
I cannot speak, because thou knew'st,
Because thou a fearful man, I skea not:
Thy brother was not s
Inference time: 2.28 seconds
Tokens per second: 219.03
---------------
Sample 2:
 surely fever.

LEONTES:
Thou hast well
The child-ship more impeach'd to the war,
Who rather to the earth have the sweetest place
On than indigest the temper of the time;
Hook'd up their terror, denies of their sights
Whom we now deserve to know, we create,
Or will keep and keep our tender performance.

Lord Marshal:
We have known it, but that we will not do;
For since we have we shown to thee and reign away
The flattering of our troth, let us sup alone
A true cloudy fear'd out of woe, with our c
Inference time: 2.13 seconds
Tokens per second: 234.74
---------------
Sample 3:
 bastard like a king: she which is
Your beards to his father's rest; and as those are
The heavens divide to make them work
What you speak, honest, who is a poisoner,
Of ungrateful, as our feast, will she stand
The way, like an event.

LEONTES:
Who's there?

First Lord:
What will I speak?

LEONTES:
This is a respect, that he fears not
The stretch next round with his feet that them?

First Lord:
Say not Bohemia?

LEONTES:
Thou didst make her stops--of breaking breath
And thus that I may say to this
Inference time: 2.04 seconds
Tokens per second: 245.23
---------------
Sample 4:
 let us have married to puling honesty.

QUEEN ELIZABETH:
Ay, now marry, great means show his grace.

KING RICHARD III:
So thrive I too, if thou hast proved in hell.

QUEEN ELIZABETH:
Either thou wilt not call her battle.

KING RICHARD III:
Well, nor England's reported prince.

QUEEN ELIZABETH:
Nor how like the sun hath been told.

KING RICHARD III:
Here be the duke's dead; and she shall purchase thee ill.

QUEEN ELIZABETH:
What would you do?

KING RICHARD III:
Long Richard and Lord Hastings, who
Inference time: 2.08 seconds
Tokens per second: 240.13
---------------
Sample 5:
 will you not.

DUKE VINCENTIO:
Where is the duke?

LUCIO:

ESCALUS:
Why do you dissemble? and you shall hear us all?

DUKE VINCENTIO:
Be ruled by constable.

LUCIO:
Why, master, where is Barnardine?

DUKE VINCENTIO:
Then you shall ask your husband.

LUCIO:
This is the woman in Claudio. Has you all about your
purse of voice: he hath won to private the worst
deputy for you; he shall be thine advised with him. You
will think and me let my son cast out of his rage: had he
deserved it out of an elder
Inference time: 1.96 seconds
Tokens per second: 255.12
---------------
Sample 6:
 my father's blame.
Look you, that there is my brother's hope,
The right impression of my manhood,
To draw him in his suits and dangerous;
And for his substances might have lender'd them
For that the commons and my revengeful friend,
Which, I have no covert'st of my suit and mine.

RICHARD:
Full of winters be a bound of back and love,
Shall be my patience to their sovereign's sons:
I am their deaths are in my gracious counsel:
Our lands are burned upon that bloody days.

YORK:
The new-devil, drea
Inference time: 2.04 seconds
Tokens per second: 244.90
---------------
Sample 7:
 with all your husband.

First Murderer:
Who was resolved to her brother's kind are not?

Second Murderer:
'Zounds, good con friar, let's away.

CLARENCE:
Madam, with him, and that with a pretty heart,
Here let us to win the breath of pain of the world
This flesh was death and many thing else
Did take her quite!

First Murderer:
Here's one, a horse.

CLARENCE:
Lord mayor, he is despite of love;
Therefore to-day can you with me soundly?

CLARENCE:
The brother's right boy.

Second Murderer:
Now I h
Inference time: 2.01 seconds
Tokens per second: 249.15
---------------
Sample 8:
 do it know
The father's mother; who, on the other side
Is your good will report you.

CORIOLANUS:
Dispatch;
The people may be: you are damn'd in graves;
If you'll be oft again, sir, do you know
That we have much better content.

CORIOLANUS:
Which is this good ready?

AUFIDIUS:
A fire unlikely!

CORIOLANUS:
I know him our general:
As there is a man of his chance, the oracle
That thou shalt carry him home.

All:
This true is not.

AUFIDIUS:
Go see what I was a good with behind
That ever you can sh
Inference time: 2.00 seconds
Tokens per second: 250.12
---------------
Sample 9:
 slanderous report.

NORTHUMBERLAND:
Yet light in the hatches of heaven,
Will not hear the lie unto his night?

LORD WILLOUGHBY:
No, God forbid his fault that breathes hid
Shall do more ease than for his soul's forest daughter.

LORD ROSS:
My Lord of Westmoreland, is that the duke wrong
Is not the truth of her grievous traitor's skull?
Not all the day shall not see her faults;
For she was fitter to me to live and down
In earth that way I for her she should show.
Now, Montague, for our brother Geo
Inference time: 2.02 seconds
Tokens per second: 247.70
---------------
Sample 10:
 makes their faces to our own grace,
And how can yet for your heart, you never want,
That you can make work.

CORIOLANUS:
Ha! my name!

VOLUMNIA:
I have spoke your curse to do your good upon
me. I would do deny you this intent, as I had had
as in Vienna down she would do him worse than her
do you can do him promise him.

VOLUMNIA:
I am already to prison from you: I have learn'd it soul.
Sirrah, get you gone, 'tis not to be shue to
pity there; and your fragments, that he lives all good to
the grea
Inference time: 2.05 seconds
Tokens per second: 244.10
---------------
Average tokens per second: 243.02
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2372, val loss 4.2295
iter 0: loss 4.2337, time 22081.85ms
iter 10: loss 3.2253, time 68.99ms
iter 20: loss 2.7688, time 82.00ms
iter 30: loss 2.6201, time 73.56ms
iter 40: loss 2.5385, time 70.97ms
iter 50: loss 2.5372, time 71.28ms
iter 60: loss 2.4994, time 77.97ms
iter 70: loss 2.4941, time 78.27ms
iter 80: loss 2.4956, time 78.19ms
iter 90: loss 2.4797, time 88.08ms
iter 100: loss 2.4705, time 76.32ms
iter 110: loss 2.4340, time 76.83ms
iter 120: loss 2.4510, time 66.58ms
iter 130: loss 2.4320, time 71.16ms
iter 140: loss 2.4234, time 78.26ms
iter 150: loss 2.3597, time 73.90ms
iter 160: loss 2.3873, time 75.30ms
iter 170: loss 2.3336, time 93.10ms
iter 180: loss 2.3175, time 71.89ms
iter 190: loss 2.2810, time 79.19ms
iter 200: loss 2.2438, time 73.65ms
iter 210: loss 2.1642, time 65.68ms
iter 220: loss 2.1270, time 72.63ms
iter 230: loss 2.0941, time 75.06ms
iter 240: loss 2.0579, time 81.53ms
step 250: train loss 1.9779, val loss 2.0816
iter 250: loss 2.0287, time 21536.20ms
iter 260: loss 2.0116, time 72.05ms
iter 270: loss 1.9454, time 76.83ms
iter 280: loss 1.9455, time 79.41ms
iter 290: loss 1.9122, time 77.08ms
iter 300: loss 1.9054, time 75.46ms
iter 310: loss 1.9008, time 74.05ms
iter 320: loss 1.8602, time 76.75ms
iter 330: loss 1.8663, time 78.12ms
iter 340: loss 1.7884, time 78.11ms
iter 350: loss 1.7895, time 76.05ms
iter 360: loss 1.7921, time 78.04ms
iter 370: loss 1.7804, time 72.56ms
iter 380: loss 1.7480, time 67.46ms
iter 390: loss 1.7389, time 67.25ms
iter 400: loss 1.7056, time 69.78ms
iter 410: loss 1.7072, time 75.92ms
iter 420: loss 1.7158, time 75.34ms
iter 430: loss 1.6903, time 70.37ms
iter 440: loss 1.6615, time 74.70ms
iter 450: loss 1.6362, time 69.26ms
iter 460: loss 1.6846, time 70.72ms
iter 470: loss 1.6478, time 68.02ms
iter 480: loss 1.6532, time 74.74ms
iter 490: loss 1.6036, time 73.89ms
step 500: train loss 1.5442, val loss 1.7415
iter 500: loss 1.6123, time 21936.39ms
iter 510: loss 1.6040, time 75.38ms
iter 520: loss 1.5718, time 76.08ms
iter 530: loss 1.6020, time 59.30ms
iter 540: loss 1.5548, time 76.70ms
iter 550: loss 1.5531, time 68.86ms
iter 560: loss 1.5567, time 71.79ms
iter 570: loss 1.5751, time 69.04ms
iter 580: loss 1.5186, time 79.93ms
iter 590: loss 1.5370, time 73.65ms
iter 600: loss 1.5009, time 78.49ms
iter 610: loss 1.5339, time 73.64ms
iter 620: loss 1.4946, time 70.83ms
iter 630: loss 1.5427, time 72.18ms
iter 640: loss 1.4644, time 68.22ms
iter 650: loss 1.4950, time 71.89ms
iter 660: loss 1.4913, time 67.66ms
iter 670: loss 1.4897, time 74.77ms
iter 680: loss 1.4456, time 78.11ms
iter 690: loss 1.4595, time 77.76ms
iter 700: loss 1.4613, time 70.89ms
iter 710: loss 1.4834, time 72.07ms
iter 720: loss 1.4425, time 70.06ms
iter 730: loss 1.4520, time 88.50ms
iter 740: loss 1.4360, time 76.84ms
step 750: train loss 1.3635, val loss 1.5888
iter 750: loss 1.4446, time 23206.10ms
iter 760: loss 1.4000, time 83.55ms
iter 770: loss 1.4136, time 84.94ms
iter 780: loss 1.4169, time 90.28ms
iter 790: loss 1.3899, time 86.41ms
iter 800: loss 1.4148, time 84.82ms
iter 810: loss 1.4276, time 66.59ms
iter 820: loss 1.4045, time 77.84ms
iter 830: loss 1.4013, time 75.03ms
iter 840: loss 1.3965, time 72.64ms
iter 850: loss 1.4217, time 84.26ms
iter 860: loss 1.3702, time 72.94ms
iter 870: loss 1.3925, time 76.16ms
iter 880: loss 1.3612, time 71.22ms
iter 890: loss 1.3844, time 80.33ms
iter 900: loss 1.3563, time 74.66ms
iter 910: loss 1.3999, time 72.54ms
iter 920: loss 1.3711, time 75.40ms
iter 930: loss 1.3617, time 72.62ms
iter 940: loss 1.3660, time 73.31ms
iter 950: loss 1.3479, time 82.00ms
iter 960: loss 1.3805, time 72.87ms
iter 970: loss 1.3697, time 96.69ms
iter 980: loss 1.3698, time 70.29ms
iter 990: loss 1.3074, time 69.33ms
step 1000: train loss 1.2723, val loss 1.5156
iter 1000: loss 1.3387, time 21781.96ms
iter 1010: loss 1.3584, time 76.12ms
iter 1020: loss 1.3494, time 71.41ms
iter 1030: loss 1.3237, time 76.29ms
iter 1040: loss 1.2957, time 78.88ms
iter 1050: loss 1.2924, time 81.34ms
iter 1060: loss 1.3052, time 72.60ms
iter 1070: loss 1.3234, time 76.55ms
iter 1080: loss 1.3212, time 81.19ms
iter 1090: loss 1.3532, time 76.42ms
iter 1100: loss 1.2839, time 75.83ms
iter 1110: loss 1.3097, time 80.56ms
iter 1120: loss 1.2954, time 80.96ms
iter 1130: loss 1.3060, time 74.61ms
iter 1140: loss 1.2988, time 75.85ms
iter 1150: loss 1.2867, time 78.34ms
iter 1160: loss 1.2864, time 75.61ms
iter 1170: loss 1.2732, time 72.97ms
iter 1180: loss 1.2725, time 79.17ms
iter 1190: loss 1.3286, time 81.01ms
iter 1200: loss 1.2947, time 72.55ms
iter 1210: loss 1.2981, time 77.08ms
iter 1220: loss 1.2956, time 79.29ms
iter 1230: loss 1.2468, time 73.99ms
iter 1240: loss 1.2679, time 77.27ms
step 1250: train loss 1.2087, val loss 1.5004
iter 1250: loss 1.3028, time 21530.39ms
iter 1260: loss 1.2943, time 69.86ms
iter 1270: loss 1.2985, time 68.00ms
iter 1280: loss 1.2684, time 72.68ms
iter 1290: loss 1.2958, time 79.17ms
iter 1300: loss 1.2981, time 79.92ms
iter 1310: loss 1.2517, time 76.10ms
iter 1320: loss 1.2854, time 73.63ms
iter 1330: loss 1.2682, time 75.20ms
iter 1340: loss 1.2449, time 82.84ms
iter 1350: loss 1.2430, time 78.93ms
iter 1360: loss 1.2980, time 72.57ms
iter 1370: loss 1.2660, time 74.85ms
iter 1380: loss 1.2484, time 72.13ms
iter 1390: loss 1.2601, time 71.75ms
iter 1400: loss 1.2578, time 73.06ms
iter 1410: loss 1.2599, time 69.92ms
iter 1420: loss 1.2416, time 70.64ms
iter 1430: loss 1.2315, time 72.53ms
iter 1440: loss 1.2141, time 74.56ms
iter 1450: loss 1.2705, time 72.88ms
iter 1460: loss 1.2399, time 73.33ms
iter 1470: loss 1.2157, time 81.83ms
iter 1480: loss 1.2166, time 73.90ms
iter 1490: loss 1.2520, time 74.59ms
step 1500: train loss 1.1502, val loss 1.4785
iter 1500: loss 1.2380, time 21531.62ms
iter 1510: loss 1.2302, time 75.19ms
iter 1520: loss 1.2411, time 68.52ms
iter 1530: loss 1.2152, time 68.63ms
iter 1540: loss 1.2451, time 68.33ms
iter 1550: loss 1.2224, time 73.90ms
iter 1560: loss 1.2519, time 75.75ms
iter 1570: loss 1.2060, time 68.38ms
iter 1580: loss 1.1967, time 75.77ms
iter 1590: loss 1.1945, time 71.23ms
iter 1600: loss 1.2102, time 70.86ms
iter 1610: loss 1.1790, time 72.47ms
iter 1620: loss 1.2184, time 75.69ms
iter 1630: loss 1.2312, time 69.04ms
iter 1640: loss 1.2226, time 71.67ms
iter 1650: loss 1.1878, time 77.61ms
iter 1660: loss 1.1943, time 74.81ms
iter 1670: loss 1.2236, time 86.91ms
iter 1680: loss 1.1735, time 73.52ms
iter 1690: loss 1.1852, time 69.22ms
iter 1700: loss 1.1782, time 77.58ms
iter 1710: loss 1.1491, time 72.63ms
iter 1720: loss 1.1745, time 81.63ms
iter 1730: loss 1.1951, time 76.31ms
iter 1740: loss 1.1924, time 72.94ms
step 1750: train loss 1.1022, val loss 1.4770
iter 1750: loss 1.1847, time 21723.03ms
iter 1760: loss 1.1931, time 76.85ms
iter 1770: loss 1.1916, time 80.05ms
iter 1780: loss 1.1442, time 99.35ms
iter 1790: loss 1.1843, time 82.30ms
iter 1800: loss 1.1414, time 73.53ms
iter 1810: loss 1.1691, time 81.02ms
iter 1820: loss 1.1700, time 83.74ms
iter 1830: loss 1.1933, time 92.59ms
iter 1840: loss 1.1649, time 88.52ms
iter 1850: loss 1.1879, time 85.25ms
iter 1860: loss 1.2141, time 83.34ms
iter 1870: loss 1.1780, time 83.11ms
iter 1880: loss 1.1572, time 86.71ms
iter 1890: loss 1.1520, time 73.19ms
iter 1900: loss 1.1593, time 81.91ms
iter 1910: loss 1.1614, time 86.42ms
iter 1920: loss 1.1476, time 74.77ms
iter 1930: loss 1.1758, time 73.04ms
iter 1940: loss 1.1446, time 70.83ms
iter 1950: loss 1.1267, time 68.50ms
iter 1960: loss 1.1552, time 71.10ms
iter 1970: loss 1.1400, time 80.95ms
iter 1980: loss 1.1929, time 63.86ms
iter 1990: loss 1.1349, time 70.63ms
step 2000: train loss 1.0575, val loss 1.4836
iter 2000: loss 1.1256, time 21650.94ms
iter 2010: loss 1.1393, time 75.82ms
iter 2020: loss 1.1237, time 67.69ms
iter 2030: loss 1.1412, time 74.55ms
iter 2040: loss 1.1409, time 71.49ms
iter 2050: loss 1.1124, time 72.04ms
iter 2060: loss 1.1145, time 75.87ms
iter 2070: loss 1.1406, time 74.87ms
iter 2080: loss 1.1094, time 75.50ms
iter 2090: loss 1.1271, time 69.55ms
iter 2100: loss 1.1369, time 70.35ms
iter 2110: loss 1.1370, time 79.11ms
iter 2120: loss 1.1461, time 73.63ms
iter 2130: loss 1.1299, time 75.66ms
iter 2140: loss 1.0995, time 73.71ms
iter 2150: loss 1.1065, time 73.55ms
iter 2160: loss 1.0992, time 86.30ms
iter 2170: loss 1.1090, time 76.94ms
iter 2180: loss 1.1170, time 74.81ms
iter 2190: loss 1.1540, time 92.23ms
iter 2200: loss 1.1345, time 72.86ms
iter 2210: loss 1.1309, time 72.44ms
iter 2220: loss 1.1211, time 72.97ms
iter 2230: loss 1.1032, time 72.21ms
iter 2240: loss 1.1166, time 68.69ms
step 2250: train loss 1.0071, val loss 1.4900
iter 2250: loss 1.1250, time 21608.65ms
iter 2260: loss 1.1047, time 79.68ms
iter 2270: loss 1.1010, time 71.45ms
iter 2280: loss 1.0786, time 67.07ms
iter 2290: loss 1.0824, time 73.00ms
iter 2300: loss 1.1224, time 67.56ms
iter 2310: loss 1.0806, time 68.78ms
iter 2320: loss 1.0797, time 72.48ms
iter 2330: loss 1.0977, time 72.97ms
iter 2340: loss 1.0933, time 66.26ms
iter 2350: loss 1.0785, time 70.17ms
iter 2360: loss 1.0857, time 68.27ms
iter 2370: loss 1.1001, time 65.03ms
iter 2380: loss 1.0852, time 70.56ms
iter 2390: loss 1.0902, time 69.05ms
iter 2400: loss 1.0453, time 69.93ms
iter 2410: loss 1.0905, time 68.50ms
iter 2420: loss 1.0680, time 77.15ms
iter 2430: loss 1.0519, time 70.46ms
iter 2440: loss 1.0528, time 68.29ms
iter 2450: loss 1.0749, time 71.43ms
iter 2460: loss 1.0833, time 71.07ms
iter 2470: loss 1.0801, time 73.11ms
iter 2480: loss 1.0594, time 70.21ms
iter 2490: loss 1.0658, time 70.90ms
step 2500: train loss 0.9581, val loss 1.5026
iter 2500: loss 1.0757, time 19733.15ms
iter 2510: loss 1.0488, time 64.12ms
iter 2520: loss 1.0674, time 68.17ms
iter 2530: loss 1.0840, time 73.03ms
iter 2540: loss 1.0448, time 71.53ms
iter 2550: loss 1.0657, time 74.24ms
iter 2560: loss 1.0561, time 68.93ms
iter 2570: loss 1.0500, time 71.44ms
iter 2580: loss 1.0462, time 68.84ms
iter 2590: loss 1.0571, time 72.77ms
iter 2600: loss 1.0673, time 71.71ms
iter 2610: loss 1.0317, time 77.30ms
iter 2620: loss 1.0553, time 69.35ms
iter 2630: loss 1.0583, time 91.42ms
iter 2640: loss 1.0185, time 65.74ms
iter 2650: loss 1.0349, time 78.46ms
iter 2660: loss 1.0482, time 67.89ms
iter 2670: loss 1.0440, time 67.25ms
iter 2680: loss 1.0400, time 70.75ms
iter 2690: loss 1.0547, time 74.84ms
iter 2700: loss 1.0111, time 70.60ms
iter 2710: loss 1.0252, time 74.59ms
iter 2720: loss 0.9946, time 78.03ms
iter 2730: loss 1.0350, time 74.10ms
iter 2740: loss 1.0282, time 67.04ms
step 2750: train loss 0.9086, val loss 1.5280
iter 2750: loss 1.0421, time 21318.64ms
iter 2760: loss 1.0039, time 71.38ms
iter 2770: loss 1.0407, time 70.24ms
iter 2780: loss 1.0328, time 69.61ms
iter 2790: loss 1.0165, time 90.66ms
iter 2800: loss 1.0171, time 67.16ms
iter 2810: loss 1.0178, time 75.52ms
iter 2820: loss 1.0355, time 67.17ms
iter 2830: loss 0.9801, time 73.44ms
iter 2840: loss 1.0036, time 86.15ms
iter 2850: loss 1.0131, time 69.98ms
iter 2860: loss 1.0000, time 71.14ms
iter 2870: loss 1.0138, time 71.51ms
iter 2880: loss 1.0170, time 81.77ms
iter 2890: loss 1.0155, time 74.77ms
iter 2900: loss 1.0296, time 71.39ms
iter 2910: loss 1.0121, time 66.40ms
iter 2920: loss 0.9694, time 63.85ms
iter 2930: loss 1.0036, time 72.76ms
iter 2940: loss 0.9976, time 70.74ms
iter 2950: loss 0.9953, time 68.43ms
iter 2960: loss 1.0045, time 69.85ms
iter 2970: loss 0.9888, time 69.76ms
iter 2980: loss 1.0217, time 73.91ms
iter 2990: loss 0.9963, time 63.26ms
step 3000: train loss 0.8644, val loss 1.5472
iter 3000: loss 0.9931, time 21014.98ms
iter 3010: loss 1.0108, time 68.42ms
iter 3020: loss 1.0050, time 71.83ms
iter 3030: loss 1.0068, time 74.30ms
iter 3040: loss 0.9904, time 71.07ms
iter 3050: loss 0.9851, time 68.15ms
iter 3060: loss 0.9989, time 70.42ms
iter 3070: loss 0.9939, time 73.65ms
iter 3080: loss 0.9847, time 72.16ms
iter 3090: loss 0.9989, time 64.77ms
iter 3100: loss 0.9775, time 71.82ms
iter 3110: loss 0.9912, time 78.76ms
iter 3120: loss 0.9814, time 70.00ms
iter 3130: loss 0.9878, time 71.43ms
iter 3140: loss 0.9950, time 75.08ms
iter 3150: loss 0.9567, time 74.88ms
iter 3160: loss 0.9644, time 68.69ms
iter 3170: loss 0.9564, time 62.63ms
iter 3180: loss 0.9470, time 62.21ms
iter 3190: loss 0.9755, time 74.42ms
iter 3200: loss 0.9775, time 72.10ms
iter 3210: loss 0.9927, time 67.80ms
iter 3220: loss 0.9778, time 67.33ms
iter 3230: loss 0.9914, time 78.03ms
iter 3240: loss 0.9428, time 67.24ms
step 3250: train loss 0.8172, val loss 1.5792
iter 3250: loss 0.9934, time 21017.63ms
iter 3260: loss 0.9761, time 73.73ms
iter 3270: loss 0.9672, time 67.72ms
iter 3280: loss 0.9592, time 69.02ms
iter 3290: loss 0.9603, time 76.45ms
iter 3300: loss 0.9476, time 74.83ms
iter 3310: loss 0.9566, time 69.92ms
iter 3320: loss 0.9404, time 80.84ms
iter 3330: loss 0.9448, time 66.76ms
iter 3340: loss 0.9632, time 69.32ms
iter 3350: loss 0.9761, time 71.17ms
iter 3360: loss 0.9361, time 71.28ms
iter 3370: loss 0.9367, time 71.82ms
iter 3380: loss 0.9514, time 75.71ms
iter 3390: loss 0.9359, time 73.30ms
iter 3400: loss 0.9358, time 73.23ms
iter 3410: loss 0.9507, time 70.36ms
iter 3420: loss 0.9331, time 75.87ms
iter 3430: loss 0.9323, time 75.52ms
iter 3440: loss 0.9256, time 70.32ms
iter 3450: loss 0.9184, time 74.71ms
iter 3460: loss 0.9813, time 72.00ms
iter 3470: loss 0.9255, time 75.87ms
iter 3480: loss 0.9298, time 67.12ms
iter 3490: loss 0.9350, time 67.27ms
step 3500: train loss 0.7748, val loss 1.5988
iter 3500: loss 0.9256, time 21005.31ms
iter 3510: loss 0.9470, time 68.66ms
iter 3520: loss 0.9060, time 70.55ms
iter 3530: loss 0.9252, time 75.20ms
iter 3540: loss 0.9429, time 80.24ms
iter 3550: loss 0.9221, time 78.36ms
iter 3560: loss 0.9568, time 65.05ms
iter 3570: loss 0.9290, time 68.51ms
iter 3580: loss 0.9210, time 72.63ms
iter 3590: loss 0.9387, time 69.16ms
iter 3600: loss 0.9101, time 70.83ms
iter 3610: loss 0.9180, time 69.54ms
iter 3620: loss 0.9416, time 72.82ms
iter 3630: loss 0.9058, time 74.09ms
iter 3640: loss 0.9273, time 73.53ms
iter 3650: loss 0.9291, time 71.11ms
iter 3660: loss 0.9129, time 70.73ms
iter 3670: loss 0.9024, time 75.53ms
iter 3680: loss 0.8907, time 75.83ms
iter 3690: loss 0.9008, time 73.28ms
iter 3700: loss 0.9094, time 83.50ms
iter 3710: loss 0.8875, time 69.16ms
iter 3720: loss 0.9101, time 71.46ms
iter 3730: loss 0.9210, time 75.98ms
iter 3740: loss 0.8850, time 71.96ms
step 3750: train loss 0.7343, val loss 1.6331
iter 3750: loss 0.9113, time 21233.22ms
iter 3760: loss 0.8845, time 69.90ms
iter 3770: loss 0.8946, time 73.03ms
iter 3780: loss 0.9048, time 76.41ms
iter 3790: loss 0.8695, time 72.88ms
iter 3800: loss 0.8978, time 78.22ms
iter 3810: loss 0.9249, time 79.49ms
iter 3820: loss 0.9164, time 70.95ms
iter 3830: loss 0.8890, time 76.17ms
iter 3840: loss 0.8940, time 72.42ms
iter 3850: loss 0.8958, time 73.46ms
iter 3860: loss 0.8711, time 73.03ms
iter 3870: loss 0.8676, time 72.38ms
iter 3880: loss 0.8697, time 70.15ms
iter 3890: loss 0.8910, time 73.14ms
iter 3900: loss 0.9219, time 67.59ms
iter 3910: loss 0.8862, time 69.81ms
iter 3920: loss 0.8952, time 71.15ms
iter 3930: loss 0.8805, time 77.74ms
iter 3940: loss 0.8904, time 73.69ms
iter 3950: loss 0.8793, time 64.61ms
iter 3960: loss 0.8774, time 65.89ms
iter 3970: loss 0.8586, time 71.42ms
iter 3980: loss 0.8847, time 72.54ms
iter 3990: loss 0.8734, time 70.75ms
step 4000: train loss 0.6998, val loss 1.6522
iter 4000: loss 0.8787, time 21073.73ms
iter 4010: loss 0.8856, time 68.68ms
iter 4020: loss 0.8451, time 72.30ms
iter 4030: loss 0.8861, time 66.72ms
iter 4040: loss 0.8728, time 72.56ms
iter 4050: loss 0.8502, time 66.45ms
iter 4060: loss 0.8681, time 70.17ms
iter 4070: loss 0.8885, time 69.64ms
iter 4080: loss 0.8735, time 68.40ms
iter 4090: loss 0.8440, time 71.51ms
iter 4100: loss 0.8766, time 76.57ms
iter 4110: loss 0.8829, time 68.53ms
iter 4120: loss 0.8472, time 67.38ms
iter 4130: loss 0.8469, time 75.16ms
iter 4140: loss 0.8635, time 69.71ms
iter 4150: loss 0.8533, time 78.45ms
iter 4160: loss 0.8493, time 69.31ms
iter 4170: loss 0.8667, time 74.63ms
iter 4180: loss 0.8751, time 67.75ms
iter 4190: loss 0.8539, time 73.08ms
iter 4200: loss 0.8581, time 70.80ms
iter 4210: loss 0.8793, time 77.49ms
iter 4220: loss 0.8596, time 72.34ms
iter 4230: loss 0.8699, time 77.08ms
iter 4240: loss 0.8568, time 76.84ms
step 4250: train loss 0.6736, val loss 1.6630
iter 4250: loss 0.8586, time 21685.69ms
iter 4260: loss 0.8693, time 68.55ms
iter 4270: loss 0.8308, time 76.36ms
iter 4280: loss 0.8581, time 65.24ms
iter 4290: loss 0.8352, time 80.48ms
iter 4300: loss 0.8520, time 73.02ms
iter 4310: loss 0.8490, time 72.16ms
iter 4320: loss 0.8667, time 72.75ms
iter 4330: loss 0.8651, time 80.24ms
iter 4340: loss 0.8764, time 67.00ms
iter 4350: loss 0.8527, time 74.40ms
iter 4360: loss 0.8452, time 69.01ms
iter 4370: loss 0.8206, time 80.19ms
iter 4380: loss 0.8510, time 66.27ms
iter 4390: loss 0.8311, time 70.09ms
iter 4400: loss 0.8294, time 71.76ms
iter 4410: loss 0.8451, time 68.00ms
iter 4420: loss 0.8393, time 70.24ms
iter 4430: loss 0.8344, time 69.73ms
iter 4440: loss 0.8288, time 66.18ms
iter 4450: loss 0.8534, time 74.38ms
iter 4460: loss 0.8374, time 65.28ms
iter 4470: loss 0.8486, time 92.00ms
iter 4480: loss 0.8447, time 67.39ms
iter 4490: loss 0.8239, time 65.37ms
step 4500: train loss 0.6468, val loss 1.6912
iter 4500: loss 0.8414, time 21141.19ms
iter 4510: loss 0.8429, time 66.23ms
iter 4520: loss 0.8403, time 75.14ms
iter 4530: loss 0.8265, time 72.97ms
iter 4540: loss 0.8180, time 76.38ms
iter 4550: loss 0.8228, time 82.03ms
iter 4560: loss 0.8358, time 78.97ms
iter 4570: loss 0.8350, time 70.03ms
iter 4580: loss 0.8136, time 73.49ms
iter 4590: loss 0.8146, time 74.90ms
iter 4600: loss 0.8304, time 72.63ms
iter 4610: loss 0.8268, time 68.57ms
iter 4620: loss 0.8089, time 70.06ms
iter 4630: loss 0.8304, time 72.07ms
iter 4640: loss 0.8416, time 73.07ms
iter 4650: loss 0.8257, time 71.97ms
iter 4660: loss 0.8502, time 66.63ms
iter 4670: loss 0.8555, time 71.29ms
iter 4680: loss 0.8616, time 59.75ms
iter 4690: loss 0.8279, time 66.21ms
iter 4700: loss 0.8291, time 70.04ms
iter 4710: loss 0.8384, time 73.69ms
iter 4720: loss 0.8210, time 77.45ms
iter 4730: loss 0.8300, time 69.82ms
iter 4740: loss 0.8207, time 71.28ms
step 4750: train loss 0.6285, val loss 1.7017
iter 4750: loss 0.8340, time 20923.57ms
iter 4760: loss 0.8393, time 68.31ms
iter 4770: loss 0.8390, time 69.46ms
iter 4780: loss 0.8203, time 70.63ms
iter 4790: loss 0.8076, time 66.43ms
iter 4800: loss 0.8047, time 72.84ms
iter 4810: loss 0.8329, time 67.01ms
iter 4820: loss 0.8500, time 69.20ms
iter 4830: loss 0.8273, time 72.88ms
iter 4840: loss 0.8034, time 68.62ms
iter 4850: loss 0.8017, time 74.68ms
iter 4860: loss 0.8069, time 69.43ms
iter 4870: loss 0.8041, time 69.51ms
iter 4880: loss 0.8057, time 63.76ms
iter 4890: loss 0.8013, time 69.09ms
iter 4900: loss 0.8005, time 63.14ms
iter 4910: loss 0.8103, time 70.24ms
iter 4920: loss 0.8126, time 68.48ms
iter 4930: loss 0.8167, time 74.91ms
iter 4940: loss 0.8330, time 68.41ms
iter 4950: loss 0.8274, time 69.41ms
iter 4960: loss 0.8407, time 68.25ms
iter 4970: loss 0.7975, time 63.52ms
iter 4980: loss 0.7888, time 71.96ms
iter 4990: loss 0.7918, time 70.50ms
step 5000: train loss 0.6114, val loss 1.7165
iter 5000: loss 0.7890, time 20770.22ms
training done
Best validation loss: 1.476982831954956
Total train time: 13.68 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 and thought our place,
Which canst thou with his contract worth to fall
Enforce the several guilty crown.

WARWICK:
What think'st thou, then, true lord?

KING HENRY VI:
Then, even that thou stay'st the friar's skull:
Say thou must needs to make haste, but not being spoke:
For every man that I kill upon him.

CLIFFORD:
Slaughter, dishonour lie there: let me see them go.

EXETER:
Ay, such a corse; I will not be long.

RIVERS:
An honest is like that mistress: but it is as a
poison which is come to 
Inference time: 2.11 seconds
Tokens per second: 236.44
---------------
Sample 2:
 but that she did is not fourteen.

GLOUCESTER:
What manners of day is time to task;
Say, that dost thou grant the worst to call us the crown,
That were no sweet blood to the duke is slain?

BUCKINGHAM:
Northumberland.

GLOUCESTER:
We are they both: upon the wench is lost;
The crown be never that reigns bears the world;
This little shall see in war, and his heir
Shall not be so tender upon your own.

BUCKINGHAM:
The duke is with the commons and full of yours.

QUEEN MARGARET:
The law hath almost 
Inference time: 2.06 seconds
Tokens per second: 242.39
---------------
Sample 3:
 his excellent king aid;
And what then you have misfortune shall be married.

KING EDWARD IV:
Now, brother Richard, Lord Rivers, are you so?

GLOUCESTER:
What say'st thou?

BUCKINGHAM:
My lord?

KING RICHARD III:
Go you to Friar Penker; therefore be true.

QUEEN ELIZABETH:
The cause were not here; and here we live.

GLOUCESTER:
Ay, ay.

BUCKINGHAM:
I part well the time that reigns me mock'd in,
But I'll need a world purchase to forego:
Let them can be vengeance by the law of their sight,
But the 
Inference time: 2.02 seconds
Tokens per second: 247.54
---------------
Sample 4:
 gentleman what we have spoke of,
Or if thou art beguiled with noble friends,
Show the deposing of thy state and fear
That it was to be the hand which is as then
That was the thing of Pilate was wooer.
I do not sleep; but I cannot woo;
And so much to reap me to comfort a tale
That I should kill on my soul: but my remember who I
May be satisfied.

A Patrician:
And think'st thou so?

Second Servingman:
Nay, I think, if thou hast too much.

CORIOLANUS:
I may not, sir, I will provide thyself a
monarc
Inference time: 2.01 seconds
Tokens per second: 248.75
---------------
Sample 5:
 a brief
Stands you too? I see, your brother,
The very heart in the your house; it shall
find me this assistant.

DUKE VINCENTIO:
One of our own desires; believe it, and the duke's
is the duke: the provost thou makest an excellent.

ISABELLA:
I pray thee, do not sue to hear me dead?

DUKE VINCENTIO:
How! canst thou hither straight?

ISABELLA:
I am done, sir. I would I fear I were trusted or thee.

DUKE VINCENTIO:
Which is yours and you?

LUCIO:
I would think, Isabel? show your partner is straight
Inference time: 2.07 seconds
Tokens per second: 241.09
---------------
Sample 6:
 death, that laid in marriage him.
And thou, Clifford, thou wilt pity him,
I'll not find it with me that I am proud age.

CLARENCE:
By his true kind which he did the tread up to death.

KING EDWARD IV:
Why, how fares the mother of Surrey
We are all accused the chamber of his side?
And whose brather father hath prevail'd
No ears to the earth begin to come?

RICHARD:
No longer than do I thee with this.

YORK:
My Lord of Warwick, I tell thee why is none.

WARWICK:
I will but follow me I saw the crow
Inference time: 2.07 seconds
Tokens per second: 241.67
---------------
Sample 7:
 desire to part them that I have:
I will not budge fools word for that.

ESCALUS:
Nor I.

POMPHEY:
Nor not a woman's money, but straight it folly.

ESCALUS:
Marry, sir, we'll desire you.

DUKE VINCENTIO:
Why, you are written to do it; for you have
leisure to do be honest, but you are all absoluted.

ESCALUS:
Gentlewomen, good friend, the prince you have done a bawd.

DUKE VINCENTIO:
I pray you, sir, how it must be a woman?

Provost:
Ay, and as a friar, this is he will bow a school.

ESCALUS:
What
Inference time: 2.11 seconds
Tokens per second: 237.39
---------------
Sample 8:
 chamber-my memory, my liege.

KING RICHARD III:
We will, so happy in peril all,
As I say, is to be thus an extremity.

QUEEN ELIZABETH:
But the king hath wrong'd the post that his dark mortal.

KING RICHARD III:
Thanks, gentle Clifford; and therefore proclaim thee,
Forthwith thou hast slain in air and idle age.

QUEEN ELIZABETH:
How long shall this land shall make me small dead.

KING RICHARD III:
Fear be thou speak'st, I'll tell thee what: madam,
Make me many days a present deeds be spent.

QUE
Inference time: 2.07 seconds
Tokens per second: 241.38
---------------
Sample 9:
 Creates venture for the search:
The one side that lay call the word's father.

WARWICK:
Sweet father, be thou so caused of mine.

Post:
In good time to seek his life to cry them;
And yet I repair to the same to my gracious life
And shrewd thy head wherein they live upon,
With outward little body hath something in
The chapes of four special blood. Then they shall plead
Will not be thus for tongue in them.

First Citizen:
To whom the steed, when he does bear the people;
But the parters of his most
Inference time: 2.05 seconds
Tokens per second: 244.24
---------------
Sample 10:
 the wretched deed bridegroom
With that were subdued in meeting weeping soul
May see the be hollow of Jove's head!
O, that of what newer is dead!

Servant:
O, sir, it is the strange of the duke.

DUKE VINCENTIO:
We have pass'd for an accusation.

ANGELO:
I thank you worship to her and none of your bed:
Let me hear no more, for I have show'd you no sin,
Than you have said your enemy is already:
When you shall in this day of men's desert;
And see we three, on the sun adversaries.
Now I tremble ther
Inference time: 2.16 seconds
Tokens per second: 231.47
---------------
Average tokens per second: 241.24
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2469, val loss 4.2417
iter 0: loss 4.2478, time 21835.86ms
iter 10: loss 3.2118, time 69.02ms
iter 20: loss 2.7844, time 78.07ms
iter 30: loss 2.6129, time 67.55ms
iter 40: loss 2.5605, time 67.07ms
iter 50: loss 2.5317, time 69.59ms
iter 60: loss 2.4924, time 64.52ms
iter 70: loss 2.4931, time 71.40ms
iter 80: loss 2.5008, time 62.19ms
iter 90: loss 2.4812, time 68.73ms
iter 100: loss 2.4790, time 64.21ms
iter 110: loss 2.4371, time 68.04ms
iter 120: loss 2.4278, time 71.42ms
iter 130: loss 2.4336, time 71.63ms
iter 140: loss 2.4013, time 72.20ms
iter 150: loss 2.3924, time 76.37ms
iter 160: loss 2.3557, time 80.13ms
iter 170: loss 2.3688, time 63.67ms
iter 180: loss 2.3029, time 70.86ms
iter 190: loss 2.3113, time 63.89ms
iter 200: loss 2.2253, time 68.53ms
iter 210: loss 2.1788, time 77.30ms
iter 220: loss 2.1408, time 67.24ms
iter 230: loss 2.0906, time 62.77ms
iter 240: loss 2.0511, time 69.88ms
step 250: train loss 1.9698, val loss 2.0712
iter 250: loss 2.0447, time 20952.21ms
iter 260: loss 1.9997, time 75.91ms
iter 270: loss 1.9517, time 155.45ms
iter 280: loss 1.9471, time 72.21ms
iter 290: loss 1.9149, time 76.98ms
iter 300: loss 1.9167, time 66.47ms
iter 310: loss 1.8639, time 70.86ms
iter 320: loss 1.8598, time 71.09ms
iter 330: loss 1.8075, time 67.64ms
iter 340: loss 1.8284, time 69.87ms
iter 350: loss 1.8016, time 64.42ms
iter 360: loss 1.7902, time 72.90ms
iter 370: loss 1.7523, time 72.25ms
iter 380: loss 1.7585, time 67.85ms
iter 390: loss 1.7235, time 59.05ms
iter 400: loss 1.7255, time 72.04ms
iter 410: loss 1.7519, time 63.34ms
iter 420: loss 1.6882, time 67.44ms
iter 430: loss 1.6506, time 69.51ms
iter 440: loss 1.7009, time 79.36ms
iter 450: loss 1.6397, time 70.01ms
iter 460: loss 1.6490, time 64.19ms
iter 470: loss 1.6363, time 72.53ms
iter 480: loss 1.6394, time 69.79ms
iter 490: loss 1.6078, time 70.16ms
step 500: train loss 1.5329, val loss 1.7200
iter 500: loss 1.5550, time 21095.17ms
iter 510: loss 1.6169, time 74.75ms
iter 520: loss 1.5670, time 72.83ms
iter 530: loss 1.5757, time 74.12ms
iter 540: loss 1.5898, time 76.82ms
iter 550: loss 1.5509, time 66.72ms
iter 560: loss 1.5550, time 72.77ms
iter 570: loss 1.5589, time 66.46ms
iter 580: loss 1.5319, time 69.49ms
iter 590: loss 1.5216, time 66.38ms
iter 600: loss 1.5144, time 75.19ms
iter 610: loss 1.5111, time 69.20ms
iter 620: loss 1.5254, time 76.11ms
iter 630: loss 1.5247, time 74.34ms
iter 640: loss 1.4928, time 69.22ms
iter 650: loss 1.5050, time 66.00ms
iter 660: loss 1.4813, time 72.43ms
iter 670: loss 1.4717, time 73.37ms
iter 680: loss 1.4562, time 72.92ms
iter 690: loss 1.4653, time 68.21ms
iter 700: loss 1.4707, time 68.11ms
iter 710: loss 1.4841, time 65.52ms
iter 720: loss 1.4645, time 72.93ms
iter 730: loss 1.4584, time 70.13ms
iter 740: loss 1.4256, time 73.94ms
step 750: train loss 1.3584, val loss 1.5812
iter 750: loss 1.4318, time 21312.02ms
iter 760: loss 1.4417, time 76.60ms
iter 770: loss 1.4501, time 70.45ms
iter 780: loss 1.4436, time 76.76ms
iter 790: loss 1.4185, time 68.89ms
iter 800: loss 1.4264, time 72.53ms
iter 810: loss 1.4298, time 76.38ms
iter 820: loss 1.3883, time 72.52ms
iter 830: loss 1.3727, time 75.65ms
iter 840: loss 1.3981, time 80.85ms
iter 850: loss 1.3727, time 76.07ms
iter 860: loss 1.3820, time 84.08ms
iter 870: loss 1.3889, time 75.93ms
iter 880: loss 1.3678, time 69.02ms
iter 890: loss 1.3786, time 73.21ms
iter 900: loss 1.3458, time 77.68ms
iter 910: loss 1.3904, time 66.56ms
iter 920: loss 1.3769, time 75.92ms
iter 930: loss 1.3632, time 79.42ms
iter 940: loss 1.3450, time 75.69ms
iter 950: loss 1.3563, time 66.95ms
iter 960: loss 1.3714, time 73.15ms
iter 970: loss 1.3690, time 68.53ms
iter 980: loss 1.3634, time 71.81ms
iter 990: loss 1.3436, time 63.24ms
step 1000: train loss 1.2681, val loss 1.5268
iter 1000: loss 1.3175, time 21688.31ms
iter 1010: loss 1.3438, time 71.91ms
iter 1020: loss 1.3230, time 76.27ms
iter 1030: loss 1.3508, time 73.17ms
iter 1040: loss 1.3167, time 71.05ms
iter 1050: loss 1.3258, time 68.83ms
iter 1060: loss 1.3436, time 71.84ms
iter 1070: loss 1.3460, time 65.94ms
iter 1080: loss 1.3175, time 67.09ms
iter 1090: loss 1.2672, time 68.87ms
iter 1100: loss 1.3178, time 72.09ms
iter 1110: loss 1.3148, time 67.59ms
iter 1120: loss 1.2947, time 78.65ms
iter 1130: loss 1.2950, time 68.95ms
iter 1140: loss 1.3242, time 70.36ms
iter 1150: loss 1.2659, time 70.25ms
iter 1160: loss 1.3078, time 68.82ms
iter 1170: loss 1.2850, time 74.40ms
iter 1180: loss 1.2616, time 77.52ms
iter 1190: loss 1.3186, time 73.92ms
iter 1200: loss 1.2893, time 83.61ms
iter 1210: loss 1.2849, time 90.93ms
iter 1220: loss 1.2996, time 71.37ms
iter 1230: loss 1.2976, time 78.95ms
iter 1240: loss 1.3021, time 77.45ms
step 1250: train loss 1.2003, val loss 1.4822
iter 1250: loss 1.3033, time 21645.61ms
iter 1260: loss 1.2997, time 80.59ms
iter 1270: loss 1.2928, time 76.92ms
iter 1280: loss 1.2723, time 77.29ms
iter 1290: loss 1.2615, time 67.81ms
iter 1300: loss 1.2957, time 80.18ms
iter 1310: loss 1.2638, time 73.76ms
iter 1320: loss 1.2563, time 74.49ms
iter 1330: loss 1.2585, time 67.35ms
iter 1340: loss 1.2638, time 77.69ms
iter 1350: loss 1.2412, time 71.00ms
iter 1360: loss 1.2323, time 76.62ms
iter 1370: loss 1.2673, time 82.28ms
iter 1380: loss 1.2331, time 93.18ms
iter 1390: loss 1.2446, time 101.98ms
iter 1400: loss 1.2263, time 74.13ms
iter 1410: loss 1.2142, time 80.22ms
iter 1420: loss 1.2601, time 69.88ms
iter 1430: loss 1.2658, time 77.98ms
iter 1440: loss 1.2064, time 72.92ms
iter 1450: loss 1.2632, time 76.10ms
iter 1460: loss 1.2458, time 72.52ms
iter 1470: loss 1.2237, time 70.86ms
iter 1480: loss 1.2135, time 68.07ms
iter 1490: loss 1.2417, time 76.19ms
step 1500: train loss 1.1440, val loss 1.4543
iter 1500: loss 1.1971, time 21126.12ms
iter 1510: loss 1.2298, time 66.84ms
iter 1520: loss 1.2122, time 75.68ms
iter 1530: loss 1.2397, time 74.25ms
iter 1540: loss 1.2109, time 76.18ms
iter 1550: loss 1.1709, time 69.74ms
iter 1560: loss 1.2190, time 66.94ms
iter 1570: loss 1.1968, time 71.76ms
iter 1580: loss 1.2089, time 71.44ms
iter 1590: loss 1.2368, time 71.39ms
iter 1600: loss 1.2087, time 70.55ms
iter 1610: loss 1.2018, time 100.80ms
iter 1620: loss 1.2033, time 69.16ms
iter 1630: loss 1.1973, time 72.28ms
iter 1640: loss 1.2201, time 76.95ms
iter 1650: loss 1.1828, time 72.13ms
iter 1660: loss 1.2138, time 74.26ms
iter 1670: loss 1.2122, time 76.99ms
iter 1680: loss 1.2213, time 76.32ms
iter 1690: loss 1.2234, time 78.97ms
iter 1700: loss 1.2180, time 83.06ms
iter 1710: loss 1.1876, time 68.83ms
iter 1720: loss 1.1868, time 73.87ms
iter 1730: loss 1.1950, time 81.13ms
iter 1740: loss 1.1941, time 68.81ms
step 1750: train loss 1.1010, val loss 1.4698
iter 1750: loss 1.1932, time 21273.87ms
iter 1760: loss 1.1672, time 74.74ms
iter 1770: loss 1.1570, time 94.46ms
iter 1780: loss 1.1589, time 71.16ms
iter 1790: loss 1.1829, time 70.90ms
iter 1800: loss 1.1648, time 86.80ms
iter 1810: loss 1.1798, time 70.57ms
iter 1820: loss 1.1575, time 69.53ms
iter 1830: loss 1.1316, time 70.21ms
iter 1840: loss 1.1695, time 62.95ms
iter 1850: loss 1.1896, time 52.26ms
iter 1860: loss 1.1936, time 61.42ms
iter 1870: loss 1.1470, time 69.14ms
iter 1880: loss 1.2015, time 61.88ms
iter 1890: loss 1.1418, time 64.95ms
iter 1900: loss 1.1857, time 64.27ms
iter 1910: loss 1.1304, time 70.22ms
iter 1920: loss 1.1399, time 68.38ms
iter 1930: loss 1.1382, time 67.64ms
iter 1940: loss 1.1436, time 69.00ms
iter 1950: loss 1.1617, time 66.58ms
iter 1960: loss 1.1365, time 84.34ms
iter 1970: loss 1.1359, time 67.57ms
iter 1980: loss 1.1475, time 65.89ms
iter 1990: loss 1.1418, time 71.52ms
step 2000: train loss 1.0493, val loss 1.4740
iter 2000: loss 1.1511, time 21004.14ms
iter 2010: loss 1.1379, time 72.05ms
iter 2020: loss 1.1483, time 81.51ms
iter 2030: loss 1.1189, time 72.87ms
iter 2040: loss 1.1474, time 70.54ms
iter 2050: loss 1.1393, time 68.40ms
iter 2060: loss 1.1623, time 73.32ms
iter 2070: loss 1.1550, time 66.00ms
iter 2080: loss 1.1370, time 67.81ms
iter 2090: loss 1.1123, time 66.28ms
iter 2100: loss 1.1134, time 71.32ms
iter 2110: loss 1.1107, time 72.27ms
iter 2120: loss 1.0789, time 64.96ms
iter 2130: loss 1.1235, time 64.11ms
iter 2140: loss 1.1126, time 68.62ms
iter 2150: loss 1.1263, time 69.44ms
iter 2160: loss 1.1338, time 68.66ms
iter 2170: loss 1.1364, time 66.08ms
iter 2180: loss 1.1447, time 67.75ms
iter 2190: loss 1.1181, time 70.16ms
iter 2200: loss 1.1097, time 70.68ms
iter 2210: loss 1.1108, time 70.90ms
iter 2220: loss 1.1022, time 84.07ms
iter 2230: loss 1.1070, time 74.12ms
iter 2240: loss 1.1105, time 64.32ms
step 2250: train loss 1.0037, val loss 1.4756
iter 2250: loss 1.0963, time 21386.28ms
iter 2260: loss 1.0992, time 65.94ms
iter 2270: loss 1.1233, time 72.42ms
iter 2280: loss 1.1012, time 66.06ms
iter 2290: loss 1.0701, time 67.63ms
iter 2300: loss 1.0697, time 73.09ms
iter 2310: loss 1.0922, time 80.59ms
iter 2320: loss 1.0898, time 66.08ms
iter 2330: loss 1.1216, time 68.04ms
iter 2340: loss 1.0741, time 71.66ms
iter 2350: loss 1.1119, time 67.43ms
iter 2360: loss 1.0752, time 71.63ms
iter 2370: loss 1.0671, time 69.42ms
iter 2380: loss 1.0831, time 69.83ms
iter 2390: loss 1.0784, time 63.47ms
iter 2400: loss 1.0913, time 67.77ms
iter 2410: loss 1.0730, time 73.85ms
iter 2420: loss 1.0570, time 65.48ms
iter 2430: loss 1.0887, time 67.87ms
iter 2440: loss 1.0892, time 68.34ms
iter 2450: loss 1.0757, time 79.66ms
iter 2460: loss 1.0357, time 69.41ms
iter 2470: loss 1.0852, time 71.72ms
iter 2480: loss 1.0589, time 67.96ms
iter 2490: loss 1.0809, time 71.61ms
step 2500: train loss 0.9597, val loss 1.4907
iter 2500: loss 1.0593, time 21331.86ms
iter 2510: loss 1.0688, time 71.08ms
iter 2520: loss 1.0648, time 70.05ms
iter 2530: loss 1.0734, time 69.59ms
iter 2540: loss 1.0536, time 73.05ms
iter 2550: loss 1.0591, time 66.92ms
iter 2560: loss 1.0516, time 58.72ms
iter 2570: loss 1.0615, time 66.82ms
iter 2580: loss 1.0409, time 65.62ms
iter 2590: loss 1.0750, time 69.52ms
iter 2600: loss 1.0837, time 67.02ms
iter 2610: loss 1.0651, time 65.55ms
iter 2620: loss 1.0490, time 66.83ms
iter 2630: loss 1.0505, time 76.53ms
iter 2640: loss 1.0452, time 82.51ms
iter 2650: loss 1.0519, time 65.75ms
iter 2660: loss 1.0489, time 65.69ms
iter 2670: loss 1.0291, time 73.41ms
iter 2680: loss 1.0416, time 68.83ms
iter 2690: loss 1.0670, time 66.52ms
iter 2700: loss 1.0700, time 69.02ms
iter 2710: loss 1.0378, time 69.00ms
iter 2720: loss 1.0325, time 67.40ms
iter 2730: loss 1.0319, time 70.01ms
iter 2740: loss 1.0297, time 80.17ms
step 2750: train loss 0.9087, val loss 1.5120
iter 2750: loss 1.0385, time 20436.18ms
iter 2760: loss 1.0033, time 60.93ms
iter 2770: loss 1.0046, time 66.29ms
iter 2780: loss 1.0169, time 68.92ms
iter 2790: loss 1.0366, time 67.34ms
iter 2800: loss 1.0141, time 73.43ms
iter 2810: loss 0.9835, time 67.03ms
iter 2820: loss 1.0118, time 73.27ms
iter 2830: loss 0.9748, time 68.44ms
iter 2840: loss 1.0299, time 68.22ms
iter 2850: loss 1.0080, time 63.83ms
iter 2860: loss 1.0210, time 69.37ms
iter 2870: loss 1.0361, time 69.17ms
iter 2880: loss 1.0043, time 70.45ms
iter 2890: loss 0.9855, time 65.25ms
iter 2900: loss 0.9978, time 76.12ms
iter 2910: loss 1.0204, time 71.61ms
iter 2920: loss 1.0216, time 77.59ms
iter 2930: loss 0.9900, time 66.42ms
iter 2940: loss 0.9858, time 64.71ms
iter 2950: loss 0.9949, time 74.22ms
iter 2960: loss 1.0255, time 72.77ms
iter 2970: loss 0.9792, time 76.51ms
iter 2980: loss 0.9809, time 74.03ms
iter 2990: loss 0.9881, time 68.48ms
step 3000: train loss 0.8633, val loss 1.5335
iter 3000: loss 0.9974, time 20925.09ms
iter 3010: loss 0.9768, time 66.31ms
iter 3020: loss 1.0222, time 62.24ms
iter 3030: loss 0.9884, time 68.98ms
iter 3040: loss 0.9678, time 64.47ms
iter 3050: loss 0.9920, time 62.70ms
iter 3060: loss 1.0056, time 70.22ms
iter 3070: loss 0.9991, time 65.31ms
iter 3080: loss 0.9747, time 65.45ms
iter 3090: loss 1.0022, time 63.26ms
iter 3100: loss 0.9862, time 71.85ms
iter 3110: loss 1.0161, time 60.94ms
iter 3120: loss 0.9648, time 72.61ms
iter 3130: loss 0.9621, time 64.02ms
iter 3140: loss 0.9763, time 71.30ms
iter 3150: loss 0.9918, time 74.74ms
iter 3160: loss 0.9604, time 70.42ms
iter 3170: loss 0.9550, time 69.55ms
iter 3180: loss 0.9830, time 62.21ms
iter 3190: loss 0.9590, time 68.86ms
iter 3200: loss 0.9511, time 66.68ms
iter 3210: loss 0.9562, time 67.10ms
iter 3220: loss 0.9401, time 69.65ms
iter 3230: loss 0.9529, time 68.47ms
iter 3240: loss 0.9544, time 67.36ms
step 3250: train loss 0.8161, val loss 1.5495
iter 3250: loss 0.9408, time 20523.26ms
iter 3260: loss 0.9469, time 76.96ms
iter 3270: loss 0.9385, time 82.88ms
iter 3280: loss 0.9311, time 75.02ms
iter 3290: loss 0.9567, time 69.63ms
iter 3300: loss 0.9587, time 75.96ms
iter 3310: loss 0.9661, time 63.48ms
iter 3320: loss 0.9076, time 60.77ms
iter 3330: loss 0.9506, time 61.62ms
iter 3340: loss 0.9713, time 65.31ms
iter 3350: loss 0.9428, time 68.45ms
iter 3360: loss 0.9633, time 62.67ms
iter 3370: loss 0.9414, time 63.25ms
iter 3380: loss 0.9262, time 65.36ms
iter 3390: loss 0.9244, time 71.08ms
iter 3400: loss 0.9761, time 62.69ms
iter 3410: loss 0.9695, time 64.33ms
iter 3420: loss 0.9221, time 72.16ms
iter 3430: loss 0.9244, time 67.81ms
iter 3440: loss 0.9475, time 67.47ms
iter 3450: loss 0.9549, time 65.66ms
iter 3460: loss 0.9406, time 69.22ms
iter 3470: loss 0.9256, time 66.32ms
iter 3480: loss 0.9070, time 66.69ms
iter 3490: loss 0.9378, time 66.54ms
step 3500: train loss 0.7750, val loss 1.5709
iter 3500: loss 0.9045, time 20873.57ms
iter 3510: loss 0.9177, time 68.14ms
iter 3520: loss 0.9397, time 66.16ms
iter 3530: loss 0.9294, time 65.39ms
iter 3540: loss 0.9225, time 66.42ms
iter 3550: loss 0.9316, time 69.61ms
iter 3560: loss 0.9261, time 69.79ms
iter 3570: loss 0.9257, time 71.88ms
iter 3580: loss 0.9346, time 69.67ms
iter 3590: loss 0.9054, time 76.18ms
iter 3600: loss 0.9357, time 67.91ms
iter 3610: loss 0.9091, time 65.31ms
iter 3620: loss 0.9109, time 71.74ms
iter 3630: loss 0.9155, time 64.57ms
iter 3640: loss 0.9384, time 73.38ms
iter 3650: loss 0.9001, time 62.65ms
iter 3660: loss 0.9166, time 66.00ms
iter 3670: loss 0.9087, time 64.17ms
iter 3680: loss 0.8986, time 70.23ms
iter 3690: loss 0.9243, time 70.12ms
iter 3700: loss 0.9237, time 62.38ms
iter 3710: loss 0.9207, time 68.79ms
iter 3720: loss 0.8964, time 63.70ms
iter 3730: loss 0.9106, time 64.71ms
iter 3740: loss 0.8845, time 67.56ms
step 3750: train loss 0.7359, val loss 1.6072
iter 3750: loss 0.9444, time 20748.66ms
iter 3760: loss 0.9024, time 76.30ms
iter 3770: loss 0.8955, time 60.14ms
iter 3780: loss 0.9076, time 64.37ms
iter 3790: loss 0.9042, time 66.76ms
iter 3800: loss 0.9134, time 66.95ms
iter 3810: loss 0.8851, time 71.33ms
iter 3820: loss 0.8671, time 69.92ms
iter 3830: loss 0.8735, time 64.72ms
iter 3840: loss 0.9130, time 63.19ms
iter 3850: loss 0.8604, time 59.37ms
iter 3860: loss 0.9035, time 65.14ms
iter 3870: loss 0.8751, time 75.89ms
iter 3880: loss 0.8750, time 65.59ms
iter 3890: loss 0.9140, time 62.78ms
iter 3900: loss 0.8670, time 61.41ms
iter 3910: loss 0.8914, time 70.54ms
iter 3920: loss 0.8866, time 62.44ms
iter 3930: loss 0.8640, time 60.95ms
iter 3940: loss 0.8728, time 68.21ms
iter 3950: loss 0.9066, time 63.04ms
iter 3960: loss 0.8811, time 66.92ms
iter 3970: loss 0.8837, time 69.71ms
iter 3980: loss 0.8840, time 76.09ms
iter 3990: loss 0.8822, time 66.55ms
step 4000: train loss 0.7005, val loss 1.6316
iter 4000: loss 0.8669, time 20779.49ms
iter 4010: loss 0.8582, time 68.75ms
iter 4020: loss 0.8719, time 69.84ms
iter 4030: loss 0.9061, time 65.21ms
iter 4040: loss 0.8892, time 74.93ms
iter 4050: loss 0.8753, time 63.22ms
iter 4060: loss 0.8687, time 66.43ms
iter 4070: loss 0.8610, time 64.64ms
iter 4080: loss 0.8523, time 84.98ms
iter 4090: loss 0.8656, time 74.91ms
iter 4100: loss 0.8444, time 65.75ms
iter 4110: loss 0.8684, time 79.93ms
iter 4120: loss 0.8772, time 68.18ms
iter 4130: loss 0.8602, time 67.24ms
iter 4140: loss 0.8588, time 69.94ms
iter 4150: loss 0.8701, time 61.96ms
iter 4160: loss 0.8485, time 67.80ms
iter 4170: loss 0.8734, time 79.67ms
iter 4180: loss 0.8705, time 73.48ms
iter 4190: loss 0.8473, time 70.67ms
iter 4200: loss 0.8472, time 65.23ms
iter 4210: loss 0.8452, time 69.45ms
iter 4220: loss 0.8606, time 79.44ms
iter 4230: loss 0.8621, time 84.42ms
iter 4240: loss 0.8520, time 65.07ms
step 4250: train loss 0.6726, val loss 1.6461
iter 4250: loss 0.8617, time 20596.46ms
iter 4260: loss 0.8740, time 64.28ms
iter 4270: loss 0.8516, time 67.04ms
iter 4280: loss 0.8646, time 64.01ms
iter 4290: loss 0.8257, time 71.75ms
iter 4300: loss 0.8512, time 70.51ms
iter 4310: loss 0.8360, time 65.29ms
iter 4320: loss 0.8758, time 69.07ms
iter 4330: loss 0.8502, time 67.55ms
iter 4340: loss 0.8264, time 75.14ms
iter 4350: loss 0.8375, time 63.38ms
iter 4360: loss 0.8251, time 69.01ms
iter 4370: loss 0.8374, time 61.19ms
iter 4380: loss 0.8353, time 59.28ms
iter 4390: loss 0.8595, time 62.99ms
iter 4400: loss 0.8472, time 67.81ms
iter 4410: loss 0.8509, time 70.77ms
iter 4420: loss 0.8532, time 81.73ms
iter 4430: loss 0.8357, time 66.09ms
iter 4440: loss 0.8535, time 68.24ms
iter 4450: loss 0.8381, time 71.13ms
iter 4460: loss 0.8294, time 66.18ms
iter 4470: loss 0.8654, time 64.19ms
iter 4480: loss 0.8218, time 62.87ms
iter 4490: loss 0.8567, time 74.32ms
step 4500: train loss 0.6469, val loss 1.6723
iter 4500: loss 0.8395, time 21342.85ms
iter 4510: loss 0.8518, time 69.61ms
iter 4520: loss 0.8269, time 69.59ms
iter 4530: loss 0.8668, time 70.39ms
iter 4540: loss 0.8335, time 67.19ms
iter 4550: loss 0.8681, time 70.07ms
iter 4560: loss 0.8457, time 62.37ms
iter 4570: loss 0.8469, time 68.55ms
iter 4580: loss 0.8148, time 72.23ms
iter 4590: loss 0.8266, time 76.73ms
iter 4600: loss 0.8306, time 75.90ms
iter 4610: loss 0.8164, time 68.42ms
iter 4620: loss 0.8180, time 71.25ms
iter 4630: loss 0.8364, time 62.55ms
iter 4640: loss 0.8534, time 65.91ms
iter 4650: loss 0.8268, time 82.32ms
iter 4660: loss 0.8403, time 67.64ms
iter 4670: loss 0.8338, time 65.25ms
iter 4680: loss 0.8485, time 64.96ms
iter 4690: loss 0.8248, time 71.28ms
iter 4700: loss 0.8164, time 72.61ms
iter 4710: loss 0.8395, time 83.58ms
iter 4720: loss 0.8311, time 65.27ms
iter 4730: loss 0.8252, time 63.22ms
iter 4740: loss 0.8080, time 67.09ms
step 4750: train loss 0.6288, val loss 1.6801
iter 4750: loss 0.8294, time 20601.20ms
iter 4760: loss 0.8114, time 74.44ms
iter 4770: loss 0.8263, time 63.42ms
iter 4780: loss 0.8369, time 69.47ms
iter 4790: loss 0.8226, time 69.84ms
iter 4800: loss 0.8358, time 68.72ms
iter 4810: loss 0.8171, time 73.38ms
iter 4820: loss 0.8176, time 63.17ms
iter 4830: loss 0.8244, time 71.33ms
iter 4840: loss 0.8178, time 61.41ms
iter 4850: loss 0.8404, time 74.38ms
iter 4860: loss 0.8134, time 62.41ms
iter 4870: loss 0.8014, time 66.01ms
iter 4880: loss 0.8076, time 63.88ms
iter 4890: loss 0.7956, time 69.02ms
iter 4900: loss 0.8186, time 77.46ms
iter 4910: loss 0.8313, time 73.80ms
iter 4920: loss 0.8211, time 65.75ms
iter 4930: loss 0.8111, time 71.49ms
iter 4940: loss 0.8157, time 70.45ms
iter 4950: loss 0.8311, time 70.90ms
iter 4960: loss 0.8076, time 69.50ms
iter 4970: loss 0.8211, time 65.34ms
iter 4980: loss 0.8279, time 61.29ms
iter 4990: loss 0.7911, time 77.44ms
step 5000: train loss 0.6133, val loss 1.6959
iter 5000: loss 0.8099, time 20440.52ms
training done
Best validation loss: 1.4542663097381592
Total train time: 13.32 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 the converse that thou wast from me,
And wilt undo thee here for my son George?

WARWICK:
True, Clifford; and thus I hear, mine oath,
While he lies married then poison more than she:
But he hath forgot you thus; he looks not fools;
But the fires he must know her growing her brothers.
Thus was our forest sad, that they return'd us all;
And therefore I hear, this is here love.

KING RICHARD III:
My vow is well three that Richard is no honest.

QUEEN ELIZABETH:
Be patient, gentle Richard: I am afra
Inference time: 1.88 seconds
Tokens per second: 266.39
---------------
Sample 2:
 away with me.

First Citizen:
We have as little poison as the region we have
sent to Rome, whose storms and compounds in prayers
from the people, and not give us our country: they are
therefore follows for Rome, still in the very time the
first way to east.

All Citizens:
I think he is not too much my fast offence; but I
had rather left the man than it would continue. But, he hath
he made fished him out at secret a mind of the vice: he
hath deserved him very note to be another deed before him.


Inference time: 1.87 seconds
Tokens per second: 267.55
---------------
Sample 3:
 of late:
The consent first friends hath made them dead;
And thou hadst been no more amongst the land:
There is no other fifteen years and tears,
But to watch the so rough as they may purchase;
They may practise their suits at the heels,
And their taking it unto a sudden calf.

KING HENRY VI:
I will not slander him in my wits,
And thee, for I shall become you this man.

LADY ANNE:
Would the wrench with him that he would slaughter him,
And shut him with this sweet windows by him,
When he touches t
Inference time: 1.94 seconds
Tokens per second: 258.15
---------------
Sample 4:
 not be so.

CLAUDIO:
Not so hot; but to stand one stones for the world.

ISABELLA:
That is the world by me?

CLAUDIO:
The world is grown to be a comfort plaint:
The most inheritable cannot be my wish;
It should be so reasonable:
'Tis rise,' quoth she the crown to lie and rise,
'This shall be a suitor's head;' be the people's matter,
With his rest solace, his piping state
And in the view of his finger suspicion:
His virtue is out, and with others government,
This live before his true intercession
Inference time: 1.94 seconds
Tokens per second: 258.32
---------------
Sample 5:
 this, the tongueless shall see
The cloister of the content of the hand,
Which he moves me worse on the gods. I'll to thee,
To see you think so tires in thy strength,
Never tell I deeds, nor shalt thou lie
That honourable mother that thou hast so troop
Unto a sudden state, though my wife I will unfold
Wherein I was a-doing. As old true men,
I must weep: the deed was married in!

ANTIGONUS:
And merried me, daughter, I beseech you,
If you had not a disgrace between an hour,
For the steed I have str
Inference time: 1.95 seconds
Tokens per second: 256.84
---------------
Sample 6:
 you, sir, and not some promise to you
out of 'y.

Shepherd:
I must confess it. I have forgot you: and you
being true, and your fellows are at ends your
themes of tribune, and I fear you, and save't your
favour.

Clown:
Well, sir. Why, ho! What's your will?

AUTOLYCUS:
Nay, my dream for me: I will practise it as before him: I
have a better fourteen than any one there Marcius; you have a hanger.

AUTOLYCUS:
He has the better than a man content to him
the large.

Clown:
I have not heard him say so.
Inference time: 1.99 seconds
Tokens per second: 251.14
---------------
Sample 7:
 the goose?

GLOUCESTER:
Nay, then, I pray; a restraint to them,
And like them far to crown their father.

LADY ANNE:
I will not do me well, then, thy lord.

GLOUCESTER:
Go, some of you with me, and do you think.

LADY ANNE:
The way is already. I pray you, let us go.

GLOUCESTER:
I take my leave to go to thy short back was I ware:
And here I come to take the widow's vow;
And so I would the heavens cannot be cure,
That forsworn his bare event.

GLOUCESTER:
Clarence, let us to our book: but what co
Inference time: 1.92 seconds
Tokens per second: 260.56
---------------
Sample 8:
 and profane and them
To give age us no less than the gods. The hands
Of good Camillo was so strange of his
wrongs and commended to his steads, which wrinkled him we
put on the interruption of the smallest should bear himself
all too countenance: these, these two fair promises
Were a gentleman of them, if untimely they still
Their companions and fair enpierce against their place,
And leave the sun not advanced in them of my fiery
The very house of love.

CAMILLO:
Why, then then, my lord,
I may be
Inference time: 2.02 seconds
Tokens per second: 247.77
---------------
Sample 9:
 and from thy success,
Whose tongue may do contempt on thy name;
And all the base earth is the wisest conjured.
What! lack I do not know what you say.
Now, sir, what news? What's with your tent?

STANLEY:
O sir, I crave no further, but so die.

DUKE OF YORK:
Fie, fiend! thou wretch, I say thou art so well:
I am the king; and I send thee these news,--

DUKE OF AUMERLE:
Well said, my master, is well of the net?

HENRY PERCY:
'Tis the matter: bethink you not passing me.

DUKE OF AUMERLE:
Excepting t
Inference time: 1.95 seconds
Tokens per second: 256.54
---------------
Sample 10:
 not the shame?

CAMILLO:
Nay, but I know 'tis well: he shall not come to
thee.

AUTOLYCUS:
I talk not of the lark, sir.

Clown:
Not amillion, though 'twere the point of eyes or two
sufficiently had been too much fools. There is true, sir, he
should take it at home for his business.

AUTOLYCUS:
Why, how now, father!

Clown:
How now, sir! what's the news in his time of me?

AUTOLYCUS:
Go to hear him overheard the matter.

Clown:
Come, sir.

AUTOLYCUS:

Clown:
Hang here! nothing but the way, sir, t
Inference time: 1.92 seconds
Tokens per second: 260.65
---------------
Average tokens per second: 258.39
tokens per iteration will be: 8,192
found vocab_size = 205 (inside ../../../data/enwik8/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.70M
num decayed parameter tensors: 26, with 10,793,856 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 5.3185, val loss 5.3184
iter 0: loss 5.3229, time 83738.43ms
iter 100: loss 2.8816, time 135.05ms
iter 200: loss 2.6490, time 145.35ms
iter 300: loss 2.5464, time 143.82ms
iter 400: loss 2.4109, time 132.06ms
iter 500: loss 2.3459, time 135.27ms
iter 600: loss 2.1648, time 124.56ms
iter 700: loss 2.1489, time 131.12ms
iter 800: loss 2.0720, time 127.52ms
iter 900: loss 1.9554, time 127.03ms
step 1000: train loss 1.8130, val loss 1.8189
iter 1000: loss 1.9608, time 53212.30ms
iter 1100: loss 1.8818, time 122.62ms
iter 1200: loss 1.8392, time 133.34ms
iter 1300: loss 1.8030, time 126.71ms
iter 1400: loss 1.6582, time 126.74ms
iter 1500: loss 1.7802, time 122.80ms
iter 1600: loss 1.6312, time 141.46ms
iter 1700: loss 1.6904, time 125.42ms
iter 1800: loss 1.5757, time 132.13ms
iter 1900: loss 1.5505, time 130.74ms
step 2000: train loss 1.4615, val loss 1.4674
iter 2000: loss 1.5449, time 53321.17ms
iter 2100: loss 1.5820, time 135.20ms
iter 2200: loss 1.4545, time 134.55ms
iter 2300: loss 1.3908, time 131.44ms
iter 2400: loss 1.5160, time 137.59ms
iter 2500: loss 1.4765, time 136.38ms
iter 2600: loss 1.5068, time 131.71ms
iter 2700: loss 1.4622, time 137.59ms
iter 2800: loss 1.5062, time 138.54ms
iter 2900: loss 1.4916, time 146.29ms
step 3000: train loss 1.3400, val loss 1.3561
iter 3000: loss 1.4437, time 56266.21ms
iter 3100: loss 1.4239, time 132.10ms
iter 3200: loss 1.3056, time 136.79ms
iter 3300: loss 1.4091, time 125.51ms
iter 3400: loss 1.3986, time 135.70ms
iter 3500: loss 1.3698, time 137.10ms
iter 3600: loss 1.3390, time 138.08ms
iter 3700: loss 1.4258, time 136.37ms
iter 3800: loss 1.4037, time 136.07ms
iter 3900: loss 1.3834, time 146.34ms
step 4000: train loss 1.2903, val loss 1.2985
iter 4000: loss 1.3377, time 54723.56ms
iter 4100: loss 1.3653, time 133.41ms
iter 4200: loss 1.2623, time 151.32ms
iter 4300: loss 1.3429, time 124.49ms
iter 4400: loss 1.4271, time 137.38ms
iter 4500: loss 1.3070, time 103.70ms
iter 4600: loss 1.3802, time 152.26ms
iter 4700: loss 1.3723, time 135.92ms
iter 4800: loss 1.3548, time 139.58ms
iter 4900: loss 1.2254, time 141.41ms
step 5000: train loss 1.2647, val loss 1.2679
iter 5000: loss 1.4006, time 58718.73ms
iter 5100: loss 1.3187, time 129.11ms
iter 5200: loss 1.2712, time 144.67ms
iter 5300: loss 1.3198, time 117.00ms
iter 5400: loss 1.3128, time 142.24ms
iter 5500: loss 1.3775, time 139.56ms
iter 5600: loss 1.2624, time 140.79ms
iter 5700: loss 1.3154, time 132.58ms
iter 5800: loss 1.2930, time 144.37ms
iter 5900: loss 1.3510, time 132.37ms
step 6000: train loss 1.2298, val loss 1.2397
iter 6000: loss 1.2738, time 55237.25ms
iter 6100: loss 1.3387, time 137.09ms
iter 6200: loss 1.3676, time 132.65ms
iter 6300: loss 1.2133, time 142.48ms
iter 6400: loss 1.2954, time 134.73ms
iter 6500: loss 1.3140, time 135.83ms
iter 6600: loss 1.2722, time 152.05ms
iter 6700: loss 1.3540, time 133.76ms
iter 6800: loss 1.2999, time 139.91ms
iter 6900: loss 1.2992, time 132.40ms
step 7000: train loss 1.2147, val loss 1.2303
iter 7000: loss 1.2726, time 53353.77ms
iter 7100: loss 1.2447, time 132.84ms
iter 7200: loss 1.1898, time 135.56ms
iter 7300: loss 1.3351, time 146.38ms
iter 7400: loss 1.2685, time 151.64ms
iter 7500: loss 1.3546, time 128.87ms
iter 7600: loss 1.2519, time 132.51ms
iter 7700: loss 1.2323, time 133.84ms
iter 7800: loss 1.3156, time 138.36ms
iter 7900: loss 1.2751, time 135.61ms
step 8000: train loss 1.1991, val loss 1.2096
iter 8000: loss 1.3319, time 52904.65ms
iter 8100: loss 1.2407, time 138.64ms
iter 8200: loss 1.2328, time 143.19ms
iter 8300: loss 1.2724, time 148.95ms
iter 8400: loss 1.1952, time 170.62ms
iter 8500: loss 1.3144, time 133.99ms
iter 8600: loss 1.2111, time 118.95ms
iter 8700: loss 1.2291, time 130.78ms
iter 8800: loss 1.2860, time 125.49ms
iter 8900: loss 1.1966, time 139.28ms
step 9000: train loss 1.1837, val loss 1.1951
iter 9000: loss 1.2173, time 54102.12ms
iter 9100: loss 1.2116, time 131.76ms
iter 9200: loss 1.1929, time 137.07ms
iter 9300: loss 1.2075, time 132.04ms
iter 9400: loss 1.2021, time 141.23ms
iter 9500: loss 1.2209, time 122.48ms
iter 9600: loss 1.3030, time 135.94ms
iter 9700: loss 1.2704, time 130.59ms
iter 9800: loss 1.2522, time 130.61ms
iter 9900: loss 1.2371, time 139.81msRun 1 timed out after 7200 seconds
To implement the adaptive weight initialization with a moving average of gradient norms, we will modify the             
_init_weights function in the GPT class. Here's how we can proceed for Run 1:                                           

 1 Add a mechanism to track the moving average of gradient norms.                                                       
 2 Adjust the weight initialization based on the moving average.                                                        

Let's implement these changes in experiment.py:                                                                         

experiment.py                                                                                                           

                                                                                                                        
 <<<<<<< SEARCH                                                                                                         
     def _init_weights(self, module):                                                                                   
         if isinstance(module, nn.Linear):                                                                              
             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)                                                   
             if module.bias is not None:                                                                                
                 torch.nn.init.zeros_(module.bias)                                                                      
         elif isinstance(module, nn.Embedding):                                                                         
             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)                                                   
 =======                                                                                                                
     def _init_weights(self, module):                                                                                   
         if isinstance(module, nn.Linear):                                                                              
             # Calculate the moving average of gradient norms                                                           
             grad_norms = []                                                                                            
             for p in module.parameters():                                                                              
                 if p.grad is not None:                                                                                 
                     grad_norms.append(p.grad.norm().item())                                                            
             if grad_norms:                                                                                             
                 moving_avg_grad_norm = np.mean(grad_norms[-10:])  # Window size of 10                                  
                 std = 0.02 * moving_avg_grad_norm                                                                      
             else:                                                                                                      
                 std = 0.02                                                                                             
             torch.nn.init.normal_(module.weight, mean=0.0, std=std)                                                    
             if module.bias is not None:                                                                                
                 torch.nn.init.zeros_(module.bias)                                                                      
         elif isinstance(module, nn.Embedding):                                                                         
             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)                                                   
 >>>>>>> REPLACE                                                                                                        
                                                                                                                        

After applying these changes, you can run the experiment with the following command:                                    

                                                                                                                        
 python experiment.py --out_dir=run_1                                                                                   
                                                                                                                        

This will execute the first run with the adaptive weight initialization using a moving average window size of 10. Let me
know when you're ready for the next step!                                                                               

Tokens: 11k sent, 422 received. Cost: $0.03 message, $0.06 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_1
To implement the adaptive weight initialization with a moving average of gradient norms, we will modify the `_init_weights` function in the `GPT` class. Here's how we can proceed for **Run 1**:

1. Add a mechanism to track the moving average of gradient norms.
2. Adjust the weight initialization based on the moving average.

Let's implement these changes in `experiment.py`:

experiment.py
```python
<<<<<<< SEARCH
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
=======
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            # Calculate the moving average of gradient norms
            grad_norms = []
            for p in module.parameters():
                if p.grad is not None:
                    grad_norms.append(p.grad.norm().item())
            if grad_norms:
                moving_avg_grad_norm = np.mean(grad_norms[-10:])  # Window size of 10
                std = 0.02 * moving_avg_grad_norm
            else:
                std = 0.02
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
>>>>>>> REPLACE
```

After applying these changes, you can run the experiment with the following command:

```bash
python experiment.py --out_dir=run_1
```

This will execute the first run with the adaptive weight initialization using a moving average window size of 10. Let me know when you're ready for the next step!
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2874, val loss 4.2823
iter 0: loss 4.2654, time 27960.83ms
iter 10: loss 3.2458, time 61.79ms
iter 20: loss 2.7914, time 66.94ms
iter 30: loss 2.6356, time 66.05ms
iter 40: loss 2.5776, time 65.54ms
iter 50: loss 2.5277, time 60.75ms
iter 60: loss 2.5195, time 66.72ms
iter 70: loss 2.4970, time 67.75ms
iter 80: loss 2.4973, time 65.26ms
iter 90: loss 2.4693, time 67.05ms
iter 100: loss 2.4582, time 71.91ms
iter 110: loss 2.4614, time 67.96ms
iter 120: loss 2.4258, time 63.65ms
iter 130: loss 2.4145, time 64.81ms
iter 140: loss 2.4144, time 63.70ms
iter 150: loss 2.4148, time 65.64ms
iter 160: loss 2.3687, time 71.20ms
iter 170: loss 2.3459, time 65.36ms
iter 180: loss 2.3218, time 64.07ms
iter 190: loss 2.2526, time 70.68ms
iter 200: loss 2.2059, time 59.69ms
iter 210: loss 2.1432, time 67.92ms
iter 220: loss 2.1450, time 67.36ms
iter 230: loss 2.0636, time 68.74ms
iter 240: loss 2.0795, time 63.88ms
step 250: train loss 1.9678, val loss 2.0625
iter 250: loss 2.0310, time 20453.25ms
iter 260: loss 1.9766, time 61.51ms
iter 270: loss 1.9813, time 68.83ms
iter 280: loss 1.9740, time 65.75ms
iter 290: loss 1.9192, time 64.81ms
iter 300: loss 1.9024, time 64.86ms
iter 310: loss 1.8660, time 63.06ms
iter 320: loss 1.8506, time 66.89ms
iter 330: loss 1.8220, time 62.93ms
iter 340: loss 1.7904, time 71.82ms
iter 350: loss 1.8228, time 63.31ms
iter 360: loss 1.7685, time 69.52ms
iter 370: loss 1.7422, time 65.64ms
iter 380: loss 1.7310, time 66.13ms
iter 390: loss 1.7270, time 61.99ms
iter 400: loss 1.7664, time 60.45ms
iter 410: loss 1.6978, time 63.27ms
iter 420: loss 1.7169, time 55.88ms
iter 430: loss 1.6927, time 66.92ms
iter 440: loss 1.6496, time 63.76ms
iter 450: loss 1.6487, time 67.21ms
iter 460: loss 1.6050, time 60.44ms
iter 470: loss 1.6402, time 65.36ms
iter 480: loss 1.6164, time 63.72ms
iter 490: loss 1.5982, time 63.29ms
step 500: train loss 1.5214, val loss 1.7229
iter 500: loss 1.5972, time 20702.23ms
iter 510: loss 1.6107, time 63.48ms
iter 520: loss 1.5956, time 63.00ms
iter 530: loss 1.5529, time 60.69ms
iter 540: loss 1.6144, time 61.16ms
iter 550: loss 1.5535, time 71.41ms
iter 560: loss 1.5616, time 76.05ms
iter 570: loss 1.5633, time 64.10ms
iter 580: loss 1.5277, time 64.93ms
iter 590: loss 1.4991, time 64.28ms
iter 600: loss 1.5152, time 67.90ms
iter 610: loss 1.5456, time 65.26ms
iter 620: loss 1.5294, time 62.29ms
iter 630: loss 1.5121, time 66.67ms
iter 640: loss 1.4679, time 71.61ms
iter 650: loss 1.5057, time 62.86ms
iter 660: loss 1.5085, time 69.08ms
iter 670: loss 1.4397, time 68.99ms
iter 680: loss 1.5095, time 63.90ms
iter 690: loss 1.4675, time 70.11ms
iter 700: loss 1.4808, time 63.15ms
iter 710: loss 1.4500, time 65.32ms
iter 720: loss 1.4450, time 65.48ms
iter 730: loss 1.4237, time 64.66ms
iter 740: loss 1.4279, time 68.24ms
step 750: train loss 1.3597, val loss 1.5803
iter 750: loss 1.4240, time 20647.62ms
iter 760: loss 1.4485, time 59.78ms
iter 770: loss 1.4269, time 70.90ms
iter 780: loss 1.4133, time 70.23ms
iter 790: loss 1.4113, time 68.17ms
iter 800: loss 1.4368, time 74.96ms
iter 810: loss 1.4145, time 68.80ms
iter 820: loss 1.4015, time 75.90ms
iter 830: loss 1.3896, time 62.43ms
iter 840: loss 1.4043, time 75.15ms
iter 850: loss 1.3938, time 68.19ms
iter 860: loss 1.3987, time 50.70ms
iter 870: loss 1.3996, time 49.46ms
iter 880: loss 1.3681, time 56.60ms
iter 890: loss 1.3875, time 62.12ms
iter 900: loss 1.3712, time 63.67ms
iter 910: loss 1.3189, time 68.41ms
iter 920: loss 1.3578, time 66.01ms
iter 930: loss 1.3593, time 61.83ms
iter 940: loss 1.3415, time 62.07ms
iter 950: loss 1.3547, time 63.95ms
iter 960: loss 1.3611, time 64.31ms
iter 970: loss 1.3561, time 63.14ms
iter 980: loss 1.3538, time 62.09ms
iter 990: loss 1.3428, time 70.15ms
step 1000: train loss 1.2707, val loss 1.5169
iter 1000: loss 1.3361, time 20168.52ms
iter 1010: loss 1.3360, time 69.39ms
iter 1020: loss 1.3168, time 66.32ms
iter 1030: loss 1.3358, time 64.52ms
iter 1040: loss 1.3536, time 69.60ms
iter 1050: loss 1.2938, time 68.18ms
iter 1060: loss 1.3344, time 71.16ms
iter 1070: loss 1.3343, time 69.49ms
iter 1080: loss 1.3365, time 61.24ms
iter 1090: loss 1.3571, time 63.47ms
iter 1100: loss 1.3207, time 66.13ms
iter 1110: loss 1.3030, time 65.50ms
iter 1120: loss 1.2959, time 67.47ms
iter 1130: loss 1.2864, time 65.82ms
iter 1140: loss 1.2952, time 64.53ms
iter 1150: loss 1.3088, time 68.19ms
iter 1160: loss 1.3306, time 68.35ms
iter 1170: loss 1.2977, time 65.45ms
iter 1180: loss 1.3209, time 69.64ms
iter 1190: loss 1.2690, time 60.96ms
iter 1200: loss 1.2932, time 69.03ms
iter 1210: loss 1.2655, time 65.16ms
iter 1220: loss 1.3101, time 58.17ms
iter 1230: loss 1.2960, time 63.84ms
iter 1240: loss 1.3025, time 62.62ms
step 1250: train loss 1.2046, val loss 1.4919
iter 1250: loss 1.2753, time 20328.18ms
iter 1260: loss 1.2857, time 65.33ms
iter 1270: loss 1.2639, time 65.46ms
iter 1280: loss 1.2539, time 65.05ms
iter 1290: loss 1.2859, time 66.66ms
iter 1300: loss 1.3044, time 63.56ms
iter 1310: loss 1.2407, time 63.54ms
iter 1320: loss 1.3039, time 68.79ms
iter 1330: loss 1.2714, time 69.54ms
iter 1340: loss 1.2986, time 64.64ms
iter 1350: loss 1.2538, time 65.93ms
iter 1360: loss 1.2829, time 64.21ms
iter 1370: loss 1.2572, time 64.60ms
iter 1380: loss 1.2645, time 67.81ms
iter 1390: loss 1.2520, time 65.36ms
iter 1400: loss 1.2581, time 63.14ms
iter 1410: loss 1.2495, time 63.72ms
iter 1420: loss 1.2726, time 67.90ms
iter 1430: loss 1.2453, time 69.57ms
iter 1440: loss 1.2532, time 65.94ms
iter 1450: loss 1.2293, time 73.42ms
iter 1460: loss 1.2371, time 67.28ms
iter 1470: loss 1.2290, time 63.88ms
iter 1480: loss 1.2196, time 68.07ms
iter 1490: loss 1.2372, time 65.55ms
step 1500: train loss 1.1552, val loss 1.4819
iter 1500: loss 1.1868, time 20278.08ms
iter 1510: loss 1.2394, time 60.98ms
iter 1520: loss 1.2316, time 68.28ms
iter 1530: loss 1.2617, time 70.43ms
iter 1540: loss 1.1955, time 65.25ms
iter 1550: loss 1.2361, time 68.86ms
iter 1560: loss 1.2065, time 61.42ms
iter 1570: loss 1.2376, time 62.70ms
iter 1580: loss 1.2088, time 62.68ms
iter 1590: loss 1.1912, time 72.68ms
iter 1600: loss 1.1981, time 67.41ms
iter 1610: loss 1.2305, time 73.36ms
iter 1620: loss 1.1840, time 68.33ms
iter 1630: loss 1.2112, time 66.46ms
iter 1640: loss 1.2032, time 69.31ms
iter 1650: loss 1.1814, time 76.14ms
iter 1660: loss 1.2178, time 75.68ms
iter 1670: loss 1.1975, time 87.78ms
iter 1680: loss 1.2021, time 67.75ms
iter 1690: loss 1.1997, time 67.50ms
iter 1700: loss 1.1798, time 74.71ms
iter 1710: loss 1.1753, time 59.66ms
iter 1720: loss 1.1861, time 68.72ms
iter 1730: loss 1.2029, time 72.97ms
iter 1740: loss 1.1689, time 60.26ms
step 1750: train loss 1.1044, val loss 1.4659
iter 1750: loss 1.1925, time 20650.51ms
iter 1760: loss 1.1964, time 64.51ms
iter 1770: loss 1.2037, time 65.82ms
iter 1780: loss 1.1933, time 68.84ms
iter 1790: loss 1.1982, time 78.73ms
iter 1800: loss 1.1801, time 75.44ms
iter 1810: loss 1.1614, time 71.52ms
iter 1820: loss 1.1711, time 67.96ms
iter 1830: loss 1.1744, time 73.55ms
iter 1840: loss 1.1615, time 66.23ms
iter 1850: loss 1.1619, time 66.12ms
iter 1860: loss 1.1734, time 65.02ms
iter 1870: loss 1.1470, time 65.99ms
iter 1880: loss 1.1862, time 62.90ms
iter 1890: loss 1.1796, time 66.12ms
iter 1900: loss 1.1357, time 67.75ms
iter 1910: loss 1.1760, time 71.89ms
iter 1920: loss 1.1707, time 69.44ms
iter 1930: loss 1.1496, time 70.56ms
iter 1940: loss 1.1315, time 68.06ms
iter 1950: loss 1.1431, time 57.25ms
iter 1960: loss 1.1528, time 74.68ms
iter 1970: loss 1.1546, time 70.49ms
iter 1980: loss 1.1569, time 80.43ms
iter 1990: loss 1.1539, time 68.00ms
step 2000: train loss 1.0581, val loss 1.4750
iter 2000: loss 1.1284, time 20418.98ms
iter 2010: loss 1.1268, time 69.26ms
iter 2020: loss 1.1185, time 80.56ms
iter 2030: loss 1.1603, time 66.66ms
iter 2040: loss 1.1471, time 61.33ms
iter 2050: loss 1.1193, time 68.60ms
iter 2060: loss 1.1062, time 75.73ms
iter 2070: loss 1.1336, time 68.82ms
iter 2080: loss 1.1295, time 62.95ms
iter 2090: loss 1.1365, time 64.89ms
iter 2100: loss 1.1400, time 64.03ms
iter 2110: loss 1.1306, time 70.94ms
iter 2120: loss 1.1349, time 64.45ms
iter 2130: loss 1.1440, time 59.51ms
iter 2140: loss 1.1415, time 65.07ms
iter 2150: loss 1.1295, time 64.96ms
iter 2160: loss 1.1487, time 62.25ms
iter 2170: loss 1.1347, time 59.07ms
iter 2180: loss 1.1173, time 66.45ms
iter 2190: loss 1.1110, time 59.79ms
iter 2200: loss 1.1256, time 68.64ms
iter 2210: loss 1.1156, time 69.20ms
iter 2220: loss 1.1226, time 65.97ms
iter 2230: loss 1.1230, time 64.08ms
iter 2240: loss 1.1293, time 72.52ms
step 2250: train loss 1.0150, val loss 1.4877
iter 2250: loss 1.1195, time 20149.32ms
iter 2260: loss 1.1098, time 68.21ms
iter 2270: loss 1.1325, time 75.10ms
iter 2280: loss 1.0955, time 72.74ms
iter 2290: loss 1.1413, time 65.30ms
iter 2300: loss 1.1216, time 66.40ms
iter 2310: loss 1.0915, time 71.08ms
iter 2320: loss 1.0986, time 61.40ms
iter 2330: loss 1.1014, time 70.00ms
iter 2340: loss 1.1144, time 66.91ms
iter 2350: loss 1.1077, time 62.53ms
iter 2360: loss 1.1162, time 64.40ms
iter 2370: loss 1.0941, time 73.01ms
iter 2380: loss 1.0844, time 66.11ms
iter 2390: loss 1.0779, time 68.40ms
iter 2400: loss 1.0861, time 72.04ms
iter 2410: loss 1.0743, time 66.95ms
iter 2420: loss 1.0861, time 64.33ms
iter 2430: loss 1.0556, time 65.35ms
iter 2440: loss 1.0643, time 64.27ms
iter 2450: loss 1.0725, time 59.54ms
iter 2460: loss 1.0925, time 62.54ms
iter 2470: loss 1.0942, time 63.49ms
iter 2480: loss 1.0925, time 64.89ms
iter 2490: loss 1.0596, time 68.92ms
step 2500: train loss 0.9642, val loss 1.4869
iter 2500: loss 1.0879, time 20291.57ms
iter 2510: loss 1.0700, time 67.82ms
iter 2520: loss 1.0477, time 66.17ms
iter 2530: loss 1.0545, time 67.05ms
iter 2540: loss 1.0612, time 63.59ms
iter 2550: loss 1.0755, time 64.57ms
iter 2560: loss 1.0637, time 66.87ms
iter 2570: loss 1.0750, time 61.07ms
iter 2580: loss 1.0818, time 62.80ms
iter 2590: loss 1.0649, time 63.91ms
iter 2600: loss 1.0759, time 61.87ms
iter 2610: loss 1.0528, time 65.80ms
iter 2620: loss 1.0512, time 62.14ms
iter 2630: loss 1.0284, time 66.67ms
iter 2640: loss 1.0455, time 67.95ms
iter 2650: loss 1.0687, time 66.47ms
iter 2660: loss 1.0470, time 67.82ms
iter 2670: loss 1.0248, time 64.86ms
iter 2680: loss 1.0556, time 68.59ms
iter 2690: loss 1.0547, time 66.29ms
iter 2700: loss 1.0229, time 68.06ms
iter 2710: loss 1.0501, time 62.64ms
iter 2720: loss 1.0446, time 68.26ms
iter 2730: loss 1.0629, time 68.96ms
iter 2740: loss 1.0202, time 74.15ms
step 2750: train loss 0.9159, val loss 1.5204
iter 2750: loss 1.0423, time 20387.47ms
iter 2760: loss 1.0285, time 89.41ms
iter 2770: loss 1.0275, time 67.08ms
iter 2780: loss 1.0279, time 62.47ms
iter 2790: loss 1.0376, time 66.93ms
iter 2800: loss 1.0138, time 66.36ms
iter 2810: loss 1.0507, time 66.72ms
iter 2820: loss 1.0233, time 68.54ms
iter 2830: loss 1.0387, time 77.32ms
iter 2840: loss 0.9944, time 71.18ms
iter 2850: loss 1.0310, time 79.00ms
iter 2860: loss 1.0240, time 71.91ms
iter 2870: loss 1.0116, time 65.88ms
iter 2880: loss 1.0381, time 71.45ms
iter 2890: loss 1.0179, time 70.10ms
iter 2900: loss 0.9931, time 77.49ms
iter 2910: loss 1.0367, time 67.58ms
iter 2920: loss 1.0185, time 59.13ms
iter 2930: loss 0.9962, time 66.13ms
iter 2940: loss 0.9863, time 67.57ms
iter 2950: loss 1.0255, time 65.54ms
iter 2960: loss 1.0019, time 67.05ms
iter 2970: loss 0.9954, time 72.60ms
iter 2980: loss 0.9971, time 69.05ms
iter 2990: loss 0.9878, time 65.45ms
step 3000: train loss 0.8701, val loss 1.5291
iter 3000: loss 0.9844, time 20198.24ms
iter 3010: loss 0.9958, time 69.60ms
iter 3020: loss 1.0050, time 69.24ms
iter 3030: loss 0.9994, time 65.16ms
iter 3040: loss 1.0288, time 69.15ms
iter 3050: loss 0.9760, time 65.04ms
iter 3060: loss 1.0041, time 69.36ms
iter 3070: loss 1.0227, time 66.37ms
iter 3080: loss 0.9966, time 63.13ms
iter 3090: loss 0.9856, time 65.93ms
iter 3100: loss 0.9941, time 61.83ms
iter 3110: loss 0.9735, time 66.10ms
iter 3120: loss 0.9985, time 62.95ms
iter 3130: loss 0.9814, time 65.41ms
iter 3140: loss 0.9804, time 66.97ms
iter 3150: loss 1.0025, time 67.48ms
iter 3160: loss 1.0088, time 63.05ms
iter 3170: loss 0.9527, time 69.67ms
iter 3180: loss 0.9782, time 64.15ms
iter 3190: loss 0.9915, time 69.62ms
iter 3200: loss 0.9724, time 69.45ms
iter 3210: loss 0.9686, time 69.95ms
iter 3220: loss 0.9592, time 70.35ms
iter 3230: loss 0.9564, time 63.89ms
iter 3240: loss 0.9586, time 68.67ms
step 3250: train loss 0.8264, val loss 1.5494
iter 3250: loss 0.9685, time 20676.46ms
iter 3260: loss 0.9651, time 71.84ms
iter 3270: loss 0.9747, time 66.20ms
iter 3280: loss 0.9502, time 67.72ms
iter 3290: loss 0.9466, time 71.28ms
iter 3300: loss 0.9389, time 69.90ms
iter 3310: loss 0.9516, time 66.41ms
iter 3320: loss 0.9648, time 74.16ms
iter 3330: loss 0.9603, time 67.84ms
iter 3340: loss 0.9669, time 63.32ms
iter 3350: loss 0.9614, time 65.41ms
iter 3360: loss 0.9283, time 74.32ms
iter 3370: loss 0.9695, time 65.58ms
iter 3380: loss 0.9573, time 74.87ms
iter 3390: loss 0.9524, time 70.77ms
iter 3400: loss 0.9506, time 70.65ms
iter 3410: loss 0.9430, time 63.29ms
iter 3420: loss 0.9506, time 66.11ms
iter 3430: loss 0.9546, time 65.54ms
iter 3440: loss 0.9805, time 73.55ms
iter 3450: loss 0.9533, time 69.94ms
iter 3460: loss 0.9487, time 69.52ms
iter 3470: loss 0.9465, time 71.85ms
iter 3480: loss 0.9541, time 66.62ms
iter 3490: loss 0.9199, time 65.93ms
step 3500: train loss 0.7869, val loss 1.5664
iter 3500: loss 0.9094, time 20312.72ms
iter 3510: loss 0.9190, time 73.53ms
iter 3520: loss 0.9235, time 64.30ms
iter 3530: loss 0.9560, time 75.50ms
iter 3540: loss 0.9365, time 63.31ms
iter 3550: loss 0.9307, time 62.96ms
iter 3560: loss 0.9575, time 62.75ms
iter 3570: loss 0.9393, time 66.88ms
iter 3580: loss 0.9352, time 71.42ms
iter 3590: loss 0.9299, time 63.86ms
iter 3600: loss 0.9275, time 67.02ms
iter 3610: loss 0.9201, time 66.79ms
iter 3620: loss 0.9160, time 65.02ms
iter 3630: loss 0.9290, time 68.96ms
iter 3640: loss 0.9211, time 75.40ms
iter 3650: loss 0.9223, time 63.95ms
iter 3660: loss 0.9391, time 74.39ms
iter 3670: loss 0.9424, time 63.42ms
iter 3680: loss 0.9158, time 69.37ms
iter 3690: loss 0.9339, time 70.29ms
iter 3700: loss 0.8772, time 64.59ms
iter 3710: loss 0.8854, time 59.72ms
iter 3720: loss 0.9068, time 68.09ms
iter 3730: loss 0.9040, time 66.38ms
iter 3740: loss 0.9091, time 65.70ms
step 3750: train loss 0.7443, val loss 1.5927
iter 3750: loss 0.9002, time 20459.75ms
iter 3760: loss 0.9406, time 72.09ms
iter 3770: loss 0.9322, time 69.81ms
iter 3780: loss 0.9215, time 66.67ms
iter 3790: loss 0.9028, time 77.40ms
iter 3800: loss 0.9174, time 65.24ms
iter 3810: loss 0.9285, time 66.47ms
iter 3820: loss 0.8920, time 65.03ms
iter 3830: loss 0.9010, time 79.21ms
iter 3840: loss 0.8889, time 64.82ms
iter 3850: loss 0.8933, time 61.57ms
iter 3860: loss 0.8802, time 72.03ms
iter 3870: loss 0.8886, time 73.35ms
iter 3880: loss 0.8917, time 69.10ms
iter 3890: loss 0.8957, time 67.66ms
iter 3900: loss 0.8916, time 60.12ms
iter 3910: loss 0.8971, time 73.59ms
iter 3920: loss 0.8767, time 72.78ms
iter 3930: loss 0.8934, time 72.26ms
iter 3940: loss 0.8734, time 62.94ms
iter 3950: loss 0.8876, time 67.74ms
iter 3960: loss 0.9111, time 70.80ms
iter 3970: loss 0.8916, time 65.57ms
iter 3980: loss 0.8986, time 67.79ms
iter 3990: loss 0.8810, time 59.25ms
step 4000: train loss 0.7132, val loss 1.6212
iter 4000: loss 0.8627, time 20292.47ms
iter 4010: loss 0.8742, time 66.38ms
iter 4020: loss 0.8914, time 66.19ms
iter 4030: loss 0.8836, time 68.36ms
iter 4040: loss 0.8802, time 61.97ms
iter 4050: loss 0.8754, time 72.79ms
iter 4060: loss 0.8649, time 64.99ms
iter 4070: loss 0.8685, time 64.52ms
iter 4080: loss 0.8918, time 63.33ms
iter 4090: loss 0.8470, time 64.05ms
iter 4100: loss 0.9010, time 60.71ms
iter 4110: loss 0.8757, time 67.35ms
iter 4120: loss 0.8818, time 64.72ms
iter 4130: loss 0.8673, time 71.44ms
iter 4140: loss 0.8885, time 65.12ms
iter 4150: loss 0.8775, time 63.58ms
iter 4160: loss 0.8522, time 69.31ms
iter 4170: loss 0.8580, time 68.04ms
iter 4180: loss 0.8689, time 72.53ms
iter 4190: loss 0.8781, time 68.32ms
iter 4200: loss 0.8529, time 63.39ms
iter 4210: loss 0.8715, time 72.08ms
iter 4220: loss 0.8621, time 64.50ms
iter 4230: loss 0.8896, time 61.59ms
iter 4240: loss 0.8683, time 62.22ms
step 4250: train loss 0.6825, val loss 1.6413
iter 4250: loss 0.8684, time 20626.46ms
iter 4260: loss 0.8610, time 68.19ms
iter 4270: loss 0.8731, time 64.16ms
iter 4280: loss 0.8582, time 71.68ms
iter 4290: loss 0.8375, time 67.72ms
iter 4300: loss 0.8362, time 66.87ms
iter 4310: loss 0.8625, time 66.68ms
iter 4320: loss 0.8387, time 60.46ms
iter 4330: loss 0.8545, time 65.97ms
iter 4340: loss 0.8376, time 65.93ms
iter 4350: loss 0.8401, time 70.49ms
iter 4360: loss 0.8606, time 71.94ms
iter 4370: loss 0.8615, time 66.57ms
iter 4380: loss 0.8380, time 64.60ms
iter 4390: loss 0.8700, time 67.66ms
iter 4400: loss 0.8509, time 72.58ms
iter 4410: loss 0.8593, time 68.29ms
iter 4420: loss 0.8745, time 64.96ms
iter 4430: loss 0.8433, time 65.95ms
iter 4440: loss 0.8505, time 63.68ms
iter 4450: loss 0.8425, time 67.09ms
iter 4460: loss 0.8498, time 67.27ms
iter 4470: loss 0.8500, time 67.42ms
iter 4480: loss 0.8345, time 66.01ms
iter 4490: loss 0.8469, time 65.73ms
step 4500: train loss 0.6559, val loss 1.6691
iter 4500: loss 0.8639, time 20746.65ms
iter 4510: loss 0.8539, time 80.32ms
iter 4520: loss 0.8331, time 72.53ms
iter 4530: loss 0.8551, time 64.99ms
iter 4540: loss 0.8501, time 64.60ms
iter 4550: loss 0.8753, time 69.18ms
iter 4560: loss 0.8433, time 66.30ms
iter 4570: loss 0.8511, time 68.15ms
iter 4580: loss 0.8627, time 67.62ms
iter 4590: loss 0.8569, time 69.74ms
iter 4600: loss 0.8300, time 62.47ms
iter 4610: loss 0.8647, time 59.90ms
iter 4620: loss 0.8322, time 52.89ms
iter 4630: loss 0.8281, time 67.09ms
iter 4640: loss 0.8466, time 67.82ms
iter 4650: loss 0.8623, time 64.21ms
iter 4660: loss 0.8444, time 50.79ms
iter 4670: loss 0.8339, time 56.85ms
iter 4680: loss 0.8580, time 51.47ms
iter 4690: loss 0.8422, time 63.47ms
iter 4700: loss 0.8281, time 71.18ms
iter 4710: loss 0.7959, time 68.82ms
iter 4720: loss 0.8380, time 70.45ms
iter 4730: loss 0.8193, time 66.44ms
iter 4740: loss 0.8371, time 73.21ms
step 4750: train loss 0.6407, val loss 1.6766
iter 4750: loss 0.8018, time 20225.28ms
iter 4760: loss 0.8227, time 73.57ms
iter 4770: loss 0.7991, time 80.42ms
iter 4780: loss 0.8129, time 67.38ms
iter 4790: loss 0.8350, time 68.23ms
iter 4800: loss 0.8300, time 74.52ms
iter 4810: loss 0.8405, time 59.31ms
iter 4820: loss 0.8269, time 62.60ms
iter 4830: loss 0.8250, time 69.10ms
iter 4840: loss 0.8348, time 81.88ms
iter 4850: loss 0.8226, time 71.26ms
iter 4860: loss 0.8213, time 64.09ms
iter 4870: loss 0.8130, time 62.72ms
iter 4880: loss 0.8341, time 69.81ms
iter 4890: loss 0.8058, time 64.35ms
iter 4900: loss 0.8105, time 63.02ms
iter 4910: loss 0.8216, time 65.11ms
iter 4920: loss 0.8224, time 66.97ms
iter 4930: loss 0.8087, time 66.12ms
iter 4940: loss 0.8075, time 65.91ms
iter 4950: loss 0.8377, time 61.68ms
iter 4960: loss 0.8396, time 72.89ms
iter 4970: loss 0.7872, time 61.88ms
iter 4980: loss 0.7963, time 67.01ms
iter 4990: loss 0.8230, time 59.10ms
step 5000: train loss 0.6249, val loss 1.6944
iter 5000: loss 0.8272, time 20154.26ms
training done
Best validation loss: 1.4659476280212402
Total train time: 12.94 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 beauty;
And yet to be still all so well as if that thou.

GLOUCESTER:
Uncle, this is no remedy.

LADY ANNE:
Foul saints discontented upon thy speech,
And despite of thy sleep disobedience:
England that, thou know'st no cause to be the field.

GLOUCESTER:
But is the Tower; and therefore be proclaim'd
For Edward's sake, thou mayst provide
The truth of his high presence and my soul sister.

KING EDWARD IV:
I was a true good time to thee gave my dew.

LADY GREY:
I'll not leave you, good morrow, my l
Inference time: 1.97 seconds
Tokens per second: 254.14
---------------
Sample 2:
 such ever
As is the entrails of our second charge,
Is all the policy of the world.

ISABELLA:
Saw you her? I have sworn to your brother's sister
To save this my brother's heart with you. How fares you?

ISABELLA:
I pray thee, good sir, good brother:
I am already.

DUKE VINCENTIO:
For this we did lose you, sir.

LUCIO:
He knows not, Pompey; your bum of it is a worse:
The duke and the prick of it your charge, I say.

DUKE VINCENTIO:
Peace, Pompey, hold! home you then, even.

LUCIO:
How! no more! n
Inference time: 1.86 seconds
Tokens per second: 268.60
---------------
Sample 3:
 bastard like a king: she will be closed
To bid her brothers, are kind of fruit.

WARWICK:
In despite of love, so that I inward her.

KING HENRY VI:
What should you were here, fair queen?

QUEEN MARGARET:
Now tell me, where art thou worthy brother's love?

WARWICK:
Then, be Duke of Gloucester, enjoy the realm.

KING EDWARD IV:
Now, brother of Clarence, is Derbydy Green?
And Warwick, and thus for our company Warwick?

WARWICK:
Suppose, my lord, they shall be convey'd
Unto the heat arm of Henry Bol
Inference time: 1.85 seconds
Tokens per second: 270.81
---------------
Sample 4:
 let embrace me,
Who follow'd me in the noble duke my brother;
And all my bosom prescription to my brother.

KING EDWARD IV:
Infer that birds your grace be in my soul:
And what shall we say, it shall be so done:
Since we have it, if ever yet the nobility
Have we put up to do that have him in such a guest
seven of the most shepherd, which I have said,
Which five his death, that is not she appeared:
Which will he jest on with our feeding with
the swelling of the dearest stones?

Second Gentleman:
W
Inference time: 1.99 seconds
Tokens per second: 251.66
---------------
Sample 5:
 will never come to bed.

First Citizen:
What, was he that would beween the world with thee?

MENENIUS:
No, I'll go along alone.

BRUTUS:
Well they are all in almost persons they weigh.

CORIOLANUS:
And so will serve: the people must have
But themselves as they have fight a three-man-meaning days
Before they are quickly their thing; which I out of them,
Without mercy their thief, with their their eyes
With their suffering lawful slip; their creeping
Made a party of praises to maintain paper scold
Inference time: 1.81 seconds
Tokens per second: 275.70
---------------
Sample 6:
 my father's son,
She will she kneel and read that die stone which I tream.
What, is my bonnet what makes this matter?
Give me my sorrow! my mother, nurse, some of you.

Nurse:

JULIET:
Ay me! an of any such a man I can.

Nurse:
Yes, I am going to the wall.

JULIET:
Yes, good friend; I'll tell you what from hence,
As well as the complexion where I may perceive:
But, since hath he had any here deliver'd my
By her transgression to a life, and as her any
The anger of Caius shall be enter'd up.
Mark 
Inference time: 1.88 seconds
Tokens per second: 265.64
---------------
Sample 7:
 with a man that blows thee
Dare ne'er sit upon thy love; though thy father bids,
And beg thee the boy of kings, though they find
The sea true king's dear. But if thou comest,
The soldiers shall post to kill thee with a prayer
That hath all the hostess of thy son: I cannot speak
Thou canst be gone and tell thee now; and so I think
Thou prefer him that I should be dispatch'd withal
To say that I will see thee truth: but when
Thou darest not, so we knowing us not, if thou darest.

Lord:
We have so;
Inference time: 1.84 seconds
Tokens per second: 272.25
---------------
Sample 8:
 do it before 't as good as becomes
Like a good napy, then so still the chaff,
When I have borne the sweeter shove or no,
Or with justice, so I take myself
To stay to the like and fortunes and dark her stars,
To greet mine own good courtesy to the Buckingham,
Who but such a miserable gentleman
Environed age me by the body of the absent moon,
Who hath pass'd the designation of the face
Enveloped of the was easy foreign proceeding;
Or else have set up the land anointed liver
Of the hell-spoke o' th
Inference time: 1.88 seconds
Tokens per second: 265.61
---------------
Sample 9:
 slanderous leave,
The manner of the gain which in the north,
The palm of treachery fines; let me have
And pain to take our pretty town of sin:
So we defend thee with thy state and these winds
That would speak the severity of thy father's death,
And straight his passage for this execution.
What doth he until recovery of this place?
Hail, no! methinks, hast thou slept: Thou hast said is done,
Honourable stays and full of weeds? When will I to help,
If I should kill thee particular with my father's
Inference time: 1.85 seconds
Tokens per second: 270.17
---------------
Sample 10:
 mine, my liege, in those own golden hate,
And I leave thee my wife, here I came.

GLOUCESTER:
My most words and longer, as thou wilt die.
Go, boy, thou wilt me unto the Tower.
But, farewell: where is the provost true knights.

BUCKINGHAM:
True, my gracious lord, I'll kiss you with sorrow,
When I married the Tower, ere we sleep it so.

BUCKINGHAM:
Go, go, go, up to the Duke of Norfolk say.

KING RICHARD III:
'Twas proud that I should wish him for him:
That which I use will command my knees which 
Inference time: 1.89 seconds
Tokens per second: 264.40
---------------
Average tokens per second: 265.90
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2372, val loss 4.2295
iter 0: loss 4.2337, time 21405.56ms
iter 10: loss 3.2253, time 62.80ms
iter 20: loss 2.7688, time 66.11ms
iter 30: loss 2.6201, time 65.45ms
iter 40: loss 2.5385, time 69.58ms
iter 50: loss 2.5372, time 60.72ms
iter 60: loss 2.4992, time 68.35ms
iter 70: loss 2.4938, time 70.86ms
iter 80: loss 2.4955, time 71.97ms
iter 90: loss 2.4795, time 67.47ms
iter 100: loss 2.4689, time 63.54ms
iter 110: loss 2.4338, time 81.59ms
iter 120: loss 2.4508, time 71.08ms
iter 130: loss 2.4341, time 69.48ms
iter 140: loss 2.4270, time 60.82ms
iter 150: loss 2.3597, time 66.85ms
iter 160: loss 2.3868, time 73.34ms
iter 170: loss 2.3377, time 62.15ms
iter 180: loss 2.3322, time 63.07ms
iter 190: loss 2.2864, time 66.62ms
iter 200: loss 2.2393, time 66.62ms
iter 210: loss 2.1677, time 66.83ms
iter 220: loss 2.1193, time 64.83ms
iter 230: loss 2.0854, time 66.72ms
iter 240: loss 2.0634, time 66.76ms
step 250: train loss 1.9746, val loss 2.0783
iter 250: loss 2.0287, time 20546.33ms
iter 260: loss 2.0147, time 81.94ms
iter 270: loss 1.9486, time 66.86ms
iter 280: loss 1.9411, time 64.52ms
iter 290: loss 1.9174, time 67.63ms
iter 300: loss 1.9049, time 72.50ms
iter 310: loss 1.9037, time 65.22ms
iter 320: loss 1.8525, time 69.27ms
iter 330: loss 1.8627, time 69.27ms
iter 340: loss 1.7927, time 68.31ms
iter 350: loss 1.7848, time 69.29ms
iter 360: loss 1.7898, time 65.17ms
iter 370: loss 1.7794, time 68.76ms
iter 380: loss 1.7505, time 59.43ms
iter 390: loss 1.7360, time 65.10ms
iter 400: loss 1.7082, time 66.07ms
iter 410: loss 1.7108, time 70.58ms
iter 420: loss 1.7086, time 71.00ms
iter 430: loss 1.6915, time 62.70ms
iter 440: loss 1.6585, time 70.79ms
iter 450: loss 1.6380, time 65.44ms
iter 460: loss 1.6801, time 63.61ms
iter 470: loss 1.6485, time 61.37ms
iter 480: loss 1.6601, time 68.16ms
iter 490: loss 1.6051, time 71.37ms
step 500: train loss 1.5393, val loss 1.7331
iter 500: loss 1.6157, time 20141.92ms
iter 510: loss 1.6089, time 64.33ms
iter 520: loss 1.5703, time 75.69ms
iter 530: loss 1.6017, time 65.58ms
iter 540: loss 1.5530, time 71.63ms
iter 550: loss 1.5579, time 63.03ms
iter 560: loss 1.5520, time 70.78ms
iter 570: loss 1.5771, time 65.47ms
iter 580: loss 1.5180, time 69.11ms
iter 590: loss 1.5406, time 74.80ms
iter 600: loss 1.5078, time 68.02ms
iter 610: loss 1.5402, time 66.98ms
iter 620: loss 1.4929, time 69.22ms
iter 630: loss 1.5426, time 62.63ms
iter 640: loss 1.4630, time 64.17ms
iter 650: loss 1.5020, time 64.71ms
iter 660: loss 1.4924, time 72.52ms
iter 670: loss 1.4878, time 73.05ms
iter 680: loss 1.4502, time 67.26ms
iter 690: loss 1.4670, time 66.48ms
iter 700: loss 1.4562, time 69.82ms
iter 710: loss 1.4862, time 66.79ms
iter 720: loss 1.4473, time 67.71ms
iter 730: loss 1.4533, time 62.25ms
iter 740: loss 1.4314, time 67.04ms
step 750: train loss 1.3679, val loss 1.5973
iter 750: loss 1.4464, time 20348.04ms
iter 760: loss 1.4001, time 64.96ms
iter 770: loss 1.4197, time 75.58ms
iter 780: loss 1.4267, time 70.34ms
iter 790: loss 1.3870, time 70.97ms
iter 800: loss 1.4183, time 63.49ms
iter 810: loss 1.4321, time 57.90ms
iter 820: loss 1.4107, time 67.28ms
iter 830: loss 1.4055, time 62.22ms
iter 840: loss 1.3962, time 67.84ms
iter 850: loss 1.4191, time 65.34ms
iter 860: loss 1.3851, time 68.91ms
iter 870: loss 1.4036, time 64.62ms
iter 880: loss 1.3593, time 53.05ms
iter 890: loss 1.3780, time 53.24ms
iter 900: loss 1.3573, time 60.71ms
iter 910: loss 1.3997, time 69.19ms
iter 920: loss 1.3722, time 65.09ms
iter 930: loss 1.3658, time 69.37ms
iter 940: loss 1.3802, time 61.35ms
iter 950: loss 1.3503, time 66.00ms
iter 960: loss 1.3868, time 63.28ms
iter 970: loss 1.3788, time 71.49ms
iter 980: loss 1.3721, time 68.58ms
iter 990: loss 1.3175, time 64.96ms
step 1000: train loss 1.2739, val loss 1.5199
iter 1000: loss 1.3424, time 20352.55ms
iter 1010: loss 1.3610, time 70.15ms
iter 1020: loss 1.3516, time 68.22ms
iter 1030: loss 1.3292, time 68.01ms
iter 1040: loss 1.2952, time 67.69ms
iter 1050: loss 1.2892, time 65.90ms
iter 1060: loss 1.3094, time 65.30ms
iter 1070: loss 1.3265, time 66.56ms
iter 1080: loss 1.3179, time 73.25ms
iter 1090: loss 1.3605, time 70.02ms
iter 1100: loss 1.2945, time 69.63ms
iter 1110: loss 1.3146, time 70.14ms
iter 1120: loss 1.2963, time 64.74ms
iter 1130: loss 1.3051, time 67.79ms
iter 1140: loss 1.2947, time 71.42ms
iter 1150: loss 1.2875, time 72.46ms
iter 1160: loss 1.2843, time 64.14ms
iter 1170: loss 1.2766, time 68.84ms
iter 1180: loss 1.2724, time 66.41ms
iter 1190: loss 1.3284, time 69.98ms
iter 1200: loss 1.2984, time 68.91ms
iter 1210: loss 1.2963, time 62.42ms
iter 1220: loss 1.3022, time 63.64ms
iter 1230: loss 1.2490, time 65.38ms
iter 1240: loss 1.2640, time 63.19ms
step 1250: train loss 1.2062, val loss 1.4961
iter 1250: loss 1.3026, time 20615.57ms
iter 1260: loss 1.2900, time 67.60ms
iter 1270: loss 1.2968, time 71.48ms
iter 1280: loss 1.2670, time 66.81ms
iter 1290: loss 1.2956, time 72.06ms
iter 1300: loss 1.3037, time 69.99ms
iter 1310: loss 1.2502, time 67.04ms
iter 1320: loss 1.2874, time 67.41ms
iter 1330: loss 1.2711, time 64.93ms
iter 1340: loss 1.2470, time 67.12ms
iter 1350: loss 1.2474, time 64.63ms
iter 1360: loss 1.2918, time 65.28ms
iter 1370: loss 1.2633, time 75.43ms
iter 1380: loss 1.2492, time 64.24ms
iter 1390: loss 1.2609, time 72.42ms
iter 1400: loss 1.2624, time 65.68ms
iter 1410: loss 1.2567, time 66.50ms
iter 1420: loss 1.2405, time 69.26ms
iter 1430: loss 1.2329, time 66.33ms
iter 1440: loss 1.2223, time 62.28ms
iter 1450: loss 1.2723, time 66.67ms
iter 1460: loss 1.2326, time 72.43ms
iter 1470: loss 1.2198, time 62.76ms
iter 1480: loss 1.2179, time 68.99ms
iter 1490: loss 1.2479, time 66.11ms
step 1500: train loss 1.1527, val loss 1.4761
iter 1500: loss 1.2446, time 20889.87ms
iter 1510: loss 1.2262, time 67.39ms
iter 1520: loss 1.2340, time 62.08ms
iter 1530: loss 1.2125, time 76.05ms
iter 1540: loss 1.2459, time 63.69ms
iter 1550: loss 1.2237, time 84.25ms
iter 1560: loss 1.2593, time 69.12ms
iter 1570: loss 1.2083, time 80.76ms
iter 1580: loss 1.1961, time 66.89ms
iter 1590: loss 1.1963, time 73.77ms
iter 1600: loss 1.2195, time 66.22ms
iter 1610: loss 1.1852, time 64.70ms
iter 1620: loss 1.2239, time 70.04ms
iter 1630: loss 1.2269, time 65.15ms
iter 1640: loss 1.2228, time 64.95ms
iter 1650: loss 1.1942, time 71.85ms
iter 1660: loss 1.1915, time 65.45ms
iter 1670: loss 1.2236, time 63.51ms
iter 1680: loss 1.1763, time 67.15ms
iter 1690: loss 1.1940, time 65.44ms
iter 1700: loss 1.1841, time 66.54ms
iter 1710: loss 1.1558, time 68.04ms
iter 1720: loss 1.1682, time 66.85ms
iter 1730: loss 1.1937, time 65.42ms
iter 1740: loss 1.1885, time 67.45ms
step 1750: train loss 1.1046, val loss 1.4749
iter 1750: loss 1.1911, time 20123.62ms
iter 1760: loss 1.2008, time 74.63ms
iter 1770: loss 1.1924, time 60.73ms
iter 1780: loss 1.1438, time 65.49ms
iter 1790: loss 1.1835, time 70.67ms
iter 1800: loss 1.1438, time 66.54ms
iter 1810: loss 1.1716, time 77.42ms
iter 1820: loss 1.1731, time 71.59ms
iter 1830: loss 1.1961, time 70.49ms
iter 1840: loss 1.1598, time 61.33ms
iter 1850: loss 1.1779, time 63.55ms
iter 1860: loss 1.2187, time 70.12ms
iter 1870: loss 1.1806, time 67.91ms
iter 1880: loss 1.1558, time 71.45ms
iter 1890: loss 1.1556, time 64.71ms
iter 1900: loss 1.1622, time 72.69ms
iter 1910: loss 1.1698, time 67.74ms
iter 1920: loss 1.1488, time 61.58ms
iter 1930: loss 1.1815, time 61.69ms
iter 1940: loss 1.1495, time 77.14ms
iter 1950: loss 1.1326, time 69.86ms
iter 1960: loss 1.1616, time 67.75ms
iter 1970: loss 1.1359, time 69.83ms
iter 1980: loss 1.1878, time 71.33ms
iter 1990: loss 1.1374, time 60.89ms
step 2000: train loss 1.0585, val loss 1.4848
iter 2000: loss 1.1302, time 19831.46ms
iter 2010: loss 1.1435, time 57.51ms
iter 2020: loss 1.1271, time 63.53ms
iter 2030: loss 1.1434, time 66.28ms
iter 2040: loss 1.1385, time 58.18ms
iter 2050: loss 1.1159, time 60.41ms
iter 2060: loss 1.1121, time 71.60ms
iter 2070: loss 1.1448, time 64.14ms
iter 2080: loss 1.1100, time 64.95ms
iter 2090: loss 1.1278, time 62.83ms
iter 2100: loss 1.1268, time 57.32ms
iter 2110: loss 1.1391, time 73.88ms
iter 2120: loss 1.1498, time 66.83ms
iter 2130: loss 1.1339, time 61.99ms
iter 2140: loss 1.1084, time 68.02ms
iter 2150: loss 1.1108, time 67.51ms
iter 2160: loss 1.1026, time 61.74ms
iter 2170: loss 1.1051, time 67.72ms
iter 2180: loss 1.1199, time 65.31ms
iter 2190: loss 1.1482, time 65.92ms
iter 2200: loss 1.1361, time 68.58ms
iter 2210: loss 1.1271, time 78.44ms
iter 2220: loss 1.1237, time 68.19ms
iter 2230: loss 1.1057, time 64.80ms
iter 2240: loss 1.1230, time 66.74ms
step 2250: train loss 1.0079, val loss 1.4964
iter 2250: loss 1.1268, time 20386.14ms
iter 2260: loss 1.1075, time 67.51ms
iter 2270: loss 1.0938, time 68.65ms
iter 2280: loss 1.0875, time 63.61ms
iter 2290: loss 1.0868, time 67.38ms
iter 2300: loss 1.1205, time 70.22ms
iter 2310: loss 1.0883, time 65.41ms
iter 2320: loss 1.0831, time 72.11ms
iter 2330: loss 1.0968, time 68.18ms
iter 2340: loss 1.0909, time 65.01ms
iter 2350: loss 1.0760, time 72.81ms
iter 2360: loss 1.0877, time 64.69ms
iter 2370: loss 1.0974, time 59.61ms
iter 2380: loss 1.0906, time 68.85ms
iter 2390: loss 1.0811, time 62.10ms
iter 2400: loss 1.0515, time 70.38ms
iter 2410: loss 1.0983, time 65.77ms
iter 2420: loss 1.0696, time 68.64ms
iter 2430: loss 1.0494, time 65.15ms
iter 2440: loss 1.0418, time 60.64ms
iter 2450: loss 1.0805, time 65.04ms
iter 2460: loss 1.0905, time 67.64ms
iter 2470: loss 1.0738, time 72.51ms
iter 2480: loss 1.0635, time 65.47ms
iter 2490: loss 1.0624, time 65.11ms
step 2500: train loss 0.9582, val loss 1.5010
iter 2500: loss 1.0740, time 20827.51ms
iter 2510: loss 1.0514, time 57.59ms
iter 2520: loss 1.0669, time 69.97ms
iter 2530: loss 1.0729, time 63.18ms
iter 2540: loss 1.0381, time 69.31ms
iter 2550: loss 1.0677, time 67.63ms
iter 2560: loss 1.0572, time 71.34ms
iter 2570: loss 1.0485, time 64.93ms
iter 2580: loss 1.0429, time 62.37ms
iter 2590: loss 1.0554, time 64.33ms
iter 2600: loss 1.0630, time 65.03ms
iter 2610: loss 1.0314, time 67.56ms
iter 2620: loss 1.0528, time 68.57ms
iter 2630: loss 1.0651, time 79.26ms
iter 2640: loss 1.0156, time 66.19ms
iter 2650: loss 1.0395, time 63.54ms
iter 2660: loss 1.0485, time 61.85ms
iter 2670: loss 1.0449, time 61.57ms
iter 2680: loss 1.0307, time 68.39ms
iter 2690: loss 1.0543, time 69.77ms
iter 2700: loss 1.0068, time 70.16ms
iter 2710: loss 1.0315, time 64.43ms
iter 2720: loss 1.0006, time 64.36ms
iter 2730: loss 1.0385, time 66.63ms
iter 2740: loss 1.0362, time 61.94ms
step 2750: train loss 0.9106, val loss 1.5319
iter 2750: loss 1.0468, time 20228.14ms
iter 2760: loss 1.0076, time 64.80ms
iter 2770: loss 1.0454, time 63.78ms
iter 2780: loss 1.0417, time 66.14ms
iter 2790: loss 1.0231, time 67.50ms
iter 2800: loss 1.0147, time 61.96ms
iter 2810: loss 1.0170, time 67.89ms
iter 2820: loss 1.0316, time 64.55ms
iter 2830: loss 0.9824, time 66.08ms
iter 2840: loss 1.0136, time 68.36ms
iter 2850: loss 1.0155, time 67.61ms
iter 2860: loss 1.0050, time 71.77ms
iter 2870: loss 1.0220, time 75.02ms
iter 2880: loss 1.0200, time 66.34ms
iter 2890: loss 1.0157, time 64.36ms
iter 2900: loss 1.0358, time 64.03ms
iter 2910: loss 1.0162, time 64.51ms
iter 2920: loss 0.9680, time 67.96ms
iter 2930: loss 0.9987, time 68.94ms
iter 2940: loss 0.9977, time 65.96ms
iter 2950: loss 0.9931, time 67.18ms
iter 2960: loss 1.0006, time 64.79ms
iter 2970: loss 0.9950, time 70.25ms
iter 2980: loss 1.0108, time 63.20ms
iter 2990: loss 0.9844, time 56.58ms
step 3000: train loss 0.8645, val loss 1.5516
iter 3000: loss 0.9927, time 20257.06ms
iter 3010: loss 1.0084, time 72.44ms
iter 3020: loss 1.0096, time 64.47ms
iter 3030: loss 1.0070, time 67.98ms
iter 3040: loss 0.9891, time 69.46ms
iter 3050: loss 0.9874, time 64.25ms
iter 3060: loss 0.9929, time 67.19ms
iter 3070: loss 0.9865, time 62.00ms
iter 3080: loss 0.9952, time 65.27ms
iter 3090: loss 0.9997, time 66.93ms
iter 3100: loss 0.9757, time 79.33ms
iter 3110: loss 0.9893, time 63.40ms
iter 3120: loss 0.9919, time 71.25ms
iter 3130: loss 0.9822, time 69.87ms
iter 3140: loss 0.9915, time 66.95ms
iter 3150: loss 0.9615, time 73.59ms
iter 3160: loss 0.9753, time 65.19ms
iter 3170: loss 0.9579, time 62.66ms
iter 3180: loss 0.9495, time 62.02ms
iter 3190: loss 0.9831, time 64.47ms
iter 3200: loss 0.9858, time 64.93ms
iter 3210: loss 0.9985, time 66.41ms
iter 3220: loss 0.9818, time 62.91ms
iter 3230: loss 0.9846, time 72.73ms
iter 3240: loss 0.9406, time 65.20ms
step 3250: train loss 0.8167, val loss 1.5736
iter 3250: loss 0.9904, time 20430.38ms
iter 3260: loss 0.9858, time 69.00ms
iter 3270: loss 0.9728, time 61.38ms
iter 3280: loss 0.9608, time 65.72ms
iter 3290: loss 0.9608, time 68.06ms
iter 3300: loss 0.9465, time 65.50ms
iter 3310: loss 0.9624, time 70.59ms
iter 3320: loss 0.9448, time 68.09ms
iter 3330: loss 0.9456, time 64.04ms
iter 3340: loss 0.9668, time 70.94ms
iter 3350: loss 0.9753, time 66.29ms
iter 3360: loss 0.9423, time 72.67ms
iter 3370: loss 0.9374, time 64.68ms
iter 3380: loss 0.9533, time 63.33ms
iter 3390: loss 0.9432, time 67.73ms
iter 3400: loss 0.9346, time 66.75ms
iter 3410: loss 0.9449, time 67.78ms
iter 3420: loss 0.9357, time 79.05ms
iter 3430: loss 0.9388, time 74.21ms
iter 3440: loss 0.9268, time 64.65ms
iter 3450: loss 0.9312, time 63.19ms
iter 3460: loss 0.9743, time 67.94ms
iter 3470: loss 0.9265, time 63.60ms
iter 3480: loss 0.9372, time 63.86ms
iter 3490: loss 0.9460, time 82.25ms
step 3500: train loss 0.7767, val loss 1.5950
iter 3500: loss 0.9263, time 20626.88ms
iter 3510: loss 0.9472, time 72.42ms
iter 3520: loss 0.9058, time 74.07ms
iter 3530: loss 0.9251, time 80.79ms
iter 3540: loss 0.9428, time 69.90ms
iter 3550: loss 0.9202, time 67.82ms
iter 3560: loss 0.9557, time 64.23ms
iter 3570: loss 0.9326, time 63.85ms
iter 3580: loss 0.9223, time 77.08ms
iter 3590: loss 0.9444, time 80.17ms
iter 3600: loss 0.9085, time 65.37ms
iter 3610: loss 0.9188, time 66.52ms
iter 3620: loss 0.9378, time 69.70ms
iter 3630: loss 0.9122, time 64.62ms
iter 3640: loss 0.9202, time 61.23ms
iter 3650: loss 0.9333, time 65.95ms
iter 3660: loss 0.9172, time 60.08ms
iter 3670: loss 0.9043, time 64.71ms
iter 3680: loss 0.8966, time 71.68ms
iter 3690: loss 0.9026, time 60.09ms
iter 3700: loss 0.9104, time 64.77ms
iter 3710: loss 0.8841, time 66.89ms
iter 3720: loss 0.9053, time 69.61ms
iter 3730: loss 0.9089, time 62.93ms
iter 3740: loss 0.8841, time 65.69ms
step 3750: train loss 0.7340, val loss 1.6292
iter 3750: loss 0.9152, time 20389.64ms
iter 3760: loss 0.8891, time 68.90ms
iter 3770: loss 0.8945, time 75.53ms
iter 3780: loss 0.9105, time 65.10ms
iter 3790: loss 0.8720, time 62.66ms
iter 3800: loss 0.9010, time 71.95ms
iter 3810: loss 0.9284, time 70.67ms
iter 3820: loss 0.9133, time 67.52ms
iter 3830: loss 0.8996, time 76.39ms
iter 3840: loss 0.8978, time 66.16ms
iter 3850: loss 0.8951, time 68.49ms
iter 3860: loss 0.8647, time 68.62ms
iter 3870: loss 0.8704, time 71.64ms
iter 3880: loss 0.8812, time 67.27ms
iter 3890: loss 0.8976, time 68.33ms
iter 3900: loss 0.9287, time 58.75ms
iter 3910: loss 0.8898, time 65.36ms
iter 3920: loss 0.8977, time 62.57ms
iter 3930: loss 0.8907, time 66.13ms
iter 3940: loss 0.8878, time 76.39ms
iter 3950: loss 0.8839, time 58.94ms
iter 3960: loss 0.8702, time 61.05ms
iter 3970: loss 0.8585, time 66.15ms
iter 3980: loss 0.8931, time 64.62ms
iter 3990: loss 0.8818, time 67.35ms
step 4000: train loss 0.7023, val loss 1.6412
iter 4000: loss 0.8869, time 20114.40ms
iter 4010: loss 0.8871, time 67.72ms
iter 4020: loss 0.8481, time 75.44ms
iter 4030: loss 0.8819, time 59.84ms
iter 4040: loss 0.8704, time 67.39ms
iter 4050: loss 0.8496, time 59.76ms
iter 4060: loss 0.8755, time 63.83ms
iter 4070: loss 0.8850, time 66.44ms
iter 4080: loss 0.8770, time 66.75ms
iter 4090: loss 0.8457, time 69.31ms
iter 4100: loss 0.8733, time 69.42ms
iter 4110: loss 0.8815, time 62.58ms
iter 4120: loss 0.8427, time 63.93ms
iter 4130: loss 0.8614, time 62.12ms
iter 4140: loss 0.8674, time 69.93ms
iter 4150: loss 0.8442, time 66.58ms
iter 4160: loss 0.8491, time 66.68ms
iter 4170: loss 0.8697, time 65.41ms
iter 4180: loss 0.8748, time 66.15ms
iter 4190: loss 0.8624, time 64.89ms
iter 4200: loss 0.8480, time 64.82ms
iter 4210: loss 0.8842, time 63.37ms
iter 4220: loss 0.8678, time 62.27ms
iter 4230: loss 0.8736, time 60.99ms
iter 4240: loss 0.8616, time 63.46ms
step 4250: train loss 0.6721, val loss 1.6688
iter 4250: loss 0.8610, time 20079.17ms
iter 4260: loss 0.8747, time 68.33ms
iter 4270: loss 0.8373, time 65.04ms
iter 4280: loss 0.8673, time 60.84ms
iter 4290: loss 0.8467, time 68.91ms
iter 4300: loss 0.8492, time 66.40ms
iter 4310: loss 0.8501, time 63.60ms
iter 4320: loss 0.8638, time 65.95ms
iter 4330: loss 0.8625, time 73.03ms
iter 4340: loss 0.8721, time 63.67ms
iter 4350: loss 0.8591, time 82.30ms
iter 4360: loss 0.8497, time 63.08ms
iter 4370: loss 0.8242, time 64.27ms
iter 4380: loss 0.8607, time 69.70ms
iter 4390: loss 0.8353, time 65.78ms
iter 4400: loss 0.8285, time 61.62ms
iter 4410: loss 0.8464, time 62.71ms
iter 4420: loss 0.8349, time 67.43ms
iter 4430: loss 0.8484, time 75.71ms
iter 4440: loss 0.8364, time 60.68ms
iter 4450: loss 0.8505, time 73.39ms
iter 4460: loss 0.8354, time 60.44ms
iter 4470: loss 0.8654, time 65.40ms
iter 4480: loss 0.8473, time 63.86ms
iter 4490: loss 0.8191, time 60.09ms
step 4500: train loss 0.6473, val loss 1.6822
iter 4500: loss 0.8557, time 19907.25ms
iter 4510: loss 0.8424, time 58.70ms
iter 4520: loss 0.8345, time 66.62ms
iter 4530: loss 0.8327, time 71.19ms
iter 4540: loss 0.8110, time 70.08ms
iter 4550: loss 0.8212, time 60.86ms
iter 4560: loss 0.8368, time 82.31ms
iter 4570: loss 0.8324, time 68.57ms
iter 4580: loss 0.8156, time 61.45ms
iter 4590: loss 0.8168, time 65.89ms
iter 4600: loss 0.8342, time 72.62ms
iter 4610: loss 0.8415, time 66.32ms
iter 4620: loss 0.8122, time 61.78ms
iter 4630: loss 0.8420, time 74.55ms
iter 4640: loss 0.8351, time 71.13ms
iter 4650: loss 0.8406, time 71.65ms
iter 4660: loss 0.8462, time 72.44ms
iter 4670: loss 0.8586, time 66.64ms
iter 4680: loss 0.8603, time 61.32ms
iter 4690: loss 0.8289, time 65.12ms
iter 4700: loss 0.8199, time 71.73ms
iter 4710: loss 0.8387, time 70.12ms
iter 4720: loss 0.8294, time 65.17ms
iter 4730: loss 0.8309, time 70.58ms
iter 4740: loss 0.8157, time 61.47ms
step 4750: train loss 0.6299, val loss 1.6961
iter 4750: loss 0.8292, time 20512.86ms
iter 4760: loss 0.8426, time 74.93ms
iter 4770: loss 0.8324, time 65.69ms
iter 4780: loss 0.8263, time 68.18ms
iter 4790: loss 0.8154, time 59.83ms
iter 4800: loss 0.7995, time 72.54ms
iter 4810: loss 0.8409, time 65.09ms
iter 4820: loss 0.8530, time 62.70ms
iter 4830: loss 0.8253, time 65.25ms
iter 4840: loss 0.8093, time 67.28ms
iter 4850: loss 0.8066, time 68.32ms
iter 4860: loss 0.8095, time 64.05ms
iter 4870: loss 0.8142, time 78.43ms
iter 4880: loss 0.8004, time 60.22ms
iter 4890: loss 0.8111, time 68.54ms
iter 4900: loss 0.7950, time 68.31ms
iter 4910: loss 0.8136, time 62.62ms
iter 4920: loss 0.8153, time 70.31ms
iter 4930: loss 0.8335, time 79.35ms
iter 4940: loss 0.8367, time 69.33ms
iter 4950: loss 0.8291, time 71.05ms
iter 4960: loss 0.8502, time 72.89ms
iter 4970: loss 0.8034, time 60.30ms
iter 4980: loss 0.7913, time 62.08ms
iter 4990: loss 0.7934, time 64.78ms
step 5000: train loss 0.6127, val loss 1.7120
iter 5000: loss 0.7972, time 20576.84ms
training done
Best validation loss: 1.474912166595459
Total train time: 12.82 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 and thought of hand that deliver'd
May have deserved thy spirits.

VOLUMNIA:
My consent brows to your request. I prate you,
As if a present subject I shall attend,
Which most more you stand upon your voices,
Which he dreams after your knees from and close:
If by your need we serve him you dispatch
The city and still his highness did be either so.

BUSHY:
Go with him, good sir; when we shall have you depose you,
Look pale and pardon of such streets in our foes,
And unknown what they were done.

B
Inference time: 2.01 seconds
Tokens per second: 248.59
---------------
Sample 2:
 but that she should not speak,
But not speak no more successively.

LUCIO:
Why, Hortensio, dost thou push me to the prince?

JULIET:
Direct me to-morrow, my master is your part,
Which should be so loathed as mock'd to be,
That brain'd his love as micely favour's right,
As well as I be more than Marcius.
So, understand you when I shall know where I may:
The one for my mouth shall be patient.

BRUTUS:
We shall have reconciled again.

SICINIUS:
Let them change this afternoon, people and bids be
Dec
Inference time: 2.00 seconds
Tokens per second: 250.09
---------------
Sample 3:
 his eye,
Quit my lips for trial and in his children's stars,
Stand bare my kingdom fathers, and two mild,
And many an old access to my eyes
That let my shame live: but I am light
In every time of Isabel,
One day's banish'd friends with the hair of Time
And bid his sails foolish patricians:
If I revolt, I fear, myself and his presence
To execute this presence may dispatch our course,
He shall have been comforted with the company.
That in their uncle devotions held,
And spite of remorse to the cou
Inference time: 1.99 seconds
Tokens per second: 250.88
---------------
Sample 4:
 gentleman what we have a contrary in the people
business to be touched? Susan is this the between
gentleman? fare you, sir?

LUCIO:
Pray, sir, give me ink and paper for it: yet, thou dost not
wear it. I am no better for you; let your bond call me thither.

DUKE VINCENTIO:
It is now your grace instantly.

LUCIO:
Why, then you shall be satisfied: you are to stand 
the prince to prove some sort of malice and drop
in progned the stocks: but let him call him alone,
When we take in prayers of him when
Inference time: 1.99 seconds
Tokens per second: 251.21
---------------
Sample 5:
 and tell the way
The other is likewise and enforced withal.

HENRY BOLINGBROKE:
I will to weeping his life.

KING RICHARD II:
Ay, but he shall not slander from him
Until his grace be disobedient.

QUEEN ELIZABETH:
O, poor soul, young Hastings, whom he hath
Been so charged to hear your grace and ears
To have seen you cause to his young princely feeding,
On Thursday next be well as strongly as I.

RIVERS:
She is good as framed to Richard: But, for our title,
She is both in with King Edward's citiz
Inference time: 1.93 seconds
Tokens per second: 258.40
---------------
Sample 6:
 partner.

ANGELO:
Stand up to distress the traitor that fourth in the world.

ISABELLA:
O, I have spoke of my speech, to myself
I will here remember with him! To see his son
Would not have had pill'd me: so with him too,
And were I thine own livery. I'ld tell the friar brother,
That I may content you, Lord Angelo,
That pass by the time of his country's shame,
That love the frowns of sweet blood at last,
Should have deadly eyes to blaze and down their heads?
O, low their spurs upon them! the want
Inference time: 1.90 seconds
Tokens per second: 263.52
---------------
Sample 7:
 desire to speak the traitor of my sorrow!
Then, Friar John of Gaunt, Sir Richard Grey,
Suggester'd with the duke: then if God's will,
And from Brother, whom myself at hand,
I should be Coriolanus.

FRIAR LAURENCE:
What is't o'clock?

ROMEO:
What, wilt thou not? doth she not guilty of love?

FRIAR LAURENCE:
Holy friar, ho! no fault? fie, friar; I had rather
My babes with swords with her face.

ROMEO:
I speak that the devil appear doth thee
Where thou shalt stir him up and the malice of thy
That s
Inference time: 1.84 seconds
Tokens per second: 271.01
---------------
Sample 8:
 chamber-maids and prayers
To stop the sea-simple steer,
And hanging pack'd out, as they speak the very beams
Which our eyes do themselves, purchase summerstition
That they do proceed.

FLORIZEL:
How! what say you, sir?

POLIXENES:
The gods forbid!
I tell you, the strangers o' the pain of sea,
As I mean so may call you, I will be aged.

CAMILLO:
I am a strange of report of mine age:
And therefore thou wouldst have ever wrought
That come to command. So for it stands,
That you shed fair, I would no
Inference time: 1.89 seconds
Tokens per second: 264.80
---------------
Sample 9:
 Crosby Place, a boy. By this face,
Whose special place hath been my father's hand,
By heaven, I pray'd the world's grave, and revenge
My grave censured servant in the encountertainment
And yet a maid of infant all of far in
The aught of prayers being i' the separation,
Two more than short match'd.

PERDITA:
Alone I should say 'tis no time to make me cry:
A perfect shall be all advised me:
Condemned with a stale and a title body
To answer it.

ANGELO:
So it must be so;
I conclude the noble is tha
Inference time: 1.90 seconds
Tokens per second: 263.62
---------------
Sample 10:
 the never state and princely forth,
If we respect, discharged us no increase
That we do exile the suppliant day which he
Beggars his eyes do attemplige and scars of love
The most pale-feasts of his friendly buried.
Therefore, no man that comes to make me at
The common people host to come the worship;
Not how he drew to-morrow how he should be proud.

RICHARD:
Then let us such secrecy as my sorrow are so
And ill the days of their young princely feeling,
I'll hence to Coventry, be revenged on thei
Inference time: 1.86 seconds
Tokens per second: 269.00
---------------
Average tokens per second: 259.11
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2469, val loss 4.2417
iter 0: loss 4.2478, time 21120.77ms
iter 10: loss 3.2118, time 63.49ms
iter 20: loss 2.7846, time 76.32ms
iter 30: loss 2.6130, time 66.04ms
iter 40: loss 2.5605, time 73.56ms
iter 50: loss 2.5317, time 60.75ms
iter 60: loss 2.4922, time 62.25ms
iter 70: loss 2.4931, time 74.55ms
iter 80: loss 2.5011, time 64.11ms
iter 90: loss 2.4810, time 64.23ms
iter 100: loss 2.4769, time 61.04ms
iter 110: loss 2.4381, time 67.33ms
iter 120: loss 2.4334, time 68.52ms
iter 130: loss 2.4329, time 65.92ms
iter 140: loss 2.3956, time 62.94ms
iter 150: loss 2.4003, time 63.27ms
iter 160: loss 2.3511, time 68.41ms
iter 170: loss 2.3788, time 62.36ms
iter 180: loss 2.3034, time 63.16ms
iter 190: loss 2.2974, time 68.10ms
iter 200: loss 2.2234, time 83.53ms
iter 210: loss 2.1819, time 61.17ms
iter 220: loss 2.1455, time 63.12ms
iter 230: loss 2.0912, time 61.03ms
iter 240: loss 2.0455, time 69.00ms
step 250: train loss 1.9714, val loss 2.0792
iter 250: loss 2.0479, time 20427.01ms
iter 260: loss 1.9953, time 70.65ms
iter 270: loss 1.9524, time 70.28ms
iter 280: loss 1.9548, time 67.73ms
iter 290: loss 1.9198, time 66.41ms
iter 300: loss 1.9126, time 71.13ms
iter 310: loss 1.8677, time 68.23ms
iter 320: loss 1.8629, time 63.05ms
iter 330: loss 1.8137, time 72.88ms
iter 340: loss 1.8225, time 67.79ms
iter 350: loss 1.8045, time 66.20ms
iter 360: loss 1.7873, time 65.52ms
iter 370: loss 1.7527, time 75.86ms
iter 380: loss 1.7534, time 64.31ms
iter 390: loss 1.7245, time 57.38ms
iter 400: loss 1.7300, time 70.24ms
iter 410: loss 1.7511, time 63.14ms
iter 420: loss 1.6943, time 60.06ms
iter 430: loss 1.6502, time 68.74ms
iter 440: loss 1.6967, time 65.98ms
iter 450: loss 1.6393, time 67.09ms
iter 460: loss 1.6506, time 72.54ms
iter 470: loss 1.6273, time 69.10ms
iter 480: loss 1.6350, time 66.65ms
iter 490: loss 1.6115, time 74.28ms
step 500: train loss 1.5220, val loss 1.7137
iter 500: loss 1.5475, time 20253.97ms
iter 510: loss 1.6167, time 70.96ms
iter 520: loss 1.5725, time 66.45ms
iter 530: loss 1.5770, time 59.44ms
iter 540: loss 1.5885, time 63.02ms
iter 550: loss 1.5513, time 62.43ms
iter 560: loss 1.5612, time 72.34ms
iter 570: loss 1.5571, time 65.11ms
iter 580: loss 1.5351, time 78.53ms
iter 590: loss 1.5259, time 62.57ms
iter 600: loss 1.5140, time 80.39ms
iter 610: loss 1.5147, time 67.14ms
iter 620: loss 1.5150, time 67.61ms
iter 630: loss 1.5192, time 69.33ms
iter 640: loss 1.4911, time 67.66ms
iter 650: loss 1.5049, time 61.94ms
iter 660: loss 1.4818, time 68.47ms
iter 670: loss 1.4717, time 65.85ms
iter 680: loss 1.4591, time 63.18ms
iter 690: loss 1.4619, time 70.49ms
iter 700: loss 1.4733, time 65.24ms
iter 710: loss 1.4811, time 65.18ms
iter 720: loss 1.4730, time 66.31ms
iter 730: loss 1.4593, time 63.02ms
iter 740: loss 1.4298, time 67.63ms
step 750: train loss 1.3593, val loss 1.5813
iter 750: loss 1.4312, time 20439.99ms
iter 760: loss 1.4413, time 65.21ms
iter 770: loss 1.4514, time 63.96ms
iter 780: loss 1.4419, time 63.23ms
iter 790: loss 1.4201, time 63.11ms
iter 800: loss 1.4252, time 63.72ms
iter 810: loss 1.4267, time 63.38ms
iter 820: loss 1.3874, time 71.94ms
iter 830: loss 1.3797, time 67.78ms
iter 840: loss 1.4022, time 69.91ms
iter 850: loss 1.3736, time 66.05ms
iter 860: loss 1.3839, time 65.10ms
iter 870: loss 1.3901, time 59.54ms
iter 880: loss 1.3628, time 58.99ms
iter 890: loss 1.3777, time 62.36ms
iter 900: loss 1.3464, time 65.28ms
iter 910: loss 1.3901, time 62.89ms
iter 920: loss 1.3739, time 58.89ms
iter 930: loss 1.3604, time 67.23ms
iter 940: loss 1.3513, time 59.26ms
iter 950: loss 1.3530, time 61.94ms
iter 960: loss 1.3747, time 63.88ms
iter 970: loss 1.3688, time 63.71ms
iter 980: loss 1.3617, time 67.52ms
iter 990: loss 1.3515, time 59.60ms
step 1000: train loss 1.2696, val loss 1.5264
iter 1000: loss 1.3202, time 20289.05ms
iter 1010: loss 1.3462, time 67.61ms
iter 1020: loss 1.3196, time 61.42ms
iter 1030: loss 1.3531, time 58.95ms
iter 1040: loss 1.3240, time 60.62ms
iter 1050: loss 1.3320, time 60.07ms
iter 1060: loss 1.3417, time 59.26ms
iter 1070: loss 1.3519, time 65.86ms
iter 1080: loss 1.3141, time 65.40ms
iter 1090: loss 1.2710, time 76.83ms
iter 1100: loss 1.3130, time 66.82ms
iter 1110: loss 1.3176, time 67.37ms
iter 1120: loss 1.2981, time 66.63ms
iter 1130: loss 1.3011, time 61.88ms
iter 1140: loss 1.3292, time 64.30ms
iter 1150: loss 1.2694, time 71.86ms
iter 1160: loss 1.3113, time 68.39ms
iter 1170: loss 1.2807, time 77.08ms
iter 1180: loss 1.2657, time 70.28ms
iter 1190: loss 1.3127, time 71.68ms
iter 1200: loss 1.2889, time 63.28ms
iter 1210: loss 1.2851, time 65.80ms
iter 1220: loss 1.2977, time 63.52ms
iter 1230: loss 1.2950, time 68.22ms
iter 1240: loss 1.2980, time 62.02ms
step 1250: train loss 1.2028, val loss 1.4890
iter 1250: loss 1.3118, time 20445.12ms
iter 1260: loss 1.2931, time 67.30ms
iter 1270: loss 1.2975, time 68.82ms
iter 1280: loss 1.2754, time 66.53ms
iter 1290: loss 1.2613, time 68.44ms
iter 1300: loss 1.2936, time 69.51ms
iter 1310: loss 1.2635, time 65.54ms
iter 1320: loss 1.2564, time 74.15ms
iter 1330: loss 1.2519, time 72.88ms
iter 1340: loss 1.2692, time 65.97ms
iter 1350: loss 1.2419, time 69.82ms
iter 1360: loss 1.2360, time 71.50ms
iter 1370: loss 1.2717, time 67.73ms
iter 1380: loss 1.2338, time 68.73ms
iter 1390: loss 1.2402, time 73.57ms
iter 1400: loss 1.2295, time 64.27ms
iter 1410: loss 1.2082, time 62.32ms
iter 1420: loss 1.2585, time 68.36ms
iter 1430: loss 1.2650, time 72.65ms
iter 1440: loss 1.2124, time 64.81ms
iter 1450: loss 1.2657, time 67.05ms
iter 1460: loss 1.2509, time 73.50ms
iter 1470: loss 1.2254, time 67.32ms
iter 1480: loss 1.2169, time 75.54ms
iter 1490: loss 1.2462, time 65.52ms
step 1500: train loss 1.1471, val loss 1.4621
iter 1500: loss 1.2022, time 19975.49ms
iter 1510: loss 1.2362, time 63.53ms
iter 1520: loss 1.2075, time 68.74ms
iter 1530: loss 1.2363, time 70.44ms
iter 1540: loss 1.2210, time 65.98ms
iter 1550: loss 1.1768, time 76.18ms
iter 1560: loss 1.2197, time 67.26ms
iter 1570: loss 1.1942, time 69.03ms
iter 1580: loss 1.2080, time 84.79ms
iter 1590: loss 1.2391, time 63.56ms
iter 1600: loss 1.2107, time 64.76ms
iter 1610: loss 1.2083, time 85.63ms
iter 1620: loss 1.2119, time 69.73ms
iter 1630: loss 1.2037, time 69.06ms
iter 1640: loss 1.2219, time 100.82ms
iter 1650: loss 1.1921, time 68.24ms
iter 1660: loss 1.2197, time 60.50ms
iter 1670: loss 1.2206, time 78.56ms
iter 1680: loss 1.2227, time 67.47ms
iter 1690: loss 1.2273, time 70.12ms
iter 1700: loss 1.2191, time 74.93ms
iter 1710: loss 1.1927, time 77.49ms
iter 1720: loss 1.1924, time 67.78ms
iter 1730: loss 1.1939, time 71.09ms
iter 1740: loss 1.1918, time 72.75ms
step 1750: train loss 1.1006, val loss 1.4778
iter 1750: loss 1.1895, time 20703.41ms
iter 1760: loss 1.1723, time 67.74ms
iter 1770: loss 1.1589, time 74.52ms
iter 1780: loss 1.1583, time 70.51ms
iter 1790: loss 1.1828, time 69.27ms
iter 1800: loss 1.1707, time 64.14ms
iter 1810: loss 1.1840, time 73.15ms
iter 1820: loss 1.1663, time 65.44ms
iter 1830: loss 1.1317, time 76.40ms
iter 1840: loss 1.1774, time 78.04ms
iter 1850: loss 1.1827, time 76.61ms
iter 1860: loss 1.1942, time 62.78ms
iter 1870: loss 1.1460, time 71.24ms
iter 1880: loss 1.2110, time 69.61ms
iter 1890: loss 1.1430, time 69.74ms
iter 1900: loss 1.1823, time 62.97ms
iter 1910: loss 1.1375, time 70.81ms
iter 1920: loss 1.1468, time 73.93ms
iter 1930: loss 1.1414, time 64.03ms
iter 1940: loss 1.1435, time 66.28ms
iter 1950: loss 1.1618, time 69.07ms
iter 1960: loss 1.1431, time 75.21ms
iter 1970: loss 1.1360, time 67.60ms
iter 1980: loss 1.1480, time 70.25ms
iter 1990: loss 1.1448, time 71.68ms
step 2000: train loss 1.0517, val loss 1.4794
iter 2000: loss 1.1536, time 20138.41ms
iter 2010: loss 1.1375, time 67.77ms
iter 2020: loss 1.1507, time 66.71ms
iter 2030: loss 1.1244, time 64.98ms
iter 2040: loss 1.1457, time 70.22ms
iter 2050: loss 1.1478, time 67.14ms
iter 2060: loss 1.1573, time 68.53ms
iter 2070: loss 1.1536, time 62.77ms
iter 2080: loss 1.1398, time 62.15ms
iter 2090: loss 1.1161, time 63.85ms
iter 2100: loss 1.1216, time 62.76ms
iter 2110: loss 1.1197, time 62.44ms
iter 2120: loss 1.0808, time 64.98ms
iter 2130: loss 1.1223, time 66.79ms
iter 2140: loss 1.1125, time 71.48ms
iter 2150: loss 1.1311, time 66.74ms
iter 2160: loss 1.1394, time 61.36ms
iter 2170: loss 1.1397, time 68.42ms
iter 2180: loss 1.1391, time 65.54ms
iter 2190: loss 1.1278, time 64.77ms
iter 2200: loss 1.1141, time 72.66ms
iter 2210: loss 1.1214, time 65.68ms
iter 2220: loss 1.0966, time 70.75ms
iter 2230: loss 1.1052, time 66.46ms
iter 2240: loss 1.1089, time 62.93ms
step 2250: train loss 1.0083, val loss 1.4851
iter 2250: loss 1.1047, time 21154.09ms
iter 2260: loss 1.1027, time 65.61ms
iter 2270: loss 1.1235, time 83.65ms
iter 2280: loss 1.1044, time 72.96ms
iter 2290: loss 1.0750, time 72.71ms
iter 2300: loss 1.0662, time 78.61ms
iter 2310: loss 1.1025, time 73.42ms
iter 2320: loss 1.0966, time 66.45ms
iter 2330: loss 1.1283, time 79.25ms
iter 2340: loss 1.0848, time 70.01ms
iter 2350: loss 1.1065, time 62.64ms
iter 2360: loss 1.0846, time 70.76ms
iter 2370: loss 1.0818, time 67.98ms
iter 2380: loss 1.0883, time 63.36ms
iter 2390: loss 1.0836, time 62.56ms
iter 2400: loss 1.1054, time 64.03ms
iter 2410: loss 1.0757, time 67.16ms
iter 2420: loss 1.0672, time 65.65ms
iter 2430: loss 1.0906, time 70.42ms
iter 2440: loss 1.0882, time 67.12ms
iter 2450: loss 1.0803, time 66.32ms
iter 2460: loss 1.0403, time 71.62ms
iter 2470: loss 1.0935, time 71.39ms
iter 2480: loss 1.0729, time 61.13ms
iter 2490: loss 1.0756, time 70.11ms
step 2500: train loss 0.9634, val loss 1.4957
iter 2500: loss 1.0581, time 20244.00ms
iter 2510: loss 1.0792, time 62.81ms
iter 2520: loss 1.0689, time 70.11ms
iter 2530: loss 1.0756, time 69.23ms
iter 2540: loss 1.0668, time 68.51ms
iter 2550: loss 1.0640, time 65.31ms
iter 2560: loss 1.0551, time 67.20ms
iter 2570: loss 1.0717, time 59.33ms
iter 2580: loss 1.0465, time 61.42ms
iter 2590: loss 1.0836, time 68.38ms
iter 2600: loss 1.0813, time 60.76ms
iter 2610: loss 1.0676, time 64.93ms
iter 2620: loss 1.0550, time 69.24ms
iter 2630: loss 1.0606, time 64.61ms
iter 2640: loss 1.0504, time 73.50ms
iter 2650: loss 1.0528, time 64.51ms
iter 2660: loss 1.0492, time 62.69ms
iter 2670: loss 1.0247, time 64.58ms
iter 2680: loss 1.0407, time 63.40ms
iter 2690: loss 1.0647, time 61.08ms
iter 2700: loss 1.0819, time 67.50ms
iter 2710: loss 1.0468, time 73.53ms
iter 2720: loss 1.0345, time 65.57ms
iter 2730: loss 1.0305, time 67.18ms
iter 2740: loss 1.0423, time 70.58ms
step 2750: train loss 0.9124, val loss 1.5112
iter 2750: loss 1.0403, time 20607.79ms
iter 2760: loss 1.0103, time 62.21ms
iter 2770: loss 1.0086, time 69.07ms
iter 2780: loss 1.0264, time 67.98ms
iter 2790: loss 1.0423, time 68.64ms
iter 2800: loss 1.0185, time 64.65ms
iter 2810: loss 0.9907, time 68.28ms
iter 2820: loss 1.0226, time 71.11ms
iter 2830: loss 0.9753, time 62.28ms
iter 2840: loss 1.0383, time 63.47ms
iter 2850: loss 1.0113, time 59.29ms
iter 2860: loss 1.0292, time 71.24ms
iter 2870: loss 1.0278, time 67.38ms
iter 2880: loss 1.0129, time 68.90ms
iter 2890: loss 0.9885, time 59.87ms
iter 2900: loss 1.0043, time 65.44ms
iter 2910: loss 1.0256, time 66.04ms
iter 2920: loss 1.0273, time 74.45ms
iter 2930: loss 0.9884, time 61.83ms
iter 2940: loss 0.9931, time 67.60ms
iter 2950: loss 0.9973, time 66.72ms
iter 2960: loss 1.0311, time 68.47ms
iter 2970: loss 0.9821, time 68.14ms
iter 2980: loss 0.9899, time 67.65ms
iter 2990: loss 0.9815, time 61.83ms
step 3000: train loss 0.8719, val loss 1.5284
iter 3000: loss 0.9979, time 20416.22ms
iter 3010: loss 0.9793, time 64.45ms
iter 3020: loss 1.0184, time 61.76ms
iter 3030: loss 0.9994, time 78.28ms
iter 3040: loss 0.9743, time 66.94ms
iter 3050: loss 0.9948, time 61.44ms
iter 3060: loss 1.0059, time 67.14ms
iter 3070: loss 0.9888, time 65.22ms
iter 3080: loss 0.9769, time 66.72ms
iter 3090: loss 1.0077, time 64.79ms
iter 3100: loss 0.9973, time 63.12ms
iter 3110: loss 1.0108, time 64.08ms
iter 3120: loss 0.9748, time 67.01ms
iter 3130: loss 0.9589, time 77.73ms
iter 3140: loss 0.9828, time 68.37ms
iter 3150: loss 0.9882, time 70.16ms
iter 3160: loss 0.9637, time 59.24ms
iter 3170: loss 0.9579, time 67.83ms
iter 3180: loss 0.9894, time 60.25ms
iter 3190: loss 0.9659, time 65.94ms
iter 3200: loss 0.9557, time 64.41ms
iter 3210: loss 0.9613, time 62.45ms
iter 3220: loss 0.9407, time 73.52ms
iter 3230: loss 0.9579, time 58.88ms
iter 3240: loss 0.9644, time 75.22ms
step 3250: train loss 0.8222, val loss 1.5458
iter 3250: loss 0.9341, time 21022.66ms
iter 3260: loss 0.9444, time 66.30ms
iter 3270: loss 0.9404, time 77.22ms
iter 3280: loss 0.9344, time 67.21ms
iter 3290: loss 0.9686, time 75.30ms
iter 3300: loss 0.9597, time 79.63ms
iter 3310: loss 0.9720, time 66.34ms
iter 3320: loss 0.9223, time 63.79ms
iter 3330: loss 0.9542, time 73.62ms
iter 3340: loss 0.9673, time 62.48ms
iter 3350: loss 0.9530, time 74.50ms
iter 3360: loss 0.9625, time 96.29ms
iter 3370: loss 0.9471, time 71.79ms
iter 3380: loss 0.9315, time 78.26ms
iter 3390: loss 0.9301, time 77.64ms
iter 3400: loss 0.9721, time 74.41ms
iter 3410: loss 0.9730, time 70.68ms
iter 3420: loss 0.9311, time 68.84ms
iter 3430: loss 0.9239, time 78.59ms
iter 3440: loss 0.9594, time 71.93ms
iter 3450: loss 0.9623, time 70.94ms
iter 3460: loss 0.9362, time 76.69ms
iter 3470: loss 0.9233, time 69.48ms
iter 3480: loss 0.9061, time 71.64ms
iter 3490: loss 0.9449, time 78.23ms
step 3500: train loss 0.7791, val loss 1.5724
iter 3500: loss 0.9107, time 21026.89ms
iter 3510: loss 0.9210, time 69.43ms
iter 3520: loss 0.9347, time 77.40ms
iter 3530: loss 0.9365, time 74.15ms
iter 3540: loss 0.9252, time 68.42ms
iter 3550: loss 0.9311, time 72.49ms
iter 3560: loss 0.9342, time 69.14ms
iter 3570: loss 0.9296, time 74.46ms
iter 3580: loss 0.9338, time 79.06ms
iter 3590: loss 0.9103, time 72.47ms
iter 3600: loss 0.9369, time 63.14ms
iter 3610: loss 0.9141, time 62.48ms
iter 3620: loss 0.9146, time 80.10ms
iter 3630: loss 0.9219, time 74.12ms
iter 3640: loss 0.9381, time 70.94ms
iter 3650: loss 0.9000, time 68.31ms
iter 3660: loss 0.9125, time 69.10ms
iter 3670: loss 0.9220, time 65.80ms
iter 3680: loss 0.9014, time 82.40ms
iter 3690: loss 0.9370, time 73.04ms
iter 3700: loss 0.9334, time 70.06ms
iter 3710: loss 0.9149, time 71.07ms
iter 3720: loss 0.8945, time 69.92ms
iter 3730: loss 0.9211, time 70.31ms
iter 3740: loss 0.8996, time 74.26ms
step 3750: train loss 0.7403, val loss 1.6047
iter 3750: loss 0.9404, time 20967.51ms
iter 3760: loss 0.9077, time 69.50ms
iter 3770: loss 0.9010, time 69.12ms
iter 3780: loss 0.9064, time 65.20ms
iter 3790: loss 0.9120, time 63.89ms
iter 3800: loss 0.9116, time 69.06ms
iter 3810: loss 0.8898, time 70.65ms
iter 3820: loss 0.8712, time 67.95ms
iter 3830: loss 0.8808, time 66.40ms
iter 3840: loss 0.9137, time 71.64ms
iter 3850: loss 0.8642, time 68.19ms
iter 3860: loss 0.9063, time 70.95ms
iter 3870: loss 0.8822, time 70.53ms
iter 3880: loss 0.8830, time 74.52ms
iter 3890: loss 0.9076, time 77.53ms
iter 3900: loss 0.8763, time 72.13ms
iter 3910: loss 0.9074, time 71.38ms
iter 3920: loss 0.8893, time 67.99ms
iter 3930: loss 0.8634, time 66.98ms
iter 3940: loss 0.8792, time 68.52ms
iter 3950: loss 0.9156, time 69.03ms
iter 3960: loss 0.8914, time 74.86ms
iter 3970: loss 0.8968, time 69.07ms
iter 3980: loss 0.8830, time 74.91ms
iter 3990: loss 0.8888, time 73.19ms
step 4000: train loss 0.7085, val loss 1.6319
iter 4000: loss 0.8673, time 21247.31ms
iter 4010: loss 0.8608, time 72.77ms
iter 4020: loss 0.8796, time 76.97ms
iter 4030: loss 0.9179, time 69.72ms
iter 4040: loss 0.8858, time 73.58ms
iter 4050: loss 0.8850, time 65.60ms
iter 4060: loss 0.8679, time 64.81ms
iter 4070: loss 0.8639, time 67.92ms
iter 4080: loss 0.8635, time 66.90ms
iter 4090: loss 0.8698, time 75.58ms
iter 4100: loss 0.8458, time 69.70ms
iter 4110: loss 0.8822, time 68.80ms
iter 4120: loss 0.8793, time 68.94ms
iter 4130: loss 0.8611, time 73.85ms
iter 4140: loss 0.8591, time 66.21ms
iter 4150: loss 0.8734, time 70.51ms
iter 4160: loss 0.8412, time 70.81ms
iter 4170: loss 0.8719, time 70.41ms
iter 4180: loss 0.8667, time 71.10ms
iter 4190: loss 0.8647, time 75.75ms
iter 4200: loss 0.8444, time 67.64ms
iter 4210: loss 0.8560, time 69.90ms
iter 4220: loss 0.8715, time 69.46ms
iter 4230: loss 0.8637, time 71.96ms
iter 4240: loss 0.8661, time 73.05ms
step 4250: train loss 0.6809, val loss 1.6421
iter 4250: loss 0.8665, time 21224.69ms
iter 4260: loss 0.8791, time 68.16ms
iter 4270: loss 0.8670, time 68.98ms
iter 4280: loss 0.8582, time 70.46ms
iter 4290: loss 0.8433, time 70.19ms
iter 4300: loss 0.8590, time 71.10ms
iter 4310: loss 0.8514, time 64.79ms
iter 4320: loss 0.8750, time 72.93ms
iter 4330: loss 0.8582, time 69.09ms
iter 4340: loss 0.8301, time 69.00ms
iter 4350: loss 0.8451, time 65.87ms
iter 4360: loss 0.8442, time 68.13ms
iter 4370: loss 0.8403, time 68.64ms
iter 4380: loss 0.8330, time 68.42ms
iter 4390: loss 0.8660, time 72.17ms
iter 4400: loss 0.8534, time 67.86ms
iter 4410: loss 0.8532, time 67.13ms
iter 4420: loss 0.8531, time 69.66ms
iter 4430: loss 0.8446, time 69.35ms
iter 4440: loss 0.8631, time 64.22ms
iter 4450: loss 0.8483, time 72.33ms
iter 4460: loss 0.8402, time 71.07ms
iter 4470: loss 0.8661, time 67.64ms
iter 4480: loss 0.8313, time 72.09ms
iter 4490: loss 0.8642, time 77.29ms
step 4500: train loss 0.6542, val loss 1.6670
iter 4500: loss 0.8438, time 21131.02ms
iter 4510: loss 0.8527, time 74.23ms
iter 4520: loss 0.8328, time 65.57ms
iter 4530: loss 0.8724, time 64.50ms
iter 4540: loss 0.8405, time 60.84ms
iter 4550: loss 0.8710, time 66.05ms
iter 4560: loss 0.8559, time 68.95ms
iter 4570: loss 0.8530, time 68.30ms
iter 4580: loss 0.8142, time 70.09ms
iter 4590: loss 0.8341, time 75.28ms
iter 4600: loss 0.8255, time 67.48ms
iter 4610: loss 0.8192, time 73.31ms
iter 4620: loss 0.8206, time 65.25ms
iter 4630: loss 0.8398, time 69.20ms
iter 4640: loss 0.8532, time 73.08ms
iter 4650: loss 0.8175, time 66.59ms
iter 4660: loss 0.8298, time 66.56ms
iter 4670: loss 0.8309, time 71.66ms
iter 4680: loss 0.8489, time 56.60ms
iter 4690: loss 0.8358, time 64.52ms
iter 4700: loss 0.8262, time 67.36ms
iter 4710: loss 0.8509, time 66.15ms
iter 4720: loss 0.8433, time 74.21ms
iter 4730: loss 0.8410, time 72.94ms
iter 4740: loss 0.8070, time 70.95ms
step 4750: train loss 0.6359, val loss 1.6783
iter 4750: loss 0.8330, time 21276.60ms
iter 4760: loss 0.8227, time 72.19ms
iter 4770: loss 0.8298, time 71.37ms
iter 4780: loss 0.8442, time 81.24ms
iter 4790: loss 0.8277, time 74.52ms
iter 4800: loss 0.8479, time 76.36ms
iter 4810: loss 0.8310, time 71.58ms
iter 4820: loss 0.8176, time 72.10ms
iter 4830: loss 0.8215, time 71.47ms
iter 4840: loss 0.8210, time 62.07ms
iter 4850: loss 0.8400, time 68.62ms
iter 4860: loss 0.8165, time 68.65ms
iter 4870: loss 0.8117, time 71.62ms
iter 4880: loss 0.8062, time 70.55ms
iter 4890: loss 0.7996, time 66.73ms
iter 4900: loss 0.8245, time 69.30ms
iter 4910: loss 0.8342, time 67.70ms
iter 4920: loss 0.8295, time 77.18ms
iter 4930: loss 0.8209, time 63.78ms
iter 4940: loss 0.8238, time 71.64ms
iter 4950: loss 0.8308, time 66.48ms
iter 4960: loss 0.8149, time 63.21ms
iter 4970: loss 0.8341, time 70.93ms
iter 4980: loss 0.8316, time 67.34ms
iter 4990: loss 0.7916, time 68.71ms
step 5000: train loss 0.6202, val loss 1.6938
iter 5000: loss 0.8198, time 21190.80ms
training done
Best validation loss: 1.4620716571807861
Total train time: 13.08 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 the contraction of the world:
The tongue my morning heart guilty men;
And here ready that my mortal mighty shall cry,
That high keep me with court'sy ears and humour.

QUEEN ELIZABETH:
Nay, but never can see how the day,
But soon might have made me mercy of her
As or how I am with my daughter-house.
These are nothing: I thank you must find your pains;
But, might you go live upon yourselves with being so,
But that, I make a grief for honour of my son,
That ever your leisure is no friend of heart:
Inference time: 1.90 seconds
Tokens per second: 263.38
---------------
Sample 2:
 away.

DUKE OF YORK:
Why then I pardon me, lords, at arms,
Which I would say, I shall know the duke is right.

DUCHESS OF YORK:
God forbid! take my daughter with my brother:
And so much is the way to me, I pray;
Two of the cause, the moveable to my mourn
Upon the court. In the county promise
What I mean, so I enjoy the oracle,
That love me not, though my hand I should love:
And when I do seek to have it, and do me:
I am the crown hour and the common morn,
Of the general impression of my counsel:
Inference time: 1.87 seconds
Tokens per second: 266.72
---------------
Sample 3:
 of late,
The counsel of the windows of death:
My lord, your fearful Edward shall prevent.

WARWICK:
O, for God's sake, fair lord and thy fortune,
And send thee to our prisoners.

WARWICK:
Such peace with these roses that would not have all
From Warwick's to flesh and his fortunes to bed
Unto the book of most parting from the world:
And for thou believe'st the weary work of my food,
And mean to my misdeeds, to my wounds are wide?
And art thou as a Christian creature as a bare,
And Bolingbroke was
Inference time: 1.91 seconds
Tokens per second: 262.24
---------------
Sample 4:
 not be so.

CLAUDIO:
He hath done so, Isabel, sir, my lord.

DUKE VINCENTIO:
Why should you know me know? I would to do it,
And she shall know well and not well meant.

LUCIO:
No, in good time, as steel my virtue hath done
For the severing of her peremption.

DUKE VINCENTIO:
Not yestilling but she's an old tune with she
That makes him mine honour.

LUCIO:
A man, thou art too storn by any man's weight so.

DUKE VINCENTIO:
And well one word to be against a man word.

MISTRESS OVERDONE:
But well, w
Inference time: 1.93 seconds
Tokens per second: 259.52
---------------
Sample 5:
 this, the poor souls of my soul's steed,
And was a cordial to the vantage of state,
Which to have a loss fled dear with cheerful bosom,
No ceremony and discontented souls,
But many men of some five decreant
And kneel the most accusation.

ISABELLA:
Those hose man may strike upon my brother,
That thou shouldst afflict me with a life or several
Like to kiss thee: for thou art a widow's tooth,
And with this dangerous labour, to this land
Deformed queen out a soldier, by so dishonours
Of the royalti
Inference time: 2.15 seconds
Tokens per second: 232.85
---------------
Sample 6:
 you, considers? We are at least,
And you shall not know that Venice' cell:
That which you do call me now, but greater time.
As you do, my noble father wrong
Hath broken from me to serve, and so offend
I will mend to your knees with your love
To the heart of the heart: so shall not the doubt,
And bite the hire of the bear to break and pale
Of mortals and the severent men'st shame;
Or pity shall point the highway, thou tear'st me not;
If not, though I shalt be troublen of,
When I wake this land as
Inference time: 2.09 seconds
Tokens per second: 239.29
---------------
Sample 7:
 the good provost:
Let me see that I have seen the advantage.

DUKE OF AUMERLE:
When he will be done, my lords, come the sun:
Or, if he be too right with my heart,
Or do not so soon as another man,
Nor so he did deserve your son that are now.

KING RICHARD II:
We will be renowned for false eyes:
By this one I hope, the want to make again:
Whereof our wisdoms and tender thou hast marr'd;
And for thy foul deeds must have married her breath;
My poor hopes here in person with two souls,
And she would
Inference time: 2.10 seconds
Tokens per second: 237.60
---------------
Sample 8:
 and to have a kind of
The king escaped it; and you know not what you are.

KING HENRY VI:
I shall not rise up for that grace of this:
Now now my state shall not need of careful mouth
And never be not known to part the way.

KING EDWARD IV:
Now joyfully is the regal throne,
So far from me and less than I; for there is motion.

LADY GREY:
Then let me know my suit in my shining boy.

KING EDWARD IV:
My Lord of Gloucester, what with a town,
Which, are you misplanted with the fatal small-day?

MONTAG
Inference time: 2.00 seconds
Tokens per second: 249.63
---------------
Sample 9:
 and from thy strength and humour'd hairs,
And prays go with us. Therefore, good my lord,
To whom my father's death befall'n him:
How thy heart is my revenge! whither away?

HENRY BOLINGBROKE:
My gracious uncle, the mayor grant
I'll rouse thee my loving father.

KING RICHARD II:
Here at the Sixth field, and all the king's womb,
Whose chief shall be written in peace and breath
That long begin to be with munet?

BIANCA:
Go to dine with him, and he hath not touch'd him.

BUCKINGHAM:
O Clifford, my l
Inference time: 2.01 seconds
Tokens per second: 248.48
---------------
Sample 10:
 nor grace.

BIONDELLO:
What with him? My lord?

LEONTES:
How now!

HERMIONE:
My lord,
You shall not bless the news in Baptista's state,
Of the sun execution of my charge,
And strike me from all the deputy;
And if you shall have some store 'gainst of me,
That I should call the Duke of Norfolk, they owe
King Edward's princes from brother, they would have Edward,
And take my leave and for beauty might have sin.

KING EDWARD IV:
Brave from my keen, and saw the word to me.

GLOUCESTER:

KING EDWARD I
Inference time: 2.03 seconds
Tokens per second: 246.35
---------------
Average tokens per second: 250.60
tokens per iteration will be: 8,192
found vocab_size = 205 (inside ../../../data/enwik8/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.70M
num decayed parameter tensors: 26, with 10,793,856 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 5.3185, val loss 5.3184
iter 0: loss 5.3229, time 67337.79ms
iter 100: loss 2.8816, time 132.30ms
iter 200: loss 2.6492, time 134.83ms
iter 300: loss 2.5464, time 139.23ms
iter 400: loss 2.4120, time 151.31ms
iter 500: loss 2.3457, time 148.32ms
iter 600: loss 2.1660, time 131.92ms
iter 700: loss 2.1487, time 133.78ms
iter 800: loss 2.0708, time 113.25ms
iter 900: loss 1.9539, time 140.05ms
step 1000: train loss 1.8114, val loss 1.8178
iter 1000: loss 1.9590, time 58704.84ms
iter 1100: loss 1.8835, time 137.77ms
iter 1200: loss 1.8379, time 147.25ms
iter 1300: loss 1.8028, time 155.99ms
iter 1400: loss 1.6589, time 134.68ms
iter 1500: loss 1.7829, time 144.07ms
iter 1600: loss 1.6256, time 128.28ms
iter 1700: loss 1.6904, time 135.33ms
iter 1800: loss 1.5772, time 136.61ms
iter 1900: loss 1.5505, time 131.42ms
step 2000: train loss 1.4629, val loss 1.4690
iter 2000: loss 1.5461, time 57195.39ms
iter 2100: loss 1.5808, time 132.48ms
iter 2200: loss 1.4556, time 128.01ms
iter 2300: loss 1.3904, time 124.76ms
iter 2400: loss 1.5168, time 140.99ms
iter 2500: loss 1.4797, time 124.66ms
iter 2600: loss 1.5131, time 133.32ms
iter 2700: loss 1.4555, time 134.06ms
iter 2800: loss 1.5060, time 139.31ms
iter 2900: loss 1.4922, time 130.22ms
step 3000: train loss 1.3415, val loss 1.3582
iter 3000: loss 1.4456, time 56897.55ms
iter 3100: loss 1.4252, time 140.17ms
iter 3200: loss 1.3126, time 129.73ms
iter 3300: loss 1.4093, time 128.53ms
iter 3400: loss 1.3985, time 133.47ms
iter 3500: loss 1.3721, time 131.53ms
iter 3600: loss 1.3442, time 139.69ms
iter 3700: loss 1.4188, time 134.80ms
iter 3800: loss 1.3955, time 131.35ms
iter 3900: loss 1.3856, time 131.10ms
step 4000: train loss 1.2893, val loss 1.2975
iter 4000: loss 1.3320, time 56836.17ms
iter 4100: loss 1.3686, time 133.50ms
iter 4200: loss 1.2634, time 135.35ms
iter 4300: loss 1.3452, time 129.93ms
iter 4400: loss 1.4296, time 138.02ms
iter 4500: loss 1.3000, time 136.43ms
iter 4600: loss 1.3758, time 134.22ms
iter 4700: loss 1.3735, time 127.92ms
iter 4800: loss 1.3565, time 151.11ms
iter 4900: loss 1.2349, time 133.49ms
step 5000: train loss 1.2652, val loss 1.2675
iter 5000: loss 1.4055, time 56774.20ms
iter 5100: loss 1.3247, time 133.92ms
iter 5200: loss 1.2740, time 137.64ms
iter 5300: loss 1.3218, time 139.75ms
iter 5400: loss 1.3160, time 129.85ms
iter 5500: loss 1.3781, time 132.15ms
iter 5600: loss 1.2681, time 131.25ms
iter 5700: loss 1.3120, time 136.45ms
iter 5800: loss 1.2919, time 127.95ms
iter 5900: loss 1.3549, time 128.22ms
step 6000: train loss 1.2301, val loss 1.2405
iter 6000: loss 1.2846, time 57763.20ms
iter 6100: loss 1.3428, time 128.48ms
iter 6200: loss 1.3677, time 130.80ms
iter 6300: loss 1.2139, time 137.62ms
iter 6400: loss 1.2976, time 130.92ms
iter 6500: loss 1.3160, time 151.82ms
iter 6600: loss 1.2768, time 144.29ms
iter 6700: loss 1.3459, time 150.71ms
iter 6800: loss 1.2999, time 136.98ms
iter 6900: loss 1.3058, time 164.86ms
step 7000: train loss 1.2140, val loss 1.2292
iter 7000: loss 1.2738, time 57817.51ms
iter 7100: loss 1.2538, time 133.40ms
iter 7200: loss 1.1872, time 134.24ms
iter 7300: loss 1.3312, time 137.40ms
iter 7400: loss 1.2677, time 140.22ms
iter 7500: loss 1.3536, time 138.57ms
iter 7600: loss 1.2510, time 140.93ms
iter 7700: loss 1.2244, time 135.15ms
iter 7800: loss 1.3193, time 137.79ms
iter 7900: loss 1.2733, time 154.76ms
step 8000: train loss 1.2003, val loss 1.2105
iter 8000: loss 1.3274, time 57011.05ms
iter 8100: loss 1.2420, time 119.59ms
iter 8200: loss 1.2345, time 143.30ms
iter 8300: loss 1.2681, time 133.20ms
iter 8400: loss 1.1967, time 132.68ms
iter 8500: loss 1.3053, time 149.04ms
iter 8600: loss 1.2147, time 147.62ms
iter 8700: loss 1.2295, time 140.34ms
iter 8800: loss 1.2857, time 152.52ms
iter 8900: loss 1.1987, time 140.20ms
step 9000: train loss 1.1829, val loss 1.1948
iter 9000: loss 1.2053, time 61423.31ms
iter 9100: loss 1.2114, time 151.31ms
iter 9200: loss 1.1875, time 146.68ms
iter 9300: loss 1.2012, time 157.15ms
iter 9400: loss 1.2085, time 145.30ms
iter 9500: loss 1.2230, time 168.38ms
iter 9600: loss 1.2984, time 146.35ms
iter 9700: loss 1.2759, time 154.95ms
iter 9800: loss 1.2593, time 131.05ms
iter 9900: loss 1.2313, time 143.58msRun 1 timed out after 7200 seconds
To address the timeout issue, we can try adjusting the recalibration mechanism to apply less frequently, which might    
help with the convergence time. Let's proceed with Run 4, where we apply the recalibration every 50 iterations instead  
of continuously.                                                                                                        

Here are the changes needed in experiment.py:                                                                           

 1 Introduce a counter to track iterations and apply recalibration every 50 iterations.                                 

Let's implement these changes.                                                                                          

experiment.py                                                                                                           

                                                                                                                        
 <<<<<<< SEARCH                                                                                                         
     def _init_weights(self, module):                                                                                   
         if isinstance(module, nn.Linear):                                                                              
             # Calculate the moving average of gradient norms                                                           
             grad_norms = []                                                                                            
             for p in module.parameters():                                                                              
                 if p.grad is not None:                                                                                 
                     grad_norms.append(p.grad.norm().item())                                                            
             if grad_norms:                                                                                             
                 moving_avg_grad_norm = np.mean(grad_norms[-10:])  # Window size of 10                                  
                 std = 0.02 * moving_avg_grad_norm                                                                      
             else:                                                                                                      
                 std = 0.02                                                                                             
             torch.nn.init.normal_(module.weight, mean=0.0, std=std)                                                    
             if module.bias is not None:                                                                                
                 torch.nn.init.zeros_(module.bias)                                                                      
         elif isinstance(module, nn.Embedding):                                                                         
             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)                                                   
 =======                                                                                                                
     def _init_weights(self, module):                                                                                   
         if isinstance(module, nn.Linear):                                                                              
             # Introduce a counter to apply recalibration every 50 iterations                                           
             if not hasattr(self, 'recalibration_counter'):                                                             
                 self.recalibration_counter = 0                                                                         
                                                                                                                        
             self.recalibration_counter += 1                                                                            
                                                                                                                        
             if self.recalibration_counter % 50 == 0:                                                                   
                 # Calculate the moving average of gradient norms                                                       
                 grad_norms = []                                                                                        
                 for p in module.parameters():                                                                          
                     if p.grad is not None:                                                                             
                         grad_norms.append(p.grad.norm().item())                                                        
                 if grad_norms:                                                                                         
                     moving_avg_grad_norm = np.mean(grad_norms[-10:])  # Window size of 10                              
                     std = 0.02 * moving_avg_grad_norm                                                                  
                 else:                                                                                                  
                     std = 0.02                                                                                         
                 torch.nn.init.normal_(module.weight, mean=0.0, std=std)                                                
             else:                                                                                                      
                 torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)                                               
                                                                                                                        
             if module.bias is not None:                                                                                
                 torch.nn.init.zeros_(module.bias)                                                                      
         elif isinstance(module, nn.Embedding):                                                                         
             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)                                                   
 >>>>>>> REPLACE                                                                                                        
                                                                                                                        

After applying these changes, you can run the experiment with the following command:                                    

                                                                                                                        
 python experiment.py --out_dir=run_4                                                                                   
                                                                                                                        

This will execute the fourth run with the recalibration applied every 50 iterations. Let me know when you're ready for  
the next step!                                                                                                          

Tokens: 12k sent, 601 received. Cost: $0.04 message, $0.10 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_4
To address the timeout issue, we can try adjusting the recalibration mechanism to apply less frequently, which might help with the convergence time. Let's proceed with **Run 4**, where we apply the recalibration every 50 iterations instead of continuously.

Here are the changes needed in `experiment.py`:

1. Introduce a counter to track iterations and apply recalibration every 50 iterations.

Let's implement these changes.

experiment.py
```python
<<<<<<< SEARCH
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            # Calculate the moving average of gradient norms
            grad_norms = []
            for p in module.parameters():
                if p.grad is not None:
                    grad_norms.append(p.grad.norm().item())
            if grad_norms:
                moving_avg_grad_norm = np.mean(grad_norms[-10:])  # Window size of 10
                std = 0.02 * moving_avg_grad_norm
            else:
                std = 0.02
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
=======
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            # Introduce a counter to apply recalibration every 50 iterations
            if not hasattr(self, 'recalibration_counter'):
                self.recalibration_counter = 0

            self.recalibration_counter += 1

            if self.recalibration_counter % 50 == 0:
                # Calculate the moving average of gradient norms
                grad_norms = []
                for p in module.parameters():
                    if p.grad is not None:
                        grad_norms.append(p.grad.norm().item())
                if grad_norms:
                    moving_avg_grad_norm = np.mean(grad_norms[-10:])  # Window size of 10
                    std = 0.02 * moving_avg_grad_norm
                else:
                    std = 0.02
                torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            else:
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
>>>>>>> REPLACE
```

After applying these changes, you can run the experiment with the following command:

```bash
python experiment.py --out_dir=run_4
```

This will execute the fourth run with the recalibration applied every 50 iterations. Let me know when you're ready for the next step!
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2874, val loss 4.2823
iter 0: loss 4.2654, time 29806.07ms
iter 10: loss 3.2457, time 73.08ms
iter 20: loss 2.7913, time 84.25ms
iter 30: loss 2.6356, time 86.46ms
iter 40: loss 2.5776, time 75.35ms
iter 50: loss 2.5276, time 73.81ms
iter 60: loss 2.5195, time 74.62ms
iter 70: loss 2.4964, time 83.31ms
iter 80: loss 2.4972, time 71.16ms
iter 90: loss 2.4684, time 80.81ms
iter 100: loss 2.4579, time 83.92ms
iter 110: loss 2.4639, time 81.89ms
iter 120: loss 2.4277, time 78.87ms
iter 130: loss 2.4129, time 78.24ms
iter 140: loss 2.4068, time 75.54ms
iter 150: loss 2.4192, time 82.62ms
iter 160: loss 2.3703, time 81.88ms
iter 170: loss 2.3495, time 93.54ms
iter 180: loss 2.3070, time 94.42ms
iter 190: loss 2.2480, time 73.00ms
iter 200: loss 2.2175, time 63.45ms
iter 210: loss 2.1400, time 78.84ms
iter 220: loss 2.1433, time 75.07ms
iter 230: loss 2.0665, time 78.92ms
iter 240: loss 2.0847, time 74.50ms
step 250: train loss 1.9665, val loss 2.0636
iter 250: loss 2.0338, time 21600.16ms
iter 260: loss 1.9765, time 76.40ms
iter 270: loss 1.9818, time 81.42ms
iter 280: loss 1.9783, time 81.95ms
iter 290: loss 1.9182, time 74.02ms
iter 300: loss 1.9039, time 76.77ms
iter 310: loss 1.8655, time 79.11ms
iter 320: loss 1.8487, time 71.70ms
iter 330: loss 1.8208, time 76.55ms
iter 340: loss 1.7866, time 86.34ms
iter 350: loss 1.8215, time 82.39ms
iter 360: loss 1.7805, time 73.09ms
iter 370: loss 1.7441, time 79.04ms
iter 380: loss 1.7285, time 85.72ms
iter 390: loss 1.7338, time 76.11ms
iter 400: loss 1.7702, time 76.68ms
iter 410: loss 1.7030, time 80.06ms
iter 420: loss 1.7201, time 70.34ms
iter 430: loss 1.6906, time 79.56ms
iter 440: loss 1.6529, time 86.74ms
iter 450: loss 1.6566, time 79.56ms
iter 460: loss 1.6023, time 68.82ms
iter 470: loss 1.6488, time 80.04ms
iter 480: loss 1.6234, time 72.68ms
iter 490: loss 1.6050, time 78.89ms
step 500: train loss 1.5248, val loss 1.7229
iter 500: loss 1.6012, time 21883.53ms
iter 510: loss 1.6144, time 75.42ms
iter 520: loss 1.5933, time 83.66ms
iter 530: loss 1.5662, time 79.91ms
iter 540: loss 1.6240, time 88.71ms
iter 550: loss 1.5690, time 78.05ms
iter 560: loss 1.5673, time 78.08ms
iter 570: loss 1.5684, time 69.13ms
iter 580: loss 1.5333, time 76.34ms
iter 590: loss 1.5023, time 73.49ms
iter 600: loss 1.5178, time 79.42ms
iter 610: loss 1.5515, time 77.14ms
iter 620: loss 1.5353, time 73.30ms
iter 630: loss 1.5116, time 78.80ms
iter 640: loss 1.4771, time 81.00ms
iter 650: loss 1.5036, time 95.08ms
iter 660: loss 1.5143, time 78.92ms
iter 670: loss 1.4487, time 79.71ms
iter 680: loss 1.5083, time 80.40ms
iter 690: loss 1.4666, time 75.72ms
iter 700: loss 1.4916, time 79.63ms
iter 710: loss 1.4646, time 74.05ms
iter 720: loss 1.4385, time 77.94ms
iter 730: loss 1.4263, time 93.19ms
iter 740: loss 1.4308, time 81.85ms
step 750: train loss 1.3650, val loss 1.5929
iter 750: loss 1.4277, time 21405.80ms
iter 760: loss 1.4477, time 59.56ms
iter 770: loss 1.4268, time 77.25ms
iter 780: loss 1.4166, time 74.81ms
iter 790: loss 1.4151, time 78.55ms
iter 800: loss 1.4358, time 87.21ms
iter 810: loss 1.4165, time 78.67ms
iter 820: loss 1.4192, time 82.02ms
iter 830: loss 1.3987, time 80.49ms
iter 840: loss 1.4025, time 84.10ms
iter 850: loss 1.3993, time 81.58ms
iter 860: loss 1.3995, time 76.58ms
iter 870: loss 1.4022, time 96.26ms
iter 880: loss 1.3761, time 84.61ms
iter 890: loss 1.3928, time 87.68ms
iter 900: loss 1.3692, time 75.37ms
iter 910: loss 1.3242, time 77.21ms
iter 920: loss 1.3639, time 80.95ms
iter 930: loss 1.3589, time 73.34ms
iter 940: loss 1.3476, time 78.12ms
iter 950: loss 1.3523, time 81.69ms
iter 960: loss 1.3654, time 80.96ms
iter 970: loss 1.3586, time 78.80ms
iter 980: loss 1.3576, time 74.50ms
iter 990: loss 1.3416, time 82.42ms
step 1000: train loss 1.2753, val loss 1.5318
iter 1000: loss 1.3436, time 21380.35ms
iter 1010: loss 1.3362, time 84.86ms
iter 1020: loss 1.3136, time 80.40ms
iter 1030: loss 1.3377, time 84.77ms
iter 1040: loss 1.3639, time 78.88ms
iter 1050: loss 1.2955, time 86.99ms
iter 1060: loss 1.3402, time 81.21ms
iter 1070: loss 1.3333, time 73.70ms
iter 1080: loss 1.3455, time 76.50ms
iter 1090: loss 1.3525, time 73.41ms
iter 1100: loss 1.3189, time 82.75ms
iter 1110: loss 1.2998, time 74.70ms
iter 1120: loss 1.3015, time 77.84ms
iter 1130: loss 1.2964, time 78.92ms
iter 1140: loss 1.2991, time 87.83ms
iter 1150: loss 1.3066, time 85.77ms
iter 1160: loss 1.3298, time 86.56ms
iter 1170: loss 1.3023, time 77.09ms
iter 1180: loss 1.3237, time 79.45ms
iter 1190: loss 1.2709, time 74.65ms
iter 1200: loss 1.2838, time 76.84ms
iter 1210: loss 1.2668, time 75.98ms
iter 1220: loss 1.3057, time 73.22ms
iter 1230: loss 1.3003, time 67.77ms
iter 1240: loss 1.2999, time 71.28ms
step 1250: train loss 1.2062, val loss 1.5055
iter 1250: loss 1.2776, time 22178.82ms
iter 1260: loss 1.2832, time 81.25ms
iter 1270: loss 1.2660, time 74.32ms
iter 1280: loss 1.2586, time 76.69ms
iter 1290: loss 1.2918, time 70.61ms
iter 1300: loss 1.3063, time 81.61ms
iter 1310: loss 1.2413, time 76.46ms
iter 1320: loss 1.3114, time 92.47ms
iter 1330: loss 1.2669, time 87.01ms
iter 1340: loss 1.3002, time 86.02ms
iter 1350: loss 1.2568, time 79.43ms
iter 1360: loss 1.2816, time 81.52ms
iter 1370: loss 1.2512, time 72.97ms
iter 1380: loss 1.2650, time 73.49ms
iter 1390: loss 1.2520, time 74.12ms
iter 1400: loss 1.2590, time 79.62ms
iter 1410: loss 1.2470, time 74.56ms
iter 1420: loss 1.2743, time 82.95ms
iter 1430: loss 1.2412, time 83.20ms
iter 1440: loss 1.2598, time 79.87ms
iter 1450: loss 1.2334, time 79.82ms
iter 1460: loss 1.2445, time 83.95ms
iter 1470: loss 1.2235, time 71.83ms
iter 1480: loss 1.2184, time 69.41ms
iter 1490: loss 1.2425, time 71.10ms
step 1500: train loss 1.1519, val loss 1.4876
iter 1500: loss 1.1892, time 22008.62ms
iter 1510: loss 1.2411, time 78.40ms
iter 1520: loss 1.2239, time 79.51ms
iter 1530: loss 1.2579, time 71.12ms
iter 1540: loss 1.1980, time 68.94ms
iter 1550: loss 1.2361, time 75.51ms
iter 1560: loss 1.2099, time 71.31ms
iter 1570: loss 1.2349, time 73.31ms
iter 1580: loss 1.2138, time 66.92ms
iter 1590: loss 1.1930, time 71.19ms
iter 1600: loss 1.2003, time 72.30ms
iter 1610: loss 1.2351, time 78.31ms
iter 1620: loss 1.1858, time 72.18ms
iter 1630: loss 1.2069, time 79.80ms
iter 1640: loss 1.2028, time 73.92ms
iter 1650: loss 1.1799, time 77.35ms
iter 1660: loss 1.2236, time 74.40ms
iter 1670: loss 1.2025, time 71.94ms
iter 1680: loss 1.2010, time 73.34ms
iter 1690: loss 1.2024, time 71.41ms
iter 1700: loss 1.1768, time 74.49ms
iter 1710: loss 1.1831, time 70.22ms
iter 1720: loss 1.1816, time 66.11ms
iter 1730: loss 1.2053, time 73.00ms
iter 1740: loss 1.1720, time 73.82ms
step 1750: train loss 1.1045, val loss 1.4642
iter 1750: loss 1.1902, time 22223.39ms
iter 1760: loss 1.1877, time 74.64ms
iter 1770: loss 1.1987, time 76.52ms
iter 1780: loss 1.1913, time 79.58ms
iter 1790: loss 1.1965, time 80.18ms
iter 1800: loss 1.1803, time 75.64ms
iter 1810: loss 1.1660, time 74.07ms
iter 1820: loss 1.1701, time 77.19ms
iter 1830: loss 1.1700, time 82.18ms
iter 1840: loss 1.1662, time 63.65ms
iter 1850: loss 1.1600, time 73.87ms
iter 1860: loss 1.1807, time 71.18ms
iter 1870: loss 1.1411, time 72.42ms
iter 1880: loss 1.1837, time 73.82ms
iter 1890: loss 1.1819, time 77.29ms
iter 1900: loss 1.1304, time 79.36ms
iter 1910: loss 1.1711, time 70.97ms
iter 1920: loss 1.1667, time 73.98ms
iter 1930: loss 1.1528, time 75.67ms
iter 1940: loss 1.1328, time 72.01ms
iter 1950: loss 1.1406, time 67.72ms
iter 1960: loss 1.1555, time 72.23ms
iter 1970: loss 1.1510, time 75.51ms
iter 1980: loss 1.1555, time 84.12ms
iter 1990: loss 1.1514, time 73.27ms
step 2000: train loss 1.0611, val loss 1.4765
iter 2000: loss 1.1344, time 22240.78ms
iter 2010: loss 1.1276, time 69.89ms
iter 2020: loss 1.1199, time 70.58ms
iter 2030: loss 1.1599, time 70.48ms
iter 2040: loss 1.1418, time 71.01ms
iter 2050: loss 1.1168, time 71.71ms
iter 2060: loss 1.1063, time 76.41ms
iter 2070: loss 1.1257, time 78.23ms
iter 2080: loss 1.1176, time 72.19ms
iter 2090: loss 1.1343, time 72.99ms
iter 2100: loss 1.1403, time 68.93ms
iter 2110: loss 1.1382, time 72.44ms
iter 2120: loss 1.1302, time 69.59ms
iter 2130: loss 1.1366, time 69.29ms
iter 2140: loss 1.1426, time 81.34ms
iter 2150: loss 1.1191, time 73.19ms
iter 2160: loss 1.1467, time 72.09ms
iter 2170: loss 1.1396, time 74.09ms
iter 2180: loss 1.1161, time 66.69ms
iter 2190: loss 1.1033, time 74.34ms
iter 2200: loss 1.1209, time 72.92ms
iter 2210: loss 1.1101, time 72.40ms
iter 2220: loss 1.1233, time 74.36ms
iter 2230: loss 1.1178, time 71.37ms
iter 2240: loss 1.1270, time 72.80ms
step 2250: train loss 1.0100, val loss 1.4777
iter 2250: loss 1.1102, time 21970.01ms
iter 2260: loss 1.1070, time 72.93ms
iter 2270: loss 1.1278, time 82.92ms
iter 2280: loss 1.0957, time 77.41ms
iter 2290: loss 1.1483, time 78.88ms
iter 2300: loss 1.1258, time 72.88ms
iter 2310: loss 1.0974, time 71.96ms
iter 2320: loss 1.0974, time 72.65ms
iter 2330: loss 1.1013, time 70.26ms
iter 2340: loss 1.1201, time 69.46ms
iter 2350: loss 1.1025, time 72.69ms
iter 2360: loss 1.1118, time 69.82ms
iter 2370: loss 1.0897, time 74.78ms
iter 2380: loss 1.0832, time 72.18ms
iter 2390: loss 1.0796, time 70.58ms
iter 2400: loss 1.0833, time 78.11ms
iter 2410: loss 1.0734, time 69.99ms
iter 2420: loss 1.0797, time 70.23ms
iter 2430: loss 1.0527, time 71.12ms
iter 2440: loss 1.0580, time 78.94ms
iter 2450: loss 1.0717, time 81.33ms
iter 2460: loss 1.0825, time 82.42ms
iter 2470: loss 1.0872, time 75.93ms
iter 2480: loss 1.0846, time 73.50ms
iter 2490: loss 1.0549, time 77.43ms
step 2500: train loss 0.9604, val loss 1.4975
iter 2500: loss 1.0793, time 22276.28ms
iter 2510: loss 1.0667, time 75.31ms
iter 2520: loss 1.0463, time 74.84ms
iter 2530: loss 1.0603, time 76.98ms
iter 2540: loss 1.0574, time 81.23ms
iter 2550: loss 1.0673, time 82.14ms
iter 2560: loss 1.0563, time 75.95ms
iter 2570: loss 1.0749, time 70.56ms
iter 2580: loss 1.0749, time 72.80ms
iter 2590: loss 1.0680, time 57.80ms
iter 2600: loss 1.0666, time 68.81ms
iter 2610: loss 1.0589, time 79.02ms
iter 2620: loss 1.0447, time 76.77ms
iter 2630: loss 1.0330, time 75.75ms
iter 2640: loss 1.0475, time 74.56ms
iter 2650: loss 1.0659, time 69.19ms
iter 2660: loss 1.0438, time 77.03ms
iter 2670: loss 1.0230, time 71.87ms
iter 2680: loss 1.0483, time 78.43ms
iter 2690: loss 1.0443, time 72.52ms
iter 2700: loss 1.0208, time 71.97ms
iter 2710: loss 1.0460, time 77.01ms
iter 2720: loss 1.0473, time 82.52ms
iter 2730: loss 1.0673, time 74.58ms
iter 2740: loss 1.0251, time 79.49ms
step 2750: train loss 0.9154, val loss 1.5088
iter 2750: loss 1.0377, time 21879.47ms
iter 2760: loss 1.0254, time 75.50ms
iter 2770: loss 1.0293, time 73.08ms
iter 2780: loss 1.0207, time 70.53ms
iter 2790: loss 1.0398, time 75.37ms
iter 2800: loss 1.0139, time 71.37ms
iter 2810: loss 1.0431, time 83.92ms
iter 2820: loss 1.0221, time 74.56ms
iter 2830: loss 1.0343, time 70.89ms
iter 2840: loss 0.9977, time 77.21ms
iter 2850: loss 1.0267, time 79.91ms
iter 2860: loss 1.0233, time 68.84ms
iter 2870: loss 1.0034, time 74.46ms
iter 2880: loss 1.0348, time 77.80ms
iter 2890: loss 1.0031, time 77.61ms
iter 2900: loss 0.9937, time 78.28ms
iter 2910: loss 1.0441, time 70.61ms
iter 2920: loss 1.0130, time 74.76ms
iter 2930: loss 0.9996, time 75.65ms
iter 2940: loss 0.9930, time 83.36ms
iter 2950: loss 1.0236, time 80.54ms
iter 2960: loss 0.9996, time 79.29ms
iter 2970: loss 0.9903, time 75.62ms
iter 2980: loss 1.0035, time 73.88ms
iter 2990: loss 0.9877, time 74.75ms
step 3000: train loss 0.8683, val loss 1.5229
iter 3000: loss 0.9870, time 22089.95ms
iter 3010: loss 1.0045, time 73.00ms
iter 3020: loss 0.9980, time 77.35ms
iter 3030: loss 1.0011, time 71.10ms
iter 3040: loss 1.0230, time 79.05ms
iter 3050: loss 0.9812, time 78.10ms
iter 3060: loss 1.0048, time 76.26ms
iter 3070: loss 1.0176, time 76.76ms
iter 3080: loss 0.9996, time 76.72ms
iter 3090: loss 0.9894, time 66.88ms
iter 3100: loss 0.9978, time 76.38ms
iter 3110: loss 0.9745, time 68.07ms
iter 3120: loss 1.0024, time 78.15ms
iter 3130: loss 0.9834, time 72.87ms
iter 3140: loss 0.9815, time 79.79ms
iter 3150: loss 0.9964, time 70.46ms
iter 3160: loss 1.0043, time 68.87ms
iter 3170: loss 0.9621, time 70.72ms
iter 3180: loss 0.9714, time 72.64ms
iter 3190: loss 0.9943, time 72.75ms
iter 3200: loss 0.9656, time 70.71ms
iter 3210: loss 0.9620, time 72.48ms
iter 3220: loss 0.9640, time 72.52ms
iter 3230: loss 0.9551, time 79.56ms
iter 3240: loss 0.9563, time 77.66ms
step 3250: train loss 0.8246, val loss 1.5541
iter 3250: loss 0.9697, time 22379.62ms
iter 3260: loss 0.9641, time 80.68ms
iter 3270: loss 0.9692, time 72.35ms
iter 3280: loss 0.9537, time 70.46ms
iter 3290: loss 0.9465, time 68.72ms
iter 3300: loss 0.9406, time 78.68ms
iter 3310: loss 0.9535, time 70.55ms
iter 3320: loss 0.9604, time 73.81ms
iter 3330: loss 0.9638, time 73.47ms
iter 3340: loss 0.9572, time 83.02ms
iter 3350: loss 0.9566, time 71.71ms
iter 3360: loss 0.9251, time 70.29ms
iter 3370: loss 0.9532, time 77.09ms
iter 3380: loss 0.9504, time 91.55ms
iter 3390: loss 0.9468, time 72.55ms
iter 3400: loss 0.9597, time 76.15ms
iter 3410: loss 0.9444, time 73.37ms
iter 3420: loss 0.9457, time 69.38ms
iter 3430: loss 0.9491, time 76.32ms
iter 3440: loss 0.9740, time 78.77ms
iter 3450: loss 0.9547, time 76.73ms
iter 3460: loss 0.9498, time 77.49ms
iter 3470: loss 0.9410, time 79.45ms
iter 3480: loss 0.9548, time 75.78ms
iter 3490: loss 0.9113, time 81.43ms
step 3500: train loss 0.7835, val loss 1.5726
iter 3500: loss 0.9134, time 21655.53ms
iter 3510: loss 0.9147, time 74.50ms
iter 3520: loss 0.9113, time 79.54ms
iter 3530: loss 0.9509, time 81.27ms
iter 3540: loss 0.9253, time 69.86ms
iter 3550: loss 0.9316, time 74.43ms
iter 3560: loss 0.9532, time 77.17ms
iter 3570: loss 0.9341, time 69.97ms
iter 3580: loss 0.9281, time 81.22ms
iter 3590: loss 0.9274, time 69.69ms
iter 3600: loss 0.9257, time 74.64ms
iter 3610: loss 0.9186, time 73.02ms
iter 3620: loss 0.9065, time 74.43ms
iter 3630: loss 0.9289, time 80.26ms
iter 3640: loss 0.9182, time 77.52ms
iter 3650: loss 0.9201, time 72.41ms
iter 3660: loss 0.9348, time 74.79ms
iter 3670: loss 0.9428, time 67.48ms
iter 3680: loss 0.9090, time 80.98ms
iter 3690: loss 0.9340, time 74.37ms
iter 3700: loss 0.8748, time 70.64ms
iter 3710: loss 0.8893, time 63.28ms
iter 3720: loss 0.9090, time 78.13ms
iter 3730: loss 0.9036, time 73.97ms
iter 3740: loss 0.9041, time 82.19ms
step 3750: train loss 0.7437, val loss 1.6005
iter 3750: loss 0.8995, time 22647.81ms
iter 3760: loss 0.9375, time 79.75ms
iter 3770: loss 0.9313, time 78.83ms
iter 3780: loss 0.9224, time 72.93ms
iter 3790: loss 0.9036, time 75.13ms
iter 3800: loss 0.9129, time 75.99ms
iter 3810: loss 0.9208, time 74.70ms
iter 3820: loss 0.8871, time 71.65ms
iter 3830: loss 0.9024, time 78.59ms
iter 3840: loss 0.8932, time 69.09ms
iter 3850: loss 0.8893, time 74.89ms
iter 3860: loss 0.8745, time 78.75ms
iter 3870: loss 0.8905, time 75.19ms
iter 3880: loss 0.8892, time 71.60ms
iter 3890: loss 0.8939, time 69.77ms
iter 3900: loss 0.8926, time 70.41ms
iter 3910: loss 0.8892, time 70.70ms
iter 3920: loss 0.8808, time 73.58ms
iter 3930: loss 0.8973, time 79.18ms
iter 3940: loss 0.8838, time 72.11ms
iter 3950: loss 0.8802, time 76.01ms
iter 3960: loss 0.9009, time 75.92ms
iter 3970: loss 0.8899, time 71.40ms
iter 3980: loss 0.9044, time 73.81ms
iter 3990: loss 0.8816, time 65.51ms
step 4000: train loss 0.7099, val loss 1.6183
iter 4000: loss 0.8657, time 21961.39ms
iter 4010: loss 0.8804, time 73.19ms
iter 4020: loss 0.8910, time 73.12ms
iter 4030: loss 0.8889, time 76.61ms
iter 4040: loss 0.8899, time 65.33ms
iter 4050: loss 0.8680, time 71.26ms
iter 4060: loss 0.8649, time 73.94ms
iter 4070: loss 0.8711, time 76.82ms
iter 4080: loss 0.8892, time 75.99ms
iter 4090: loss 0.8469, time 79.91ms
iter 4100: loss 0.9114, time 74.48ms
iter 4110: loss 0.8712, time 74.77ms
iter 4120: loss 0.8803, time 67.10ms
iter 4130: loss 0.8620, time 72.75ms
iter 4140: loss 0.8804, time 70.84ms
iter 4150: loss 0.8752, time 70.46ms
iter 4160: loss 0.8625, time 72.34ms
iter 4170: loss 0.8594, time 77.79ms
iter 4180: loss 0.8732, time 71.74ms
iter 4190: loss 0.8736, time 71.32ms
iter 4200: loss 0.8580, time 69.90ms
iter 4210: loss 0.8676, time 71.32ms
iter 4220: loss 0.8582, time 79.38ms
iter 4230: loss 0.8839, time 72.91ms
iter 4240: loss 0.8705, time 70.87ms
step 4250: train loss 0.6811, val loss 1.6468
iter 4250: loss 0.8745, time 22054.18ms
iter 4260: loss 0.8683, time 81.41ms
iter 4270: loss 0.8635, time 79.65ms
iter 4280: loss 0.8571, time 73.81ms
iter 4290: loss 0.8346, time 75.02ms
iter 4300: loss 0.8317, time 71.81ms
iter 4310: loss 0.8533, time 71.58ms
iter 4320: loss 0.8435, time 64.47ms
iter 4330: loss 0.8605, time 68.55ms
iter 4340: loss 0.8396, time 75.13ms
iter 4350: loss 0.8388, time 68.39ms
iter 4360: loss 0.8596, time 75.19ms
iter 4370: loss 0.8551, time 79.80ms
iter 4380: loss 0.8347, time 81.06ms
iter 4390: loss 0.8651, time 80.48ms
iter 4400: loss 0.8469, time 73.29ms
iter 4410: loss 0.8664, time 77.77ms
iter 4420: loss 0.8659, time 67.63ms
iter 4430: loss 0.8469, time 80.30ms
iter 4440: loss 0.8448, time 78.49ms
iter 4450: loss 0.8556, time 75.38ms
iter 4460: loss 0.8364, time 69.18ms
iter 4470: loss 0.8525, time 75.64ms
iter 4480: loss 0.8414, time 74.79ms
iter 4490: loss 0.8530, time 71.27ms
step 4500: train loss 0.6552, val loss 1.6673
iter 4500: loss 0.8647, time 21916.37ms
iter 4510: loss 0.8501, time 77.67ms
iter 4520: loss 0.8319, time 76.29ms
iter 4530: loss 0.8468, time 70.91ms
iter 4540: loss 0.8490, time 77.22ms
iter 4550: loss 0.8749, time 82.66ms
iter 4560: loss 0.8482, time 73.39ms
iter 4570: loss 0.8453, time 77.82ms
iter 4580: loss 0.8649, time 77.34ms
iter 4590: loss 0.8423, time 73.35ms
iter 4600: loss 0.8290, time 69.55ms
iter 4610: loss 0.8785, time 72.77ms
iter 4620: loss 0.8426, time 72.18ms
iter 4630: loss 0.8200, time 73.98ms
iter 4640: loss 0.8473, time 75.70ms
iter 4650: loss 0.8658, time 63.94ms
iter 4660: loss 0.8461, time 73.30ms
iter 4670: loss 0.8392, time 74.38ms
iter 4680: loss 0.8545, time 70.44ms
iter 4690: loss 0.8489, time 80.10ms
iter 4700: loss 0.8153, time 72.37ms
iter 4710: loss 0.7964, time 85.68ms
iter 4720: loss 0.8295, time 78.50ms
iter 4730: loss 0.8301, time 74.16ms
iter 4740: loss 0.8382, time 75.98ms
step 4750: train loss 0.6383, val loss 1.6798
iter 4750: loss 0.8015, time 21812.31ms
iter 4760: loss 0.8210, time 73.29ms
iter 4770: loss 0.8029, time 74.06ms
iter 4780: loss 0.8123, time 74.94ms
iter 4790: loss 0.8356, time 75.06ms
iter 4800: loss 0.8169, time 78.22ms
iter 4810: loss 0.8494, time 67.31ms
iter 4820: loss 0.8250, time 76.51ms
iter 4830: loss 0.8318, time 69.73ms
iter 4840: loss 0.8152, time 72.00ms
iter 4850: loss 0.8238, time 75.19ms
iter 4860: loss 0.8180, time 80.87ms
iter 4870: loss 0.8076, time 75.39ms
iter 4880: loss 0.8349, time 79.14ms
iter 4890: loss 0.8092, time 71.34ms
iter 4900: loss 0.8013, time 70.84ms
iter 4910: loss 0.8320, time 69.71ms
iter 4920: loss 0.8199, time 75.52ms
iter 4930: loss 0.8110, time 75.48ms
iter 4940: loss 0.7976, time 82.82ms
iter 4950: loss 0.8342, time 66.34ms
iter 4960: loss 0.8353, time 81.34ms
iter 4970: loss 0.7853, time 69.77ms
iter 4980: loss 0.8012, time 74.83ms
iter 4990: loss 0.8255, time 72.36ms
step 5000: train loss 0.6247, val loss 1.6972
iter 5000: loss 0.8262, time 22170.96ms
training done
Best validation loss: 1.4642444849014282
Total train time: 14.26 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 bear the dream of night
To gaze upon the riddle age against the black.
As will I be long with unscourged women of men,
And the most imprisonment of the commonwealth,
Which the common deformity bears and unseen.

LORD ROSS:
The heavens have their words together, and their life.

LORD WILLOUGHBY:
The sullen lies of their drows together,
And base their harms hath strong and ends at his hands.

LORD WILLOUGHBY:
If I refuge her, hear me all this man,
That continue you might have been consent to die.

Inference time: 2.38 seconds
Tokens per second: 210.24
---------------
Sample 2:
 such every criment perform'd
By false foul in the dead bowels of war.

ROMEO:
Alas, I warrant thee, my lord! husband!
Love love, down their sleep, need would have leave to
For fear from thine! there may be soldiers:
And, by consent to my friend, I must end,
I will keep a bastard, and not my knee,
Where every tedious tongues sit this state:
Yet look to be a strict in this night,
That I might fear to die for shame to Warwick's pain,
And therefore chance distingined the Duke of York.

WARWICK:
When
Inference time: 2.23 seconds
Tokens per second: 223.79
---------------
Sample 3:
 bastard lengthen'd his tongue.

ESCALUS:
Why, then be spoken of value? what can you tell him?

ELBOW:
Marry, sir, I sent for the law upon him.

ANGELO:
I will not discredit in all here;
And better be thou at the very place of death,
By all the sovereignty thoughts of the hand,
Let not have made a thousand courtesy,
But one fair man's piteous territories
Find the root of the poor bound of the mind,
To the hearts, and forswore men lives.
Would I were barbariage of my husband,
Who like their substa
Inference time: 2.30 seconds
Tokens per second: 217.19
---------------
Sample 4:
 let us alone;
And therefore is no wife, that is mine.

GLOUCESTER:

KING EDWARD IV:
Ay, there thou liest; but repeal to fall.

LADY GREY:
Nay, though it please you, till thou didst do it.

KING EDWARD IV:
You speak with mine eyes, and I will grant him return.

LADY GREY:
The mayor will not speak to make a long of them.

KING EDWARD IV:
Begun, victory, of thy birth die misplace.

CLARENCE:
There is thy right: and, as thou shouldst die.

GLOUCESTER:
Nay, now thou hast done. Richard, where is thy w
Inference time: 2.19 seconds
Tokens per second: 228.64
---------------
Sample 5:
 will you not?

DUKE VINCENTIO:
Where is the duke?

LUCIO:
Why do you purchase of him?

DUKE VINCENTIO:
None, but the duke. Here comes the duke.

LUCIO:
A pretty thing, in good time.

ISABELLA:
And this sight of request, I cannot live.

DUKE VINCENTIO:
You are a feather of a world, insolent companion.

LUCIO:
'Tis bold, that is not here like your company.

ISABELLA:
Not accuse my buttered with diseasesting,
Nor counsel to help me on your love, that he
Would say that time had given me leave on my 
Inference time: 2.37 seconds
Tokens per second: 210.84
---------------
Sample 6:
 my father's son,
She would be king, and never broke it to me.

EXETER:
Good counsel, even to my way so,
And we meet her in Dickness of Gloucester,
One that lives in fear: some shall be a wife
With contradictions show a tabour of his babe,
Which fear'd the fool with sullen blood dwells
Could be thus provertied.

HERMIONE:
There's some ill:
Do ever stamp calls it.

LEONTES:
Pray, let's home;
The rather by two children he that bears it,
That we never shall all.

ANTIGONUS:
If he had not disciplined
Inference time: 2.38 seconds
Tokens per second: 210.27
---------------
Sample 7:
 with a man in his limbs
Than can make an egg a treaty of them!

TYBALT:
Ay, in good time, we'll have too much distracted.

MERCUTIO:
And wrinkled with a liberal times.

BENVOLIO:
Soft, a sir; a woman and more quarter.

MERCUTIO:
A sail, a shame! a scratch, a scrape, a soldier's
choks off; a some sail, some corse; a sir, such a
motion of my gentlewoman, a fisher, if my
friends it should be a thing that appeared to the first, I say.

MERCUTIO:
Pray, you, let's see how that be sin, we'll buy for yo
Inference time: 2.40 seconds
Tokens per second: 208.38
---------------
Sample 8:
 do it well, my lord, the king for me.

EXETER:
I protest, so do now I think.

KING EDWARD IV:
Welcome, my lord, to London press to London;
And therefore I'll stay to London on,
And with that harvest thou mayst consent the rest.

KING EDWARD IV:
Now, brother Richard, lords, how have I the wrong?

GLOUCESTER:
But hadst thou need of his true knighthood
That ever made me in his happy days to tell.

QUEEN MARGARET:
Thy father didst thou love me here in thy slave?

KING EDWARD IV:
Thou art not wont th
Inference time: 2.29 seconds
Tokens per second: 218.56
---------------
Sample 9:
 slanderous men, and make pronounce of my son;
For that I must not say 'tis cannot be so bleved.

KING HENRY VI:
Ah, know you not my wife, since we should stand to die.

RATCLIFF:
My lord! night, for all this I do love thee well.

KING RICHARD III:
'Zounds, madam! we must know myself again.

BUCKINGHAM:
Go hence, go, and I am so too high and fortune.

KING RICHARD III:
I have; as but done, as thou canst do it.

BUCKINGHAM:
O, how chance is it, my lord! I have little learn to
return to him that wa
Inference time: 2.33 seconds
Tokens per second: 214.79
---------------
Sample 10:
 mine, sir,
Your son, only clouded ministers, and me
To say 'nothing down,' madam,' I doubt not
At once but the councey and show now
So easy our country's enemy: so true me
Defy his banishment that does me so,
And make one that all my faults I did see
And stand last not that I am. Farewell, farewell.
Good friar; I prithee, come; I'll be lest mended
In good time.

DUKE VINCENTIO:
In brief, I am going to prison with her:
She is brief, and here to make her action.

LUCIO:
This is a pretty prince, si
Inference time: 2.21 seconds
Tokens per second: 226.65
---------------
Average tokens per second: 216.94
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2372, val loss 4.2295
iter 0: loss 4.2337, time 22095.81ms
iter 10: loss 3.2253, time 74.91ms
iter 20: loss 2.7688, time 79.76ms
iter 30: loss 2.6201, time 79.36ms
iter 40: loss 2.5385, time 75.80ms
iter 50: loss 2.5372, time 65.09ms
iter 60: loss 2.4993, time 78.09ms
iter 70: loss 2.4941, time 69.44ms
iter 80: loss 2.4953, time 74.84ms
iter 90: loss 2.4799, time 80.94ms
iter 100: loss 2.4716, time 71.30ms
iter 110: loss 2.4354, time 73.43ms
iter 120: loss 2.4512, time 65.63ms
iter 130: loss 2.4289, time 73.24ms
iter 140: loss 2.4244, time 71.12ms
iter 150: loss 2.3580, time 81.85ms
iter 160: loss 2.3853, time 77.23ms
iter 170: loss 2.3355, time 83.11ms
iter 180: loss 2.3277, time 77.19ms
iter 190: loss 2.2834, time 70.57ms
iter 200: loss 2.2317, time 70.98ms
iter 210: loss 2.1658, time 71.67ms
iter 220: loss 2.1234, time 74.23ms
iter 230: loss 2.0878, time 71.65ms
iter 240: loss 2.0561, time 77.80ms
step 250: train loss 1.9745, val loss 2.0747
iter 250: loss 2.0256, time 21754.64ms
iter 260: loss 2.0085, time 71.66ms
iter 270: loss 1.9482, time 75.04ms
iter 280: loss 1.9381, time 93.98ms
iter 290: loss 1.9152, time 85.01ms
iter 300: loss 1.9030, time 73.23ms
iter 310: loss 1.9071, time 83.61ms
iter 320: loss 1.8564, time 71.72ms
iter 330: loss 1.8579, time 78.57ms
iter 340: loss 1.7966, time 72.87ms
iter 350: loss 1.7870, time 72.46ms
iter 360: loss 1.7866, time 78.26ms
iter 370: loss 1.7800, time 68.87ms
iter 380: loss 1.7466, time 65.92ms
iter 390: loss 1.7311, time 68.02ms
iter 400: loss 1.7082, time 72.81ms
iter 410: loss 1.7050, time 72.41ms
iter 420: loss 1.7058, time 73.70ms
iter 430: loss 1.6868, time 77.59ms
iter 440: loss 1.6612, time 82.53ms
iter 450: loss 1.6408, time 71.13ms
iter 460: loss 1.6847, time 66.32ms
iter 470: loss 1.6443, time 72.97ms
iter 480: loss 1.6530, time 83.55ms
iter 490: loss 1.6058, time 76.70ms
step 500: train loss 1.5423, val loss 1.7407
iter 500: loss 1.6171, time 22012.40ms
iter 510: loss 1.6062, time 72.83ms
iter 520: loss 1.5753, time 75.62ms
iter 530: loss 1.5947, time 71.54ms
iter 540: loss 1.5544, time 75.88ms
iter 550: loss 1.5538, time 78.04ms
iter 560: loss 1.5576, time 74.25ms
iter 570: loss 1.5735, time 75.31ms
iter 580: loss 1.5170, time 70.31ms
iter 590: loss 1.5419, time 62.43ms
iter 600: loss 1.5050, time 74.85ms
iter 610: loss 1.5410, time 69.77ms
iter 620: loss 1.4876, time 73.67ms
iter 630: loss 1.5503, time 74.45ms
iter 640: loss 1.4667, time 70.46ms
iter 650: loss 1.4984, time 73.39ms
iter 660: loss 1.4891, time 72.34ms
iter 670: loss 1.4910, time 71.51ms
iter 680: loss 1.4559, time 71.62ms
iter 690: loss 1.4633, time 74.27ms
iter 700: loss 1.4618, time 72.89ms
iter 710: loss 1.4883, time 79.78ms
iter 720: loss 1.4507, time 80.16ms
iter 730: loss 1.4491, time 68.23ms
iter 740: loss 1.4389, time 70.32ms
step 750: train loss 1.3634, val loss 1.5916
iter 750: loss 1.4449, time 22347.95ms
iter 760: loss 1.4021, time 77.49ms
iter 770: loss 1.4217, time 82.59ms
iter 780: loss 1.4307, time 74.53ms
iter 790: loss 1.3923, time 73.42ms
iter 800: loss 1.4252, time 61.26ms
iter 810: loss 1.4362, time 64.76ms
iter 820: loss 1.4030, time 61.06ms
iter 830: loss 1.4047, time 62.10ms
iter 840: loss 1.3999, time 65.19ms
iter 850: loss 1.4259, time 59.66ms
iter 860: loss 1.3770, time 72.97ms
iter 870: loss 1.3988, time 72.26ms
iter 880: loss 1.3620, time 71.26ms
iter 890: loss 1.3753, time 75.31ms
iter 900: loss 1.3541, time 79.41ms
iter 910: loss 1.3981, time 89.03ms
iter 920: loss 1.3759, time 74.91ms
iter 930: loss 1.3580, time 76.15ms
iter 940: loss 1.3733, time 73.56ms
iter 950: loss 1.3521, time 77.58ms
iter 960: loss 1.3902, time 71.50ms
iter 970: loss 1.3728, time 73.46ms
iter 980: loss 1.3704, time 75.48ms
iter 990: loss 1.3105, time 73.23ms
step 1000: train loss 1.2707, val loss 1.5123
iter 1000: loss 1.3371, time 21853.81ms
iter 1010: loss 1.3626, time 71.76ms
iter 1020: loss 1.3472, time 72.38ms
iter 1030: loss 1.3203, time 73.90ms
iter 1040: loss 1.3074, time 72.56ms
iter 1050: loss 1.2975, time 68.64ms
iter 1060: loss 1.3108, time 75.96ms
iter 1070: loss 1.3234, time 80.11ms
iter 1080: loss 1.3160, time 81.59ms
iter 1090: loss 1.3662, time 76.02ms
iter 1100: loss 1.2911, time 72.14ms
iter 1110: loss 1.3132, time 78.04ms
iter 1120: loss 1.2981, time 76.85ms
iter 1130: loss 1.3055, time 78.71ms
iter 1140: loss 1.2998, time 81.40ms
iter 1150: loss 1.2843, time 77.79ms
iter 1160: loss 1.2835, time 72.29ms
iter 1170: loss 1.2736, time 73.22ms
iter 1180: loss 1.2759, time 67.88ms
iter 1190: loss 1.3311, time 77.93ms
iter 1200: loss 1.2969, time 77.06ms
iter 1210: loss 1.3049, time 76.27ms
iter 1220: loss 1.2890, time 82.36ms
iter 1230: loss 1.2485, time 77.54ms
iter 1240: loss 1.2654, time 85.63ms
step 1250: train loss 1.2086, val loss 1.5001
iter 1250: loss 1.3045, time 22353.52ms
iter 1260: loss 1.2941, time 73.10ms
iter 1270: loss 1.2942, time 77.12ms
iter 1280: loss 1.2678, time 77.94ms
iter 1290: loss 1.2995, time 80.25ms
iter 1300: loss 1.2943, time 77.93ms
iter 1310: loss 1.2510, time 83.48ms
iter 1320: loss 1.2896, time 73.84ms
iter 1330: loss 1.2698, time 78.22ms
iter 1340: loss 1.2501, time 84.19ms
iter 1350: loss 1.2499, time 79.72ms
iter 1360: loss 1.2949, time 78.93ms
iter 1370: loss 1.2671, time 82.39ms
iter 1380: loss 1.2455, time 74.36ms
iter 1390: loss 1.2667, time 72.12ms
iter 1400: loss 1.2650, time 74.59ms
iter 1410: loss 1.2609, time 76.27ms
iter 1420: loss 1.2379, time 85.15ms
iter 1430: loss 1.2345, time 74.78ms
iter 1440: loss 1.2169, time 76.57ms
iter 1450: loss 1.2693, time 80.93ms
iter 1460: loss 1.2337, time 72.35ms
iter 1470: loss 1.2227, time 78.75ms
iter 1480: loss 1.2209, time 75.87ms
iter 1490: loss 1.2520, time 73.70ms
step 1500: train loss 1.1534, val loss 1.4734
iter 1500: loss 1.2450, time 22379.58ms
iter 1510: loss 1.2298, time 72.99ms
iter 1520: loss 1.2418, time 75.76ms
iter 1530: loss 1.2182, time 75.72ms
iter 1540: loss 1.2504, time 77.22ms
iter 1550: loss 1.2187, time 86.79ms
iter 1560: loss 1.2566, time 77.19ms
iter 1570: loss 1.2072, time 69.34ms
iter 1580: loss 1.2020, time 72.42ms
iter 1590: loss 1.1931, time 76.72ms
iter 1600: loss 1.2095, time 77.17ms
iter 1610: loss 1.1825, time 69.49ms
iter 1620: loss 1.2160, time 64.60ms
iter 1630: loss 1.2282, time 78.81ms
iter 1640: loss 1.2229, time 78.21ms
iter 1650: loss 1.1961, time 74.90ms
iter 1660: loss 1.1958, time 84.00ms
iter 1670: loss 1.2196, time 80.84ms
iter 1680: loss 1.1741, time 83.49ms
iter 1690: loss 1.1855, time 75.11ms
iter 1700: loss 1.1834, time 78.46ms
iter 1710: loss 1.1538, time 74.35ms
iter 1720: loss 1.1696, time 74.46ms
iter 1730: loss 1.1916, time 67.77ms
iter 1740: loss 1.1969, time 78.39ms
step 1750: train loss 1.1059, val loss 1.4753
iter 1750: loss 1.1918, time 22044.44ms
iter 1760: loss 1.1978, time 75.91ms
iter 1770: loss 1.1919, time 71.04ms
iter 1780: loss 1.1429, time 73.06ms
iter 1790: loss 1.1774, time 77.83ms
iter 1800: loss 1.1444, time 81.88ms
iter 1810: loss 1.1736, time 76.06ms
iter 1820: loss 1.1768, time 75.55ms
iter 1830: loss 1.1944, time 73.80ms
iter 1840: loss 1.1600, time 80.49ms
iter 1850: loss 1.1785, time 75.20ms
iter 1860: loss 1.2159, time 76.63ms
iter 1870: loss 1.1768, time 77.90ms
iter 1880: loss 1.1521, time 73.12ms
iter 1890: loss 1.1576, time 72.50ms
iter 1900: loss 1.1550, time 71.82ms
iter 1910: loss 1.1607, time 71.59ms
iter 1920: loss 1.1484, time 72.70ms
iter 1930: loss 1.1763, time 72.40ms
iter 1940: loss 1.1518, time 67.13ms
iter 1950: loss 1.1324, time 69.97ms
iter 1960: loss 1.1534, time 74.10ms
iter 1970: loss 1.1422, time 73.56ms
iter 1980: loss 1.1964, time 70.94ms
iter 1990: loss 1.1410, time 71.94ms
step 2000: train loss 1.0599, val loss 1.4760
iter 2000: loss 1.1291, time 22091.36ms
iter 2010: loss 1.1444, time 70.45ms
iter 2020: loss 1.1294, time 66.70ms
iter 2030: loss 1.1418, time 68.76ms
iter 2040: loss 1.1374, time 68.36ms
iter 2050: loss 1.1193, time 66.71ms
iter 2060: loss 1.1145, time 75.30ms
iter 2070: loss 1.1411, time 74.54ms
iter 2080: loss 1.1082, time 70.43ms
iter 2090: loss 1.1286, time 74.58ms
iter 2100: loss 1.1379, time 71.51ms
iter 2110: loss 1.1429, time 70.51ms
iter 2120: loss 1.1461, time 68.73ms
iter 2130: loss 1.1406, time 68.57ms
iter 2140: loss 1.1069, time 68.60ms
iter 2150: loss 1.1105, time 86.11ms
iter 2160: loss 1.0973, time 82.41ms
iter 2170: loss 1.1147, time 80.49ms
iter 2180: loss 1.1206, time 75.52ms
iter 2190: loss 1.1537, time 72.20ms
iter 2200: loss 1.1369, time 77.70ms
iter 2210: loss 1.1257, time 78.09ms
iter 2220: loss 1.1204, time 72.11ms
iter 2230: loss 1.0985, time 71.29ms
iter 2240: loss 1.1168, time 67.90ms
step 2250: train loss 1.0079, val loss 1.4895
iter 2250: loss 1.1241, time 21905.62ms
iter 2260: loss 1.1102, time 82.87ms
iter 2270: loss 1.0999, time 73.28ms
iter 2280: loss 1.0833, time 76.82ms
iter 2290: loss 1.0842, time 77.47ms
iter 2300: loss 1.1302, time 76.17ms
iter 2310: loss 1.0861, time 72.35ms
iter 2320: loss 1.0804, time 78.53ms
iter 2330: loss 1.0927, time 71.58ms
iter 2340: loss 1.0971, time 73.42ms
iter 2350: loss 1.0744, time 78.81ms
iter 2360: loss 1.0910, time 70.22ms
iter 2370: loss 1.1013, time 68.40ms
iter 2380: loss 1.0902, time 75.89ms
iter 2390: loss 1.0867, time 79.10ms
iter 2400: loss 1.0498, time 77.49ms
iter 2410: loss 1.0925, time 75.26ms
iter 2420: loss 1.0763, time 75.81ms
iter 2430: loss 1.0563, time 80.95ms
iter 2440: loss 1.0519, time 73.68ms
iter 2450: loss 1.0820, time 76.71ms
iter 2460: loss 1.0837, time 72.79ms
iter 2470: loss 1.0828, time 70.59ms
iter 2480: loss 1.0579, time 69.43ms
iter 2490: loss 1.0704, time 74.04ms
step 2500: train loss 0.9574, val loss 1.4915
iter 2500: loss 1.0722, time 22195.28ms
iter 2510: loss 1.0544, time 73.84ms
iter 2520: loss 1.0712, time 74.57ms
iter 2530: loss 1.0760, time 72.61ms
iter 2540: loss 1.0436, time 73.65ms
iter 2550: loss 1.0688, time 73.22ms
iter 2560: loss 1.0565, time 67.15ms
iter 2570: loss 1.0473, time 71.69ms
iter 2580: loss 1.0519, time 72.63ms
iter 2590: loss 1.0640, time 71.64ms
iter 2600: loss 1.0604, time 73.08ms
iter 2610: loss 1.0289, time 85.63ms
iter 2620: loss 1.0506, time 80.96ms
iter 2630: loss 1.0616, time 75.77ms
iter 2640: loss 1.0211, time 74.69ms
iter 2650: loss 1.0331, time 78.07ms
iter 2660: loss 1.0519, time 72.65ms
iter 2670: loss 1.0436, time 77.28ms
iter 2680: loss 1.0343, time 79.14ms
iter 2690: loss 1.0511, time 75.49ms
iter 2700: loss 1.0031, time 73.63ms
iter 2710: loss 1.0292, time 71.74ms
iter 2720: loss 0.9926, time 81.39ms
iter 2730: loss 1.0395, time 75.00ms
iter 2740: loss 1.0300, time 76.34ms
step 2750: train loss 0.9083, val loss 1.5167
iter 2750: loss 1.0495, time 22183.28ms
iter 2760: loss 1.0124, time 72.47ms
iter 2770: loss 1.0383, time 73.02ms
iter 2780: loss 1.0345, time 72.68ms
iter 2790: loss 1.0246, time 73.00ms
iter 2800: loss 1.0230, time 71.34ms
iter 2810: loss 1.0209, time 74.38ms
iter 2820: loss 1.0354, time 75.81ms
iter 2830: loss 0.9880, time 72.94ms
iter 2840: loss 1.0148, time 81.75ms
iter 2850: loss 1.0173, time 72.69ms
iter 2860: loss 0.9945, time 78.38ms
iter 2870: loss 1.0194, time 88.79ms
iter 2880: loss 1.0255, time 76.13ms
iter 2890: loss 1.0099, time 76.45ms
iter 2900: loss 1.0258, time 70.78ms
iter 2910: loss 1.0169, time 69.36ms
iter 2920: loss 0.9782, time 63.92ms
iter 2930: loss 1.0002, time 77.25ms
iter 2940: loss 0.9959, time 72.59ms
iter 2950: loss 0.9917, time 80.22ms
iter 2960: loss 1.0014, time 79.07ms
iter 2970: loss 0.9910, time 78.52ms
iter 2980: loss 1.0187, time 73.45ms
iter 2990: loss 0.9870, time 68.02ms
step 3000: train loss 0.8639, val loss 1.5393
iter 3000: loss 0.9876, time 22386.29ms
iter 3010: loss 1.0153, time 78.15ms
iter 3020: loss 1.0072, time 83.09ms
iter 3030: loss 1.0091, time 84.35ms
iter 3040: loss 0.9850, time 80.18ms
iter 3050: loss 0.9911, time 69.08ms
iter 3060: loss 0.9995, time 71.22ms
iter 3070: loss 0.9869, time 73.71ms
iter 3080: loss 0.9964, time 76.27ms
iter 3090: loss 0.9971, time 68.60ms
iter 3100: loss 0.9818, time 75.58ms
iter 3110: loss 1.0026, time 74.54ms
iter 3120: loss 0.9958, time 69.88ms
iter 3130: loss 0.9904, time 70.60ms
iter 3140: loss 0.9886, time 60.31ms
iter 3150: loss 0.9593, time 62.51ms
iter 3160: loss 0.9734, time 58.62ms
iter 3170: loss 0.9532, time 73.20ms
iter 3180: loss 0.9504, time 65.35ms
iter 3190: loss 0.9802, time 77.59ms
iter 3200: loss 0.9895, time 80.55ms
iter 3210: loss 0.9980, time 66.20ms
iter 3220: loss 0.9822, time 78.76ms
iter 3230: loss 0.9839, time 76.40ms
iter 3240: loss 0.9477, time 78.20ms
step 3250: train loss 0.8152, val loss 1.5669
iter 3250: loss 0.9856, time 22128.43ms
iter 3260: loss 0.9745, time 72.61ms
iter 3270: loss 0.9657, time 70.36ms
iter 3280: loss 0.9583, time 64.70ms
iter 3290: loss 0.9548, time 74.48ms
iter 3300: loss 0.9523, time 72.35ms
iter 3310: loss 0.9645, time 77.30ms
iter 3320: loss 0.9473, time 73.61ms
iter 3330: loss 0.9462, time 77.29ms
iter 3340: loss 0.9627, time 74.41ms
iter 3350: loss 0.9721, time 87.27ms
iter 3360: loss 0.9336, time 82.28ms
iter 3370: loss 0.9362, time 71.55ms
iter 3380: loss 0.9453, time 75.42ms
iter 3390: loss 0.9425, time 76.07ms
iter 3400: loss 0.9398, time 74.76ms
iter 3410: loss 0.9545, time 72.37ms
iter 3420: loss 0.9340, time 72.14ms
iter 3430: loss 0.9273, time 76.50ms
iter 3440: loss 0.9339, time 75.91ms
iter 3450: loss 0.9216, time 73.92ms
iter 3460: loss 0.9754, time 74.90ms
iter 3470: loss 0.9207, time 80.29ms
iter 3480: loss 0.9341, time 87.02ms
iter 3490: loss 0.9511, time 94.91ms
step 3500: train loss 0.7748, val loss 1.5906
iter 3500: loss 0.9186, time 21748.19ms
iter 3510: loss 0.9513, time 75.45ms
iter 3520: loss 0.9109, time 73.36ms
iter 3530: loss 0.9203, time 79.19ms
iter 3540: loss 0.9427, time 79.48ms
iter 3550: loss 0.9204, time 76.01ms
iter 3560: loss 0.9490, time 69.15ms
iter 3570: loss 0.9249, time 73.48ms
iter 3580: loss 0.9229, time 78.66ms
iter 3590: loss 0.9423, time 71.93ms
iter 3600: loss 0.9068, time 76.68ms
iter 3610: loss 0.9301, time 70.84ms
iter 3620: loss 0.9446, time 71.84ms
iter 3630: loss 0.9047, time 84.12ms
iter 3640: loss 0.9250, time 85.44ms
iter 3650: loss 0.9309, time 73.61ms
iter 3660: loss 0.9172, time 66.83ms
iter 3670: loss 0.9102, time 71.78ms
iter 3680: loss 0.8918, time 76.09ms
iter 3690: loss 0.9115, time 68.04ms
iter 3700: loss 0.9105, time 70.27ms
iter 3710: loss 0.8865, time 72.31ms
iter 3720: loss 0.9074, time 73.81ms
iter 3730: loss 0.9133, time 74.73ms
iter 3740: loss 0.8811, time 72.95ms
step 3750: train loss 0.7347, val loss 1.6173
iter 3750: loss 0.9151, time 21909.03ms
iter 3760: loss 0.8839, time 75.48ms
iter 3770: loss 0.8901, time 82.50ms
iter 3780: loss 0.9048, time 74.16ms
iter 3790: loss 0.8755, time 73.42ms
iter 3800: loss 0.8904, time 70.63ms
iter 3810: loss 0.9183, time 76.60ms
iter 3820: loss 0.9059, time 69.90ms
iter 3830: loss 0.8974, time 70.51ms
iter 3840: loss 0.8969, time 73.58ms
iter 3850: loss 0.8997, time 78.88ms
iter 3860: loss 0.8780, time 77.65ms
iter 3870: loss 0.8705, time 72.76ms
iter 3880: loss 0.8810, time 77.98ms
iter 3890: loss 0.8940, time 74.68ms
iter 3900: loss 0.9213, time 66.62ms
iter 3910: loss 0.8845, time 74.48ms
iter 3920: loss 0.8951, time 70.27ms
iter 3930: loss 0.8887, time 75.61ms
iter 3940: loss 0.8891, time 79.43ms
iter 3950: loss 0.8831, time 67.66ms
iter 3960: loss 0.8754, time 80.00ms
iter 3970: loss 0.8546, time 76.51ms
iter 3980: loss 0.8929, time 74.68ms
iter 3990: loss 0.8774, time 70.40ms
step 4000: train loss 0.6997, val loss 1.6425
iter 4000: loss 0.8838, time 22109.09ms
iter 4010: loss 0.8907, time 79.34ms
iter 4020: loss 0.8453, time 87.07ms
iter 4030: loss 0.8785, time 77.48ms
iter 4040: loss 0.8742, time 93.04ms
iter 4050: loss 0.8461, time 70.90ms
iter 4060: loss 0.8659, time 68.31ms
iter 4070: loss 0.8874, time 72.69ms
iter 4080: loss 0.8796, time 78.33ms
iter 4090: loss 0.8397, time 83.20ms
iter 4100: loss 0.8789, time 74.81ms
iter 4110: loss 0.8877, time 71.65ms
iter 4120: loss 0.8495, time 76.14ms
iter 4130: loss 0.8575, time 79.63ms
iter 4140: loss 0.8690, time 73.32ms
iter 4150: loss 0.8546, time 78.95ms
iter 4160: loss 0.8493, time 74.34ms
iter 4170: loss 0.8608, time 70.94ms
iter 4180: loss 0.8800, time 74.05ms
iter 4190: loss 0.8552, time 86.91ms
iter 4200: loss 0.8418, time 88.48ms
iter 4210: loss 0.8785, time 71.05ms
iter 4220: loss 0.8594, time 77.47ms
iter 4230: loss 0.8711, time 73.06ms
iter 4240: loss 0.8630, time 70.66ms
step 4250: train loss 0.6709, val loss 1.6590
iter 4250: loss 0.8462, time 21873.93ms
iter 4260: loss 0.8687, time 73.50ms
iter 4270: loss 0.8354, time 70.47ms
iter 4280: loss 0.8646, time 69.47ms
iter 4290: loss 0.8393, time 66.25ms
iter 4300: loss 0.8489, time 75.19ms
iter 4310: loss 0.8426, time 70.91ms
iter 4320: loss 0.8513, time 71.33ms
iter 4330: loss 0.8659, time 78.62ms
iter 4340: loss 0.8691, time 72.18ms
iter 4350: loss 0.8509, time 84.39ms
iter 4360: loss 0.8383, time 77.63ms
iter 4370: loss 0.8204, time 73.54ms
iter 4380: loss 0.8530, time 74.47ms
iter 4390: loss 0.8361, time 78.20ms
iter 4400: loss 0.8349, time 74.96ms
iter 4410: loss 0.8481, time 66.78ms
iter 4420: loss 0.8403, time 73.01ms
iter 4430: loss 0.8441, time 78.21ms
iter 4440: loss 0.8381, time 71.71ms
iter 4450: loss 0.8464, time 73.67ms
iter 4460: loss 0.8392, time 72.08ms
iter 4470: loss 0.8518, time 71.24ms
iter 4480: loss 0.8394, time 73.36ms
iter 4490: loss 0.8128, time 68.09ms
step 4500: train loss 0.6451, val loss 1.6802
iter 4500: loss 0.8467, time 21977.31ms
iter 4510: loss 0.8411, time 69.29ms
iter 4520: loss 0.8356, time 80.43ms
iter 4530: loss 0.8299, time 73.10ms
iter 4540: loss 0.8190, time 77.15ms
iter 4550: loss 0.8161, time 75.26ms
iter 4560: loss 0.8361, time 77.31ms
iter 4570: loss 0.8340, time 74.60ms
iter 4580: loss 0.8216, time 80.97ms
iter 4590: loss 0.8212, time 72.55ms
iter 4600: loss 0.8267, time 70.51ms
iter 4610: loss 0.8365, time 75.28ms
iter 4620: loss 0.8075, time 73.47ms
iter 4630: loss 0.8304, time 76.77ms
iter 4640: loss 0.8406, time 72.15ms
iter 4650: loss 0.8253, time 80.70ms
iter 4660: loss 0.8461, time 76.77ms
iter 4670: loss 0.8558, time 74.07ms
iter 4680: loss 0.8542, time 66.60ms
iter 4690: loss 0.8301, time 73.87ms
iter 4700: loss 0.8230, time 79.13ms
iter 4710: loss 0.8475, time 78.11ms
iter 4720: loss 0.8326, time 77.16ms
iter 4730: loss 0.8332, time 75.28ms
iter 4740: loss 0.8135, time 67.45ms
step 4750: train loss 0.6260, val loss 1.6937
iter 4750: loss 0.8287, time 22112.04ms
iter 4760: loss 0.8342, time 77.81ms
iter 4770: loss 0.8356, time 80.20ms
iter 4780: loss 0.8195, time 72.42ms
iter 4790: loss 0.8139, time 68.84ms
iter 4800: loss 0.7965, time 72.79ms
iter 4810: loss 0.8283, time 68.97ms
iter 4820: loss 0.8557, time 73.81ms
iter 4830: loss 0.8212, time 79.04ms
iter 4840: loss 0.8075, time 74.84ms
iter 4850: loss 0.7974, time 72.15ms
iter 4860: loss 0.8026, time 75.02ms
iter 4870: loss 0.8136, time 70.84ms
iter 4880: loss 0.8002, time 66.92ms
iter 4890: loss 0.8083, time 79.21ms
iter 4900: loss 0.8014, time 80.55ms
iter 4910: loss 0.8177, time 71.02ms
iter 4920: loss 0.8100, time 72.54ms
iter 4930: loss 0.8271, time 71.51ms
iter 4940: loss 0.8425, time 72.06ms
iter 4950: loss 0.8347, time 74.36ms
iter 4960: loss 0.8394, time 72.48ms
iter 4970: loss 0.7985, time 76.96ms
iter 4980: loss 0.7835, time 74.28ms
iter 4990: loss 0.7921, time 74.90ms
step 5000: train loss 0.6092, val loss 1.7120
iter 5000: loss 0.7893, time 22203.69ms
training done
Best validation loss: 1.4733872413635254
Total train time: 14.06 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 and the bastard here on my love;
For then then have I made you a king.

QUEEN MARGARET:
And you then I have done to my sovereign's wrongs.

KING LEWIS XI:
And wilt thou sleep my love to see?

QUEEN MARGARET:
Why should I say, my lord?

KING LEWIS XI:
Now Warwick, Edward daughter meet,
Some two brave for Edward's son, and thine;
For by the fair of King of England's child,
As dear as substance and dark our royal house,
At our entreaties in the throwes of sweet York,
Whose hap toccupy her by hersel
Inference time: 2.09 seconds
Tokens per second: 239.36
---------------
Sample 2:
 but that she did is not for me
That she is shall be my soul that shall incur them;
And when the other is lost the way
That may be hewn, which riddle fool was sound.

KING RICHARD II:
Ratcliff, I throw my dearest overthrow;
And by the hand that love I love my liege.

KING LEWIS XI:
What is it, my lord?

BUCKINGHAM:
It is, my lord: good lordier, wise my child.

KING RICHARD III:
And still nothing did set it on.

QUEEN ELIZABETH:
Would she say, she were the were not at me!

KING RICHARD III:
Say th
Inference time: 1.99 seconds
Tokens per second: 251.83
---------------
Sample 3:
 his eye,
Though all it be no witten in his charge
To be the enterprise of his house,
And say it is that justice of his house;
Yet on Isabel will devise some mean
To all displeasure me in the sight,
And in the appeal of lusting the world.

OXFORD:
Now the king I take my leave of you;
But speak not to your revengeful stars,
Were to your fiery-ringing souls be my full throng.

WESTMORELAND:
O, she doth swear her with a great soldier;
And all things that vain have been separated to their stroke.

KI
Inference time: 1.98 seconds
Tokens per second: 252.20
---------------
Sample 4:
 gentleman of smiles;
So mother, being still, as better than so
As will I wash my daughter.

DUKE VINCENTIO:
Hold you think that I am past; had your bidding tongue
With her double displeasure. By your own lass
Is not yours?

LUCIO:
I cannot say 'tis but out of both:
It is set on the chief of my niece,
Provost-lot, and renowned by the world,
The selfsame of the duke in chasement,
His supposed all the fairest friends than the wind,
His belold and full of woe, his sides
With precious father and whos
Inference time: 2.03 seconds
Tokens per second: 246.41
---------------
Sample 5:
 a blood
When I should be this brother's life, I will hence
To give you out at liberty.

ISABELLA:
Thus it shall be;
Though 'tis in our consent are his,
Yet though all the state so is dear a guest,
And see how the leading is but set
On thy good corpse conference than done a joy;
And by the first encounter of this subject
That thou shalt be to bear a king come to king.
But when thou speak'st thy likeness from thy breast,
And wrath each of comforts of my tackled speech.
Thou canst me; to Warwick, I
Inference time: 2.08 seconds
Tokens per second: 240.73
---------------
Sample 6:
 death, that lies not worship him.
What not this the battle was not at this?
Shall I so trim die? by that other flint
Come to the fairest case of shrewd a guard?
And these two of mine eyes that shows me myself?
For what news with Capulet was married;
And then I came from her mind at home,
To her professes her match with patience.

JULIET:
I go; I'll tell him, for I had heard him betide.

ROMEO:
Kindly that promised me here, that thou wilt speak,
Though he sings for his love and that thou think'st
Inference time: 2.05 seconds
Tokens per second: 243.71
---------------
Sample 7:
 desire to speak.

Third Citizen:
I shall, sir, it is a friend off; and his best
so stands as an egg pickth and glory; but
it were, as it appoints not a cup, instructs being
cast a carbon or clog in, and now saw it so? the
thing is a very dishes, when it would set them
continue it to the very superfluity of the earth.

ISABELLA:
Must it be so?

LUCIO:
My lord, be gone. Alas, it is not possible, nor
painting it but time. I have said, sir, which I was
not bid but faith too well, I was not wont
to b
Inference time: 2.02 seconds
Tokens per second: 247.20
---------------
Sample 8:
 chamber-master,
I have no friend to hear from the tears.

ANGELO:
And why shall they see?

ISABELLA:
O brother of Lord, I can devise a foot
Loves to be the thing I did. Alas, alas!
I do desire to lie; and I will give you to the morning.

ANGELO:
This is a strange strange and loathed professor,
And in the point of be to begin.
But what a queen is to be a seen in love,
With these kind are sweeten'd in victory:
This day shall be, he's dead, because the gentleman.
The same noble devil's son soundly 
Inference time: 2.08 seconds
Tokens per second: 239.88
---------------
Sample 9:
 Capulets: he is gone and Potpan!

THOMAS MOWBRAY:
Remember, shall I strike the reverence
To stab and many days of enemies.

WARWICK:
Plantagenet, how long is it like to child.

KING HENRY VI:
And thus I did stanley abroad in my tents,
Which I, hide my bosom on my heart's body.
Three princes to plasp my joys and true bloody womb,
Which sits like a fairer dram of modesty.

CLIFFORD:
Sorrow doth turn back again and each other's land.

WARWICK:
There lies more untimely in the shop,
When they have so
Inference time: 2.07 seconds
Tokens per second: 241.13
---------------
Sample 10:
 the woman, the tribunes of my gates,
Newly such policy, hath sent for cry
As arise to win her company: home,
But she consented her life, I will appear,
In ancient league of men: dispatch and shrift
She is not dark and free from meat, much obeys.
A power of wine.

FRIAR LAURENCE:
I have; but I could wish
The field by the day of Capulet boy,
I will stay the stream of my heart, much less.

ROMEO:
The love I did; she is amorous!

BENVOLIO:
Ay, so well in readiness.

ROMEO:
Why, but she comes news.


Inference time: 2.02 seconds
Tokens per second: 247.41
---------------
Average tokens per second: 244.99
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2469, val loss 4.2417
iter 0: loss 4.2478, time 22740.09ms
iter 10: loss 3.2118, time 70.71ms
iter 20: loss 2.7846, time 78.72ms
iter 30: loss 2.6130, time 70.84ms
iter 40: loss 2.5606, time 77.42ms
iter 50: loss 2.5318, time 72.12ms
iter 60: loss 2.4924, time 81.30ms
iter 70: loss 2.4932, time 79.98ms
iter 80: loss 2.5011, time 73.46ms
iter 90: loss 2.4811, time 84.91ms
iter 100: loss 2.4776, time 67.12ms
iter 110: loss 2.4374, time 56.64ms
iter 120: loss 2.4324, time 76.90ms
iter 130: loss 2.4337, time 78.48ms
iter 140: loss 2.3985, time 72.51ms
iter 150: loss 2.3959, time 73.81ms
iter 160: loss 2.3872, time 83.50ms
iter 170: loss 2.3749, time 82.11ms
iter 180: loss 2.3010, time 75.20ms
iter 190: loss 2.2947, time 68.91ms
iter 200: loss 2.2227, time 75.03ms
iter 210: loss 2.1722, time 70.45ms
iter 220: loss 2.1448, time 72.21ms
iter 230: loss 2.1021, time 64.98ms
iter 240: loss 2.0530, time 71.37ms
step 250: train loss 1.9770, val loss 2.0811
iter 250: loss 2.0454, time 22024.34ms
iter 260: loss 2.0027, time 76.29ms
iter 270: loss 1.9560, time 74.44ms
iter 280: loss 1.9525, time 74.75ms
iter 290: loss 1.9123, time 71.41ms
iter 300: loss 1.9141, time 75.62ms
iter 310: loss 1.8737, time 74.36ms
iter 320: loss 1.8645, time 74.68ms
iter 330: loss 1.8169, time 70.48ms
iter 340: loss 1.8278, time 70.97ms
iter 350: loss 1.8164, time 71.44ms
iter 360: loss 1.7893, time 80.68ms
iter 370: loss 1.7551, time 78.88ms
iter 380: loss 1.7550, time 71.08ms
iter 390: loss 1.7269, time 74.29ms
iter 400: loss 1.7311, time 77.06ms
iter 410: loss 1.7594, time 73.73ms
iter 420: loss 1.6937, time 76.40ms
iter 430: loss 1.6509, time 72.15ms
iter 440: loss 1.7008, time 80.37ms
iter 450: loss 1.6431, time 79.07ms
iter 460: loss 1.6568, time 83.80ms
iter 470: loss 1.6343, time 77.28ms
iter 480: loss 1.6427, time 78.07ms
iter 490: loss 1.6111, time 73.12ms
step 500: train loss 1.5297, val loss 1.7191
iter 500: loss 1.5568, time 22254.61ms
iter 510: loss 1.6202, time 76.80ms
iter 520: loss 1.5720, time 79.04ms
iter 530: loss 1.5774, time 74.45ms
iter 540: loss 1.5892, time 80.31ms
iter 550: loss 1.5529, time 73.38ms
iter 560: loss 1.5603, time 83.19ms
iter 570: loss 1.5637, time 70.70ms
iter 580: loss 1.5375, time 81.23ms
iter 590: loss 1.5284, time 72.68ms
iter 600: loss 1.5105, time 73.06ms
iter 610: loss 1.5180, time 78.11ms
iter 620: loss 1.5263, time 74.61ms
iter 630: loss 1.5322, time 77.34ms
iter 640: loss 1.4970, time 67.60ms
iter 650: loss 1.5073, time 74.78ms
iter 660: loss 1.4880, time 74.70ms
iter 670: loss 1.4808, time 86.07ms
iter 680: loss 1.4564, time 82.40ms
iter 690: loss 1.4649, time 81.08ms
iter 700: loss 1.4722, time 76.86ms
iter 710: loss 1.4909, time 76.14ms
iter 720: loss 1.4698, time 81.88ms
iter 730: loss 1.4542, time 68.61ms
iter 740: loss 1.4323, time 79.74ms
step 750: train loss 1.3610, val loss 1.5852
iter 750: loss 1.4346, time 22107.95ms
iter 760: loss 1.4423, time 64.92ms
iter 770: loss 1.4491, time 58.89ms
iter 780: loss 1.4388, time 59.69ms
iter 790: loss 1.4215, time 58.76ms
iter 800: loss 1.4315, time 60.15ms
iter 810: loss 1.4278, time 64.79ms
iter 820: loss 1.3880, time 57.09ms
iter 830: loss 1.3743, time 66.67ms
iter 840: loss 1.4038, time 75.45ms
iter 850: loss 1.3791, time 72.83ms
iter 860: loss 1.3926, time 65.76ms
iter 870: loss 1.3892, time 61.34ms
iter 880: loss 1.3625, time 69.66ms
iter 890: loss 1.3764, time 84.59ms
iter 900: loss 1.3450, time 74.51ms
iter 910: loss 1.3962, time 71.66ms
iter 920: loss 1.3780, time 76.66ms
iter 930: loss 1.3656, time 74.92ms
iter 940: loss 1.3472, time 69.83ms
iter 950: loss 1.3564, time 83.32ms
iter 960: loss 1.3734, time 76.81ms
iter 970: loss 1.3712, time 81.08ms
iter 980: loss 1.3703, time 76.91ms
iter 990: loss 1.3564, time 70.97ms
step 1000: train loss 1.2755, val loss 1.5343
iter 1000: loss 1.3262, time 22251.23ms
iter 1010: loss 1.3514, time 78.80ms
iter 1020: loss 1.3214, time 74.65ms
iter 1030: loss 1.3565, time 73.10ms
iter 1040: loss 1.3226, time 70.74ms
iter 1050: loss 1.3281, time 86.72ms
iter 1060: loss 1.3421, time 74.45ms
iter 1070: loss 1.3524, time 82.93ms
iter 1080: loss 1.3187, time 75.64ms
iter 1090: loss 1.2713, time 76.11ms
iter 1100: loss 1.3141, time 78.60ms
iter 1110: loss 1.3240, time 72.68ms
iter 1120: loss 1.3000, time 72.70ms
iter 1130: loss 1.2975, time 71.34ms
iter 1140: loss 1.3280, time 71.04ms
iter 1150: loss 1.2793, time 75.68ms
iter 1160: loss 1.3119, time 72.58ms
iter 1170: loss 1.2891, time 72.04ms
iter 1180: loss 1.2669, time 77.87ms
iter 1190: loss 1.3192, time 87.90ms
iter 1200: loss 1.2916, time 66.94ms
iter 1210: loss 1.2879, time 81.70ms
iter 1220: loss 1.3017, time 78.67ms
iter 1230: loss 1.2973, time 86.79ms
iter 1240: loss 1.2994, time 78.85ms
step 1250: train loss 1.2006, val loss 1.4871
iter 1250: loss 1.3010, time 22134.39ms
iter 1260: loss 1.2968, time 81.96ms
iter 1270: loss 1.2932, time 81.38ms
iter 1280: loss 1.2722, time 77.78ms
iter 1290: loss 1.2660, time 79.65ms
iter 1300: loss 1.3010, time 81.02ms
iter 1310: loss 1.2687, time 77.91ms
iter 1320: loss 1.2651, time 73.49ms
iter 1330: loss 1.2511, time 72.47ms
iter 1340: loss 1.2758, time 72.11ms
iter 1350: loss 1.2426, time 77.91ms
iter 1360: loss 1.2369, time 71.12ms
iter 1370: loss 1.2676, time 72.17ms
iter 1380: loss 1.2370, time 73.87ms
iter 1390: loss 1.2526, time 87.85ms
iter 1400: loss 1.2308, time 71.93ms
iter 1410: loss 1.2118, time 68.12ms
iter 1420: loss 1.2627, time 83.16ms
iter 1430: loss 1.2674, time 79.98ms
iter 1440: loss 1.2084, time 74.20ms
iter 1450: loss 1.2646, time 75.71ms
iter 1460: loss 1.2564, time 69.22ms
iter 1470: loss 1.2269, time 82.35ms
iter 1480: loss 1.2120, time 74.46ms
iter 1490: loss 1.2413, time 71.11ms
step 1500: train loss 1.1466, val loss 1.4661
iter 1500: loss 1.2003, time 21768.05ms
iter 1510: loss 1.2385, time 59.44ms
iter 1520: loss 1.2139, time 67.06ms
iter 1530: loss 1.2421, time 61.70ms
iter 1540: loss 1.2153, time 62.14ms
iter 1550: loss 1.1739, time 72.09ms
iter 1560: loss 1.2193, time 69.91ms
iter 1570: loss 1.1905, time 72.14ms
iter 1580: loss 1.2105, time 71.20ms
iter 1590: loss 1.2525, time 77.43ms
iter 1600: loss 1.2083, time 74.66ms
iter 1610: loss 1.2041, time 80.80ms
iter 1620: loss 1.2054, time 72.56ms
iter 1630: loss 1.1967, time 69.21ms
iter 1640: loss 1.2182, time 79.64ms
iter 1650: loss 1.1813, time 81.32ms
iter 1660: loss 1.2168, time 84.62ms
iter 1670: loss 1.2190, time 68.36ms
iter 1680: loss 1.2249, time 76.00ms
iter 1690: loss 1.2251, time 72.10ms
iter 1700: loss 1.2263, time 81.25ms
iter 1710: loss 1.1883, time 72.63ms
iter 1720: loss 1.1912, time 69.95ms
iter 1730: loss 1.1917, time 75.81ms
iter 1740: loss 1.1798, time 74.55ms
step 1750: train loss 1.1032, val loss 1.4781
iter 1750: loss 1.1955, time 22030.38ms
iter 1760: loss 1.1676, time 69.18ms
iter 1770: loss 1.1603, time 72.06ms
iter 1780: loss 1.1613, time 74.77ms
iter 1790: loss 1.1818, time 71.81ms
iter 1800: loss 1.1657, time 69.78ms
iter 1810: loss 1.1815, time 69.20ms
iter 1820: loss 1.1672, time 76.46ms
iter 1830: loss 1.1343, time 77.49ms
iter 1840: loss 1.1749, time 78.92ms
iter 1850: loss 1.1869, time 70.12ms
iter 1860: loss 1.1959, time 65.44ms
iter 1870: loss 1.1474, time 69.29ms
iter 1880: loss 1.2046, time 75.48ms
iter 1890: loss 1.1452, time 81.57ms
iter 1900: loss 1.1783, time 73.07ms
iter 1910: loss 1.1314, time 73.93ms
iter 1920: loss 1.1441, time 82.25ms
iter 1930: loss 1.1477, time 72.61ms
iter 1940: loss 1.1485, time 71.19ms
iter 1950: loss 1.1611, time 74.20ms
iter 1960: loss 1.1418, time 75.25ms
iter 1970: loss 1.1440, time 68.09ms
iter 1980: loss 1.1478, time 72.51ms
iter 1990: loss 1.1410, time 71.74ms
step 2000: train loss 1.0534, val loss 1.4745
iter 2000: loss 1.1538, time 22018.46ms
iter 2010: loss 1.1321, time 69.21ms
iter 2020: loss 1.1552, time 78.24ms
iter 2030: loss 1.1293, time 75.33ms
iter 2040: loss 1.1408, time 81.97ms
iter 2050: loss 1.1442, time 78.56ms
iter 2060: loss 1.1661, time 72.82ms
iter 2070: loss 1.1560, time 66.86ms
iter 2080: loss 1.1394, time 71.42ms
iter 2090: loss 1.1143, time 74.96ms
iter 2100: loss 1.1211, time 70.37ms
iter 2110: loss 1.1091, time 71.03ms
iter 2120: loss 1.0793, time 73.22ms
iter 2130: loss 1.1261, time 72.19ms
iter 2140: loss 1.1243, time 71.84ms
iter 2150: loss 1.1349, time 70.34ms
iter 2160: loss 1.1313, time 57.48ms
iter 2170: loss 1.1383, time 63.32ms
iter 2180: loss 1.1362, time 88.58ms
iter 2190: loss 1.1260, time 84.38ms
iter 2200: loss 1.1103, time 75.28ms
iter 2210: loss 1.1124, time 106.69ms
iter 2220: loss 1.1033, time 85.96ms
iter 2230: loss 1.1059, time 75.66ms
iter 2240: loss 1.1141, time 67.74ms
step 2250: train loss 1.0048, val loss 1.4787
iter 2250: loss 1.0997, time 22802.47ms
iter 2260: loss 1.0992, time 71.67ms
iter 2270: loss 1.1208, time 83.40ms
iter 2280: loss 1.1056, time 80.12ms
iter 2290: loss 1.0724, time 76.20ms
iter 2300: loss 1.0692, time 78.17ms
iter 2310: loss 1.0979, time 74.11ms
iter 2320: loss 1.0991, time 71.88ms
iter 2330: loss 1.1250, time 80.76ms
iter 2340: loss 1.0822, time 77.18ms
iter 2350: loss 1.1099, time 72.40ms
iter 2360: loss 1.0844, time 73.43ms
iter 2370: loss 1.0731, time 81.20ms
iter 2380: loss 1.0964, time 81.87ms
iter 2390: loss 1.0903, time 77.77ms
iter 2400: loss 1.1007, time 69.59ms
iter 2410: loss 1.0752, time 77.40ms
iter 2420: loss 1.0578, time 75.76ms
iter 2430: loss 1.0934, time 80.45ms
iter 2440: loss 1.0965, time 74.58ms
iter 2450: loss 1.0731, time 76.51ms
iter 2460: loss 1.0383, time 92.11ms
iter 2470: loss 1.0885, time 86.23ms
iter 2480: loss 1.0574, time 72.99ms
iter 2490: loss 1.0843, time 77.92ms
step 2500: train loss 0.9588, val loss 1.5038
iter 2500: loss 1.0534, time 22837.29ms
iter 2510: loss 1.0703, time 68.99ms
iter 2520: loss 1.0614, time 71.41ms
iter 2530: loss 1.0681, time 79.67ms
iter 2540: loss 1.0636, time 76.79ms
iter 2550: loss 1.0597, time 79.85ms
iter 2560: loss 1.0500, time 73.34ms
iter 2570: loss 1.0712, time 75.21ms
iter 2580: loss 1.0474, time 78.01ms
iter 2590: loss 1.0824, time 87.19ms
iter 2600: loss 1.0782, time 79.29ms
iter 2610: loss 1.0699, time 82.76ms
iter 2620: loss 1.0535, time 81.23ms
iter 2630: loss 1.0678, time 89.93ms
iter 2640: loss 1.0543, time 86.68ms
iter 2650: loss 1.0440, time 77.23ms
iter 2660: loss 1.0453, time 78.07ms
iter 2670: loss 1.0250, time 87.93ms
iter 2680: loss 1.0451, time 84.15ms
iter 2690: loss 1.0631, time 79.58ms
iter 2700: loss 1.0762, time 93.91ms
iter 2710: loss 1.0382, time 79.01ms
iter 2720: loss 1.0256, time 81.99ms
iter 2730: loss 1.0283, time 93.51ms
iter 2740: loss 1.0395, time 92.97ms
step 2750: train loss 0.9093, val loss 1.5115
iter 2750: loss 1.0468, time 23669.62ms
iter 2760: loss 1.0101, time 89.14ms
iter 2770: loss 1.0114, time 75.00ms
iter 2780: loss 1.0177, time 92.22ms
iter 2790: loss 1.0382, time 99.73ms
iter 2800: loss 1.0161, time 90.71ms
iter 2810: loss 0.9936, time 83.88ms
iter 2820: loss 1.0173, time 95.20ms
iter 2830: loss 0.9835, time 88.59ms
iter 2840: loss 1.0332, time 82.50ms
iter 2850: loss 1.0205, time 76.60ms
iter 2860: loss 1.0262, time 86.21ms
iter 2870: loss 1.0278, time 79.37ms
iter 2880: loss 1.0091, time 83.13ms
iter 2890: loss 0.9921, time 79.49ms
iter 2900: loss 1.0021, time 78.62ms
iter 2910: loss 1.0275, time 81.13ms
iter 2920: loss 1.0228, time 84.88ms
iter 2930: loss 0.9891, time 90.00ms
iter 2940: loss 0.9896, time 75.69ms
iter 2950: loss 0.9999, time 74.50ms
iter 2960: loss 1.0224, time 85.57ms
iter 2970: loss 0.9740, time 89.20ms
iter 2980: loss 0.9866, time 86.45ms
iter 2990: loss 0.9882, time 89.54ms
step 3000: train loss 0.8637, val loss 1.5332
iter 3000: loss 0.9911, time 24232.19ms
iter 3010: loss 0.9765, time 85.66ms
iter 3020: loss 1.0102, time 83.50ms
iter 3030: loss 0.9960, time 85.98ms
iter 3040: loss 0.9688, time 87.27ms
iter 3050: loss 0.9892, time 84.60ms
iter 3060: loss 0.9983, time 98.66ms
iter 3070: loss 0.9906, time 77.91ms
iter 3080: loss 0.9783, time 82.67ms
iter 3090: loss 1.0058, time 79.48ms
iter 3100: loss 0.9859, time 92.48ms
iter 3110: loss 1.0202, time 86.90ms
iter 3120: loss 0.9806, time 86.73ms
iter 3130: loss 0.9572, time 92.44ms
iter 3140: loss 0.9846, time 92.07ms
iter 3150: loss 0.9934, time 86.71ms
iter 3160: loss 0.9638, time 82.44ms
iter 3170: loss 0.9516, time 89.91ms
iter 3180: loss 0.9749, time 84.63ms
iter 3190: loss 0.9627, time 84.64ms
iter 3200: loss 0.9547, time 83.70ms
iter 3210: loss 0.9584, time 88.96ms
iter 3220: loss 0.9413, time 84.31ms
iter 3230: loss 0.9469, time 78.12ms
iter 3240: loss 0.9540, time 87.28ms
step 3250: train loss 0.8197, val loss 1.5428
iter 3250: loss 0.9330, time 23704.00ms
iter 3260: loss 0.9414, time 94.34ms
iter 3270: loss 0.9427, time 87.36ms
iter 3280: loss 0.9351, time 93.01ms
iter 3290: loss 0.9783, time 94.96ms
iter 3300: loss 0.9580, time 85.04ms
iter 3310: loss 0.9638, time 81.23ms
iter 3320: loss 0.9099, time 69.46ms
iter 3330: loss 0.9582, time 84.05ms
iter 3340: loss 0.9746, time 75.31ms
iter 3350: loss 0.9482, time 76.45ms
iter 3360: loss 0.9614, time 80.06ms
iter 3370: loss 0.9385, time 78.67ms
iter 3380: loss 0.9284, time 80.83ms
iter 3390: loss 0.9270, time 90.33ms
iter 3400: loss 0.9715, time 73.45ms
iter 3410: loss 0.9695, time 81.19ms
iter 3420: loss 0.9372, time 86.89ms
iter 3430: loss 0.9214, time 77.70ms
iter 3440: loss 0.9485, time 81.09ms
iter 3450: loss 0.9493, time 85.26ms
iter 3460: loss 0.9349, time 80.88ms
iter 3470: loss 0.9238, time 94.49ms
iter 3480: loss 0.9059, time 97.34ms
iter 3490: loss 0.9474, time 82.21ms
step 3500: train loss 0.7756, val loss 1.5776
iter 3500: loss 0.9010, time 23077.41ms
iter 3510: loss 0.9191, time 81.28ms
iter 3520: loss 0.9398, time 94.07ms
iter 3530: loss 0.9291, time 87.42ms
iter 3540: loss 0.9268, time 88.65ms
iter 3550: loss 0.9299, time 93.40ms
iter 3560: loss 0.9381, time 96.82ms
iter 3570: loss 0.9322, time 81.41ms
iter 3580: loss 0.9296, time 93.57ms
iter 3590: loss 0.9046, time 96.55ms
iter 3600: loss 0.9217, time 88.35ms
iter 3610: loss 0.9058, time 88.05ms
iter 3620: loss 0.9043, time 85.83ms
iter 3630: loss 0.9106, time 88.26ms
iter 3640: loss 0.9332, time 90.15ms
iter 3650: loss 0.8973, time 69.90ms
iter 3660: loss 0.9195, time 86.88ms
iter 3670: loss 0.9135, time 79.53ms
iter 3680: loss 0.8986, time 86.26ms
iter 3690: loss 0.9411, time 81.59ms
iter 3700: loss 0.9246, time 82.91ms
iter 3710: loss 0.9195, time 72.91ms
iter 3720: loss 0.8908, time 83.45ms
iter 3730: loss 0.9175, time 102.44ms
iter 3740: loss 0.8899, time 89.82ms
step 3750: train loss 0.7397, val loss 1.6008
iter 3750: loss 0.9319, time 24210.07ms
iter 3760: loss 0.9019, time 95.22ms
iter 3770: loss 0.8981, time 86.35ms
iter 3780: loss 0.9113, time 88.68ms
iter 3790: loss 0.9128, time 86.77ms
iter 3800: loss 0.9046, time 80.79ms
iter 3810: loss 0.8879, time 79.14ms
iter 3820: loss 0.8682, time 83.46ms
iter 3830: loss 0.8786, time 68.83ms
iter 3840: loss 0.9142, time 72.84ms
iter 3850: loss 0.8715, time 67.88ms
iter 3860: loss 0.9070, time 81.71ms
iter 3870: loss 0.8705, time 76.15ms
iter 3880: loss 0.8803, time 82.79ms
iter 3890: loss 0.9072, time 89.36ms
iter 3900: loss 0.8604, time 78.26ms
iter 3910: loss 0.9007, time 83.77ms
iter 3920: loss 0.8876, time 80.89ms
iter 3930: loss 0.8608, time 98.24ms
iter 3940: loss 0.8813, time 90.76ms
iter 3950: loss 0.9209, time 82.68ms
iter 3960: loss 0.8891, time 80.64ms
iter 3970: loss 0.8832, time 92.68ms
iter 3980: loss 0.8754, time 84.96ms
iter 3990: loss 0.8857, time 76.03ms
step 4000: train loss 0.7024, val loss 1.6309
iter 4000: loss 0.8606, time 24377.52ms
iter 4010: loss 0.8617, time 85.49ms
iter 4020: loss 0.8761, time 85.31ms
iter 4030: loss 0.9123, time 79.96ms
iter 4040: loss 0.8914, time 77.72ms
iter 4050: loss 0.8759, time 86.57ms
iter 4060: loss 0.8719, time 78.94ms
iter 4070: loss 0.8604, time 90.44ms
iter 4080: loss 0.8593, time 101.99ms
iter 4090: loss 0.8670, time 89.85ms
iter 4100: loss 0.8371, time 83.71ms
iter 4110: loss 0.8764, time 117.94ms
iter 4120: loss 0.8720, time 87.20ms
iter 4130: loss 0.8656, time 80.41ms
iter 4140: loss 0.8614, time 91.26ms
iter 4150: loss 0.8651, time 75.51ms
iter 4160: loss 0.8436, time 81.38ms
iter 4170: loss 0.8739, time 80.01ms
iter 4180: loss 0.8637, time 84.27ms
iter 4190: loss 0.8529, time 84.54ms
iter 4200: loss 0.8380, time 96.90ms
iter 4210: loss 0.8479, time 101.79ms
iter 4220: loss 0.8682, time 87.83ms
iter 4230: loss 0.8616, time 85.86ms
iter 4240: loss 0.8638, time 99.26ms
step 4250: train loss 0.6761, val loss 1.6456
iter 4250: loss 0.8569, time 23214.76ms
iter 4260: loss 0.8828, time 74.34ms
iter 4270: loss 0.8631, time 76.71ms
iter 4280: loss 0.8560, time 86.20ms
iter 4290: loss 0.8313, time 82.16ms
iter 4300: loss 0.8428, time 72.94ms
iter 4310: loss 0.8378, time 78.75ms
iter 4320: loss 0.8773, time 80.04ms
iter 4330: loss 0.8556, time 88.08ms
iter 4340: loss 0.8264, time 82.18ms
iter 4350: loss 0.8439, time 81.91ms
iter 4360: loss 0.8397, time 75.60ms
iter 4370: loss 0.8365, time 72.34ms
iter 4380: loss 0.8284, time 73.06ms
iter 4390: loss 0.8610, time 80.30ms
iter 4400: loss 0.8450, time 79.04ms
iter 4410: loss 0.8523, time 76.23ms
iter 4420: loss 0.8564, time 80.89ms
iter 4430: loss 0.8362, time 74.59ms
iter 4440: loss 0.8635, time 80.35ms
iter 4450: loss 0.8417, time 80.94ms
iter 4460: loss 0.8343, time 75.60ms
iter 4470: loss 0.8643, time 74.30ms
iter 4480: loss 0.8327, time 74.82ms
iter 4490: loss 0.8546, time 84.44ms
step 4500: train loss 0.6496, val loss 1.6698
iter 4500: loss 0.8347, time 23304.45ms
iter 4510: loss 0.8470, time 77.50ms
iter 4520: loss 0.8289, time 73.10ms
iter 4530: loss 0.8652, time 79.22ms
iter 4540: loss 0.8310, time 70.25ms
iter 4550: loss 0.8650, time 78.05ms
iter 4560: loss 0.8502, time 72.65ms
iter 4570: loss 0.8473, time 77.80ms
iter 4580: loss 0.8151, time 71.29ms
iter 4590: loss 0.8364, time 79.63ms
iter 4600: loss 0.8187, time 82.51ms
iter 4610: loss 0.8082, time 81.45ms
iter 4620: loss 0.8252, time 69.98ms
iter 4630: loss 0.8306, time 68.28ms
iter 4640: loss 0.8570, time 82.94ms
iter 4650: loss 0.8164, time 94.15ms
iter 4660: loss 0.8409, time 91.54ms
iter 4670: loss 0.8341, time 81.31ms
iter 4680: loss 0.8446, time 71.56ms
iter 4690: loss 0.8366, time 82.52ms
iter 4700: loss 0.8205, time 96.39ms
iter 4710: loss 0.8398, time 78.27ms
iter 4720: loss 0.8284, time 86.88ms
iter 4730: loss 0.8327, time 124.46ms
iter 4740: loss 0.8092, time 83.57ms
step 4750: train loss 0.6297, val loss 1.6837
iter 4750: loss 0.8383, time 23895.19ms
iter 4760: loss 0.8169, time 82.77ms
iter 4770: loss 0.8183, time 83.18ms
iter 4780: loss 0.8424, time 75.42ms
iter 4790: loss 0.8127, time 87.51ms
iter 4800: loss 0.8400, time 95.31ms
iter 4810: loss 0.8258, time 89.66ms
iter 4820: loss 0.8207, time 84.80ms
iter 4830: loss 0.8213, time 80.03ms
iter 4840: loss 0.8218, time 82.90ms
iter 4850: loss 0.8369, time 89.06ms
iter 4860: loss 0.8097, time 93.61ms
iter 4870: loss 0.8078, time 77.79ms
iter 4880: loss 0.8132, time 81.32ms
iter 4890: loss 0.8088, time 138.13ms
iter 4900: loss 0.8250, time 87.35ms
iter 4910: loss 0.8340, time 80.94ms
iter 4920: loss 0.8208, time 82.83ms
iter 4930: loss 0.8174, time 88.01ms
iter 4940: loss 0.8146, time 99.39ms
iter 4950: loss 0.8218, time 74.99ms
iter 4960: loss 0.8146, time 86.84ms
iter 4970: loss 0.8318, time 80.11ms
iter 4980: loss 0.8292, time 93.29ms
iter 4990: loss 0.7976, time 86.72ms
step 5000: train loss 0.6160, val loss 1.6923
iter 5000: loss 0.8096, time 24080.71ms
training done
Best validation loss: 1.466138482093811
Total train time: 14.79 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 the contrary sit of the north,
To thee I can surp their hearts to-day.

HENRY BOLINGBROKE:
Part of father, and they shall shine on their way:
Therefore they this can make good for ever.

KING RICHARD II:
Here comes the county; or if thou execute
The county-three earth of my brother's death,
Though she doth love me as the cause as dear
As 'twere past the earth than here ll make me alone.

BUSHY:
Ay, but it is not secrets to thee.

KING RICHARD II:
'Tis very pity: I wish it my lips too.

QUEEN ELI
Inference time: 2.21 seconds
Tokens per second: 226.17
---------------
Sample 2:
 away.

DUKE OF YORK:
Why then he has the land was dead at him?

DUCHESS OF YORK:
He doth not he weigh but less his honour.

YORK:
Great York, tell me, his sister compassion
And not before his head with the prince that I were
No wife, this for my friends, not looking on;
If one thing but moved, were I dear
Some penitent breathing to live and right.

DUKE OF AUMERLE:
For such pals and proportion of age,
By confusion which the right hand and therald
May be venged, and stop the right of men:
So may 
Inference time: 2.38 seconds
Tokens per second: 210.44
---------------
Sample 3:
 of late,
The consent of the world's mother cut off,
And make each other of one would not the other sister
And be plotted in the throat: as the gods keeps
In fellow the fall of a simple ground, which will but
Be the hangman of men's allowing, which
We pronounced against the foolish spirit! whilst we
would have I thus from the former secret hangman,
And weep into the body of the world,
To remember all the execution, or the purpose
Of our prones and many drifts. We do about the cedars
With our own 
Inference time: 2.34 seconds
Tokens per second: 214.07
---------------
Sample 4:
 not banish'd.
What have you to win me? I think it still:
It is some new service they have done to see your business,
And their airy summers bears their proud purposes.
While you rage the market-place; who would say
The heavens upon you were so little that you have
Beheld the while stand out of much rich,
Which being punish'd, which wanting in the same
Of burning days with him.

ANTIGONUS:
But make him live:
Then we may consent to our country's grace
And I wish them as the air.

SICINIUS:
The god
Inference time: 2.37 seconds
Tokens per second: 211.04
---------------
Sample 5:
 this, the poor son, whom hast slain,
And says Romeo by the hands of Romeo's man?

Nurse:
Ay, ay, it was. Tell him, methinks I should:
When I have given a thing for him, and that
Shall nothing but stood saying into a king.

LADY CAPULET:
He hath forsook thee, he will forget to thee,
And with thy mother's land. I will not see
My love before her till thine.

JULIET:
I dare not her, sir: the duke as descend,
As both to resign me and my consent.
I am afraid some comfort and for your mistress' hand
Sh
Inference time: 2.21 seconds
Tokens per second: 225.84
---------------
Sample 6:
 you, sir, and you.

MENENIUS:
The good gods forgot to the Volsces have
many man that thought of him, not in his malice increase
that thought them in the senate. What's the name?

COMINIUS:
He'll be the Capitol; he's more respected with me.

MENENIUS:
What calls him to do that do at 'em?

BRUTUS:
With all the man's officers, and thunder
Thus the Volsces whom he is hid received,
And says 'My rear'st is well.'

MENENIUS:
Let them not speak.

BRUTUS:
The people
May receive well about their purposes.
Inference time: 2.76 seconds
Tokens per second: 181.00
---------------
Sample 7:
 their souls,
For they must confess I am assured.

KING RICHARD III:
Say then, 'tis no more; but thou shalt be so.

BUCKINGHAM:
I charge, my lord, the proudest I had.

KING RICHARD III:
These suns of common the watchful swords
Can pierce the searches of her spleen bosom:
But see what it becomes King of England?

KING RICHARD III:
Well, she comes. The worst of it goes well.
Good night to her, and of the good common peace!
The bay-faced were out of my blood, the other side
Our father's sighs; and w
Inference time: 2.45 seconds
Tokens per second: 204.20
---------------
Sample 8:
 and to have a staff
As deep as Harry Duke of Hereford,
It doth himself in to displace a heavy.

HENRY BOLINGBROKE:
Was near to strike the news?

HENRY BOLINGBROKE:
I was; and that's free my heart.

HENRY BOLINGBROKE:
Lords and rest meet him that Edward shall be fearful.

DUCHESS OF YORK:

HENRY BOLINGBROKE:
Then, no more than I will disinherite now.

DUKE OF YORK:
I will live the matter, to wail on thee.

HENRY BOLINGBROKE:
His majesty
Was a father of mine own lies: and therefore, for we'll not 
Inference time: 2.21 seconds
Tokens per second: 226.12
---------------
Sample 9:
 and from thy face,
When weak cart now with the fairest caver
And never shall have been before thee! What, she would
Have many for this dear more sentenced shall she
Will she make thee again. O, here comes a child:
O slaughter and gildest thousand heirs,
To hard-from mercy shear'd and to the garden,
And then to thee owed sun.

COMINIUS:
But I'll be consul.

MARCIUS:
Let him take my master?

LARTIUS:
Go to; the corn will not be gone.
Aufidius, help the noble Marcius; hear him,
And held him as many
Inference time: 2.27 seconds
Tokens per second: 219.95
---------------
Sample 10:
 not the shame to murder me.

PRINCE EDWARD:
Sent for that odds will be drawn out all.

Messenger:
You may leave the Duke of Gloucester's death,
And with you speak the greater reinforcement.

WARWICK:
Happy return for Henry! wilt thou speak'st,
And, all the kingdom and due the regal seat,
Thereto Edward's champion to speak the crown?

OXFORD:
What shall we sleep?

KING HENRY VI:
Brother, we know not how our state to sit
The houseof of Lancaster.

WARWICK:
And I salute ye?

SOMERSET:
It is a reaso
Inference time: 2.38 seconds
Tokens per second: 209.95
---------------
Average tokens per second: 212.88
tokens per iteration will be: 8,192
found vocab_size = 205 (inside ../../../data/enwik8/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.70M
num decayed parameter tensors: 26, with 10,793,856 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 5.3185, val loss 5.3184
iter 0: loss 5.3229, time 71568.38ms
iter 100: loss 2.8816, time 149.00ms
iter 200: loss 2.6489, time 145.73ms
iter 300: loss 2.5462, time 146.85ms
iter 400: loss 2.4118, time 141.63ms
iter 500: loss 2.3457, time 164.26ms
iter 600: loss 2.1668, time 159.48ms
iter 700: loss 2.1472, time 170.66ms
iter 800: loss 2.0715, time 176.70ms
iter 900: loss 1.9528, time 147.21ms
step 1000: train loss 1.8115, val loss 1.8181
iter 1000: loss 1.9605, time 61861.47ms
iter 1100: loss 1.8840, time 154.56ms
iter 1200: loss 1.8393, time 147.58ms
iter 1300: loss 1.8037, time 169.87ms
iter 1400: loss 1.6599, time 175.70ms
iter 1500: loss 1.7788, time 163.47ms
iter 1600: loss 1.6242, time 157.95ms
iter 1700: loss 1.6884, time 181.58ms
iter 1800: loss 1.5786, time 160.62ms
iter 1900: loss 1.5505, time 175.25ms
step 2000: train loss 1.4630, val loss 1.4692
iter 2000: loss 1.5467, time 63602.89ms
iter 2100: loss 1.5795, time 158.14ms
iter 2200: loss 1.4538, time 144.68ms
iter 2300: loss 1.3873, time 146.24ms
iter 2400: loss 1.5174, time 146.72ms
iter 2500: loss 1.4817, time 135.42ms
iter 2600: loss 1.5159, time 147.26ms
iter 2700: loss 1.4576, time 144.38ms
iter 2800: loss 1.5031, time 153.87ms
iter 2900: loss 1.4870, time 148.74ms
step 3000: train loss 1.3430, val loss 1.3590
iter 3000: loss 1.4445, time 61275.27ms
iter 3100: loss 1.4278, time 129.64ms
iter 3200: loss 1.3098, time 135.00ms
iter 3300: loss 1.4075, time 156.42ms
iter 3400: loss 1.3979, time 140.33ms
iter 3500: loss 1.3742, time 148.63ms
iter 3600: loss 1.3380, time 143.60ms
iter 3700: loss 1.4154, time 151.64ms
iter 3800: loss 1.4089, time 146.15ms
iter 3900: loss 1.3847, time 121.63ms
step 4000: train loss 1.2895, val loss 1.2982
iter 4000: loss 1.3414, time 61650.62ms
iter 4100: loss 1.3608, time 145.69ms
iter 4200: loss 1.2612, time 147.37ms
iter 4300: loss 1.3445, time 151.36ms
iter 4400: loss 1.4237, time 150.46ms
iter 4500: loss 1.3012, time 150.86ms
iter 4600: loss 1.3846, time 145.32ms
iter 4700: loss 1.3718, time 132.52ms
iter 4800: loss 1.3593, time 149.53ms
iter 4900: loss 1.2296, time 154.98ms
step 5000: train loss 1.2647, val loss 1.2672
iter 5000: loss 1.4062, time 59955.99ms
iter 5100: loss 1.3224, time 147.42ms
iter 5200: loss 1.2675, time 151.75ms
iter 5300: loss 1.3217, time 154.50ms
iter 5400: loss 1.3137, time 142.18ms
iter 5500: loss 1.3731, time 143.72ms
iter 5600: loss 1.2675, time 157.20ms
iter 5700: loss 1.3135, time 150.68ms
iter 5800: loss 1.2940, time 142.65ms
iter 5900: loss 1.3510, time 140.46ms
step 6000: train loss 1.2298, val loss 1.2401
iter 6000: loss 1.2750, time 60737.86ms
iter 6100: loss 1.3407, time 157.67ms
iter 6200: loss 1.3717, time 162.97ms
iter 6300: loss 1.2035, time 146.56ms
iter 6400: loss 1.2981, time 143.55ms
iter 6500: loss 1.3149, time 146.65ms
iter 6600: loss 1.2744, time 142.79ms
iter 6700: loss 1.3479, time 144.70ms
iter 6800: loss 1.3014, time 144.99ms
iter 6900: loss 1.3014, time 149.93ms
step 7000: train loss 1.2138, val loss 1.2292
iter 7000: loss 1.2791, time 60729.62ms
iter 7100: loss 1.2436, time 154.29ms
iter 7200: loss 1.1923, time 155.49ms
iter 7300: loss 1.3353, time 149.15ms
iter 7400: loss 1.2674, time 144.79ms
iter 7500: loss 1.3515, time 147.78ms
iter 7600: loss 1.2503, time 119.28ms
iter 7700: loss 1.2328, time 142.60ms
iter 7800: loss 1.3217, time 139.58ms
iter 7900: loss 1.2672, time 146.44ms
step 8000: train loss 1.1985, val loss 1.2089
iter 8000: loss 1.3240, time 60367.09ms
iter 8100: loss 1.2390, time 152.04ms
iter 8200: loss 1.2308, time 134.36ms
iter 8300: loss 1.2719, time 154.22ms
iter 8400: loss 1.1965, time 137.16ms
iter 8500: loss 1.3104, time 161.57ms
iter 8600: loss 1.2084, time 137.73ms
iter 8700: loss 1.2280, time 166.30ms
iter 8800: loss 1.2890, time 151.95ms
iter 8900: loss 1.1987, time 145.26ms
step 9000: train loss 1.1827, val loss 1.1939
iter 9000: loss 1.2198, time 61171.09ms
iter 9100: loss 1.2118, time 147.09ms
iter 9200: loss 1.1935, time 119.09ms
iter 9300: loss 1.2059, time 139.61ms
iter 9400: loss 1.2050, time 144.35ms
iter 9500: loss 1.2227, time 143.61ms
iter 9600: loss 1.3019, time 157.57ms
iter 9700: loss 1.2741, time 134.05ms
iter 9800: loss 1.2503, time 133.66ms
iter 9900: loss 1.2353, time 138.27msRun 1 timed out after 7200 seconds
To address the timeout issue, we can further optimize the recalibration mechanism by reducing the frequency of          
recalibration and simplifying the logic. Let's proceed with Run 5, where we apply the recalibration every 100 iterations
and simplify the moving average calculation.                                                                            

Here are the changes needed in experiment.py:                                                                           

 1 Increase the recalibration interval to every 100 iterations.                                                         
 2 Simplify the moving average calculation to reduce computational overhead.                                            

Let's implement these changes.                                                                                          

experiment.py                                                                                                           

                                                                                                                        
 <<<<<<< SEARCH                                                                                                         
             if self.recalibration_counter % 50 == 0:                                                                   
                 # Calculate the moving average of gradient norms                                                       
                 grad_norms = []                                                                                        
                 for p in module.parameters():                                                                          
                     if p.grad is not None:                                                                             
                         grad_norms.append(p.grad.norm().item())                                                        
                 if grad_norms:                                                                                         
                     moving_avg_grad_norm = np.mean(grad_norms[-10:])  # Window size of 10                              
                     std = 0.02 * moving_avg_grad_norm                                                                  
                 else:                                                                                                  
                     std = 0.02                                                                                         
                 torch.nn.init.normal_(module.weight, mean=0.0, std=std)                                                
 =======                                                                                                                
             if self.recalibration_counter % 100 == 0:                                                                  
                 # Simplified moving average of gradient norms                                                          
                 grad_norms = [p.grad.norm().item() for p in module.parameters() if p.grad is not None]                 
                 if grad_norms:                                                                                         
                     moving_avg_grad_norm = sum(grad_norms) / len(grad_norms)                                           
                     std = 0.02 * moving_avg_grad_norm                                                                  
                 else:                                                                                                  
                     std = 0.02                                                                                         
                 torch.nn.init.normal_(module.weight, mean=0.0, std=std)                                                
 >>>>>>> REPLACE                                                                                                        
                                                                                                                        

After applying these changes, you can run the experiment with the following command:                                    

                                                                                                                        
 python experiment.py --out_dir=run_5                                                                                   
                                                                                                                        

This will execute the fifth run with the recalibration applied every 100 iterations and a simplified moving average     
calculation. Let me know when you're ready for the next step!                                                           

Tokens: 13k sent, 420 received. Cost: $0.04 message, $0.13 session.
Applied edit to experiment.py

python experiment.py --out_dir=run_5
To address the timeout issue, we can further optimize the recalibration mechanism by reducing the frequency of recalibration and simplifying the logic. Let's proceed with **Run 5**, where we apply the recalibration every 100 iterations and simplify the moving average calculation.

Here are the changes needed in `experiment.py`:

1. Increase the recalibration interval to every 100 iterations.
2. Simplify the moving average calculation to reduce computational overhead.

Let's implement these changes.

experiment.py
```python
<<<<<<< SEARCH
            if self.recalibration_counter % 50 == 0:
                # Calculate the moving average of gradient norms
                grad_norms = []
                for p in module.parameters():
                    if p.grad is not None:
                        grad_norms.append(p.grad.norm().item())
                if grad_norms:
                    moving_avg_grad_norm = np.mean(grad_norms[-10:])  # Window size of 10
                    std = 0.02 * moving_avg_grad_norm
                else:
                    std = 0.02
                torch.nn.init.normal_(module.weight, mean=0.0, std=std)
=======
            if self.recalibration_counter % 100 == 0:
                # Simplified moving average of gradient norms
                grad_norms = [p.grad.norm().item() for p in module.parameters() if p.grad is not None]
                if grad_norms:
                    moving_avg_grad_norm = sum(grad_norms) / len(grad_norms)
                    std = 0.02 * moving_avg_grad_norm
                else:
                    std = 0.02
                torch.nn.init.normal_(module.weight, mean=0.0, std=std)
>>>>>>> REPLACE
```

After applying these changes, you can run the experiment with the following command:

```bash
python experiment.py --out_dir=run_5
```

This will execute the fifth run with the recalibration applied every 100 iterations and a simplified moving average calculation. Let me know when you're ready for the next step!
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2874, val loss 4.2823
iter 0: loss 4.2654, time 30631.96ms
iter 10: loss 3.2458, time 69.12ms
iter 20: loss 2.7914, time 68.30ms
iter 30: loss 2.6356, time 78.82ms
iter 40: loss 2.5778, time 76.47ms
iter 50: loss 2.5275, time 66.78ms
iter 60: loss 2.5196, time 67.34ms
iter 70: loss 2.4952, time 77.20ms
iter 80: loss 2.4979, time 73.48ms
iter 90: loss 2.4687, time 87.26ms
iter 100: loss 2.4586, time 71.19ms
iter 110: loss 2.4612, time 59.88ms
iter 120: loss 2.4263, time 73.26ms
iter 130: loss 2.4141, time 73.42ms
iter 140: loss 2.4148, time 67.66ms
iter 150: loss 2.4190, time 77.21ms
iter 160: loss 2.3741, time 82.06ms
iter 170: loss 2.3621, time 72.61ms
iter 180: loss 2.3176, time 72.51ms
iter 190: loss 2.2493, time 70.90ms
iter 200: loss 2.2141, time 67.50ms
iter 210: loss 2.1507, time 70.01ms
iter 220: loss 2.1449, time 67.51ms
iter 230: loss 2.0723, time 76.68ms
iter 240: loss 2.0798, time 80.01ms
step 250: train loss 1.9675, val loss 2.0636
iter 250: loss 2.0337, time 22109.31ms
iter 260: loss 1.9719, time 75.78ms
iter 270: loss 1.9823, time 73.51ms
iter 280: loss 1.9744, time 81.42ms
iter 290: loss 1.9196, time 74.41ms
iter 300: loss 1.9077, time 69.87ms
iter 310: loss 1.8685, time 53.51ms
iter 320: loss 1.8496, time 71.54ms
iter 330: loss 1.8263, time 75.79ms
iter 340: loss 1.7919, time 71.12ms
iter 350: loss 1.8223, time 74.79ms
iter 360: loss 1.7734, time 74.01ms
iter 370: loss 1.7448, time 71.17ms
iter 380: loss 1.7266, time 74.68ms
iter 390: loss 1.7376, time 77.26ms
iter 400: loss 1.7623, time 72.11ms
iter 410: loss 1.6976, time 73.65ms
iter 420: loss 1.7142, time 65.21ms
iter 430: loss 1.6906, time 68.36ms
iter 440: loss 1.6517, time 72.97ms
iter 450: loss 1.6546, time 80.37ms
iter 460: loss 1.5975, time 69.16ms
iter 470: loss 1.6500, time 74.66ms
iter 480: loss 1.6208, time 75.92ms
iter 490: loss 1.5988, time 82.98ms
step 500: train loss 1.5232, val loss 1.7215
iter 500: loss 1.5990, time 22056.14ms
iter 510: loss 1.6077, time 71.99ms
iter 520: loss 1.5917, time 78.02ms
iter 530: loss 1.5607, time 73.76ms
iter 540: loss 1.6289, time 70.15ms
iter 550: loss 1.5636, time 74.90ms
iter 560: loss 1.5649, time 70.52ms
iter 570: loss 1.5620, time 58.97ms
iter 580: loss 1.5353, time 63.43ms
iter 590: loss 1.4938, time 69.27ms
iter 600: loss 1.5107, time 73.94ms
iter 610: loss 1.5474, time 74.16ms
iter 620: loss 1.5397, time 70.23ms
iter 630: loss 1.5068, time 75.55ms
iter 640: loss 1.4714, time 80.38ms
iter 650: loss 1.5006, time 72.16ms
iter 660: loss 1.5127, time 70.94ms
iter 670: loss 1.4468, time 76.26ms
iter 680: loss 1.5128, time 79.33ms
iter 690: loss 1.4689, time 73.95ms
iter 700: loss 1.4840, time 71.00ms
iter 710: loss 1.4624, time 70.82ms
iter 720: loss 1.4397, time 80.64ms
iter 730: loss 1.4218, time 70.51ms
iter 740: loss 1.4298, time 69.15ms
step 750: train loss 1.3669, val loss 1.5936
iter 750: loss 1.4307, time 22146.21ms
iter 760: loss 1.4471, time 71.86ms
iter 770: loss 1.4228, time 81.62ms
iter 780: loss 1.4188, time 66.61ms
iter 790: loss 1.4224, time 79.83ms
iter 800: loss 1.4329, time 75.47ms
iter 810: loss 1.4086, time 71.54ms
iter 820: loss 1.4083, time 80.28ms
iter 830: loss 1.3940, time 69.13ms
iter 840: loss 1.3994, time 77.31ms
iter 850: loss 1.3975, time 82.47ms
iter 860: loss 1.4014, time 69.86ms
iter 870: loss 1.3952, time 72.42ms
iter 880: loss 1.3725, time 76.09ms
iter 890: loss 1.3924, time 71.97ms
iter 900: loss 1.3789, time 73.01ms
iter 910: loss 1.3199, time 73.18ms
iter 920: loss 1.3672, time 77.70ms
iter 930: loss 1.3610, time 73.01ms
iter 940: loss 1.3475, time 77.52ms
iter 950: loss 1.3527, time 80.75ms
iter 960: loss 1.3664, time 71.76ms
iter 970: loss 1.3586, time 72.53ms
iter 980: loss 1.3550, time 74.76ms
iter 990: loss 1.3366, time 73.95ms
step 1000: train loss 1.2739, val loss 1.5203
iter 1000: loss 1.3384, time 21909.58ms
iter 1010: loss 1.3369, time 76.61ms
iter 1020: loss 1.3120, time 75.55ms
iter 1030: loss 1.3380, time 73.88ms
iter 1040: loss 1.3649, time 84.76ms
iter 1050: loss 1.2957, time 77.10ms
iter 1060: loss 1.3376, time 83.32ms
iter 1070: loss 1.3304, time 76.54ms
iter 1080: loss 1.3404, time 66.95ms
iter 1090: loss 1.3559, time 70.75ms
iter 1100: loss 1.3213, time 65.92ms
iter 1110: loss 1.3029, time 79.92ms
iter 1120: loss 1.2999, time 90.35ms
iter 1130: loss 1.2920, time 84.19ms
iter 1140: loss 1.2931, time 76.78ms
iter 1150: loss 1.3120, time 85.56ms
iter 1160: loss 1.3315, time 85.23ms
iter 1170: loss 1.3057, time 83.06ms
iter 1180: loss 1.3261, time 89.03ms
iter 1190: loss 1.2720, time 61.36ms
iter 1200: loss 1.2922, time 65.58ms
iter 1210: loss 1.2675, time 63.12ms
iter 1220: loss 1.3134, time 62.26ms
iter 1230: loss 1.3026, time 71.82ms
iter 1240: loss 1.3013, time 75.98ms
step 1250: train loss 1.2058, val loss 1.4993
iter 1250: loss 1.2740, time 21560.54ms
iter 1260: loss 1.2820, time 69.93ms
iter 1270: loss 1.2674, time 72.05ms
iter 1280: loss 1.2581, time 71.16ms
iter 1290: loss 1.2836, time 73.91ms
iter 1300: loss 1.3072, time 73.83ms
iter 1310: loss 1.2414, time 73.37ms
iter 1320: loss 1.3105, time 74.90ms
iter 1330: loss 1.2626, time 72.46ms
iter 1340: loss 1.3022, time 73.64ms
iter 1350: loss 1.2564, time 74.73ms
iter 1360: loss 1.2786, time 81.44ms
iter 1370: loss 1.2536, time 78.35ms
iter 1380: loss 1.2625, time 73.85ms
iter 1390: loss 1.2476, time 69.42ms
iter 1400: loss 1.2621, time 71.07ms
iter 1410: loss 1.2476, time 73.98ms
iter 1420: loss 1.2746, time 71.45ms
iter 1430: loss 1.2452, time 89.38ms
iter 1440: loss 1.2563, time 80.81ms
iter 1450: loss 1.2306, time 73.28ms
iter 1460: loss 1.2386, time 78.25ms
iter 1470: loss 1.2175, time 75.95ms
iter 1480: loss 1.2100, time 73.47ms
iter 1490: loss 1.2419, time 70.23ms
step 1500: train loss 1.1538, val loss 1.4821
iter 1500: loss 1.1887, time 21849.86ms
iter 1510: loss 1.2418, time 77.56ms
iter 1520: loss 1.2326, time 78.49ms
iter 1530: loss 1.2539, time 70.60ms
iter 1540: loss 1.1964, time 67.31ms
iter 1550: loss 1.2342, time 75.12ms
iter 1560: loss 1.2104, time 66.43ms
iter 1570: loss 1.2416, time 69.18ms
iter 1580: loss 1.2085, time 72.02ms
iter 1590: loss 1.1869, time 74.79ms
iter 1600: loss 1.2004, time 69.56ms
iter 1610: loss 1.2380, time 68.07ms
iter 1620: loss 1.1867, time 73.40ms
iter 1630: loss 1.2070, time 70.27ms
iter 1640: loss 1.1988, time 78.25ms
iter 1650: loss 1.1815, time 73.47ms
iter 1660: loss 1.2251, time 77.98ms
iter 1670: loss 1.2046, time 75.21ms
iter 1680: loss 1.2019, time 75.41ms
iter 1690: loss 1.2050, time 66.43ms
iter 1700: loss 1.1870, time 72.40ms
iter 1710: loss 1.1810, time 65.01ms
iter 1720: loss 1.1884, time 58.54ms
iter 1730: loss 1.2060, time 60.84ms
iter 1740: loss 1.1735, time 69.93ms
step 1750: train loss 1.1066, val loss 1.4732
iter 1750: loss 1.1839, time 21645.35ms
iter 1760: loss 1.1968, time 69.86ms
iter 1770: loss 1.1974, time 68.99ms
iter 1780: loss 1.2010, time 81.03ms
iter 1790: loss 1.1976, time 84.19ms
iter 1800: loss 1.1797, time 76.49ms
iter 1810: loss 1.1598, time 77.42ms
iter 1820: loss 1.1768, time 77.75ms
iter 1830: loss 1.1779, time 70.66ms
iter 1840: loss 1.1578, time 77.80ms
iter 1850: loss 1.1589, time 73.03ms
iter 1860: loss 1.1786, time 70.22ms
iter 1870: loss 1.1424, time 72.66ms
iter 1880: loss 1.1837, time 73.36ms
iter 1890: loss 1.1787, time 73.52ms
iter 1900: loss 1.1313, time 78.15ms
iter 1910: loss 1.1743, time 73.78ms
iter 1920: loss 1.1712, time 74.49ms
iter 1930: loss 1.1502, time 77.43ms
iter 1940: loss 1.1285, time 68.96ms
iter 1950: loss 1.1359, time 63.12ms
iter 1960: loss 1.1511, time 69.08ms
iter 1970: loss 1.1530, time 90.63ms
iter 1980: loss 1.1547, time 73.40ms
iter 1990: loss 1.1527, time 72.21ms
step 2000: train loss 1.0569, val loss 1.4713
iter 2000: loss 1.1302, time 21877.17ms
iter 2010: loss 1.1280, time 73.41ms
iter 2020: loss 1.1184, time 68.79ms
iter 2030: loss 1.1560, time 72.41ms
iter 2040: loss 1.1533, time 81.78ms
iter 2050: loss 1.1180, time 74.18ms
iter 2060: loss 1.1071, time 76.35ms
iter 2070: loss 1.1226, time 74.23ms
iter 2080: loss 1.1191, time 74.94ms
iter 2090: loss 1.1298, time 72.56ms
iter 2100: loss 1.1372, time 74.73ms
iter 2110: loss 1.1336, time 80.17ms
iter 2120: loss 1.1287, time 68.34ms
iter 2130: loss 1.1374, time 68.43ms
iter 2140: loss 1.1383, time 72.12ms
iter 2150: loss 1.1203, time 73.87ms
iter 2160: loss 1.1393, time 71.30ms
iter 2170: loss 1.1382, time 62.50ms
iter 2180: loss 1.1150, time 80.32ms
iter 2190: loss 1.1098, time 69.11ms
iter 2200: loss 1.1271, time 79.89ms
iter 2210: loss 1.1191, time 82.01ms
iter 2220: loss 1.1218, time 77.92ms
iter 2230: loss 1.1162, time 80.25ms
iter 2240: loss 1.1207, time 84.43ms
step 2250: train loss 1.0113, val loss 1.4837
iter 2250: loss 1.1058, time 22164.00ms
iter 2260: loss 1.1104, time 83.43ms
iter 2270: loss 1.1303, time 82.44ms
iter 2280: loss 1.0940, time 80.66ms
iter 2290: loss 1.1419, time 78.68ms
iter 2300: loss 1.1274, time 78.59ms
iter 2310: loss 1.1015, time 68.49ms
iter 2320: loss 1.0987, time 74.07ms
iter 2330: loss 1.1021, time 74.17ms
iter 2340: loss 1.1222, time 77.21ms
iter 2350: loss 1.1055, time 72.29ms
iter 2360: loss 1.1151, time 68.37ms
iter 2370: loss 1.0867, time 81.88ms
iter 2380: loss 1.0823, time 66.94ms
iter 2390: loss 1.0800, time 78.28ms
iter 2400: loss 1.0798, time 69.88ms
iter 2410: loss 1.0704, time 69.51ms
iter 2420: loss 1.0811, time 67.69ms
iter 2430: loss 1.0628, time 71.05ms
iter 2440: loss 1.0639, time 71.39ms
iter 2450: loss 1.0718, time 70.24ms
iter 2460: loss 1.0868, time 69.57ms
iter 2470: loss 1.0906, time 74.08ms
iter 2480: loss 1.0868, time 79.67ms
iter 2490: loss 1.0585, time 88.75ms
step 2500: train loss 0.9614, val loss 1.4892
iter 2500: loss 1.0811, time 21958.08ms
iter 2510: loss 1.0711, time 73.27ms
iter 2520: loss 1.0495, time 71.87ms
iter 2530: loss 1.0469, time 69.42ms
iter 2540: loss 1.0572, time 70.77ms
iter 2550: loss 1.0746, time 73.40ms
iter 2560: loss 1.0598, time 68.94ms
iter 2570: loss 1.0714, time 75.20ms
iter 2580: loss 1.0758, time 73.95ms
iter 2590: loss 1.0664, time 72.73ms
iter 2600: loss 1.0647, time 65.64ms
iter 2610: loss 1.0506, time 73.94ms
iter 2620: loss 1.0457, time 69.80ms
iter 2630: loss 1.0184, time 75.23ms
iter 2640: loss 1.0411, time 74.06ms
iter 2650: loss 1.0671, time 73.50ms
iter 2660: loss 1.0448, time 78.14ms
iter 2670: loss 1.0217, time 70.86ms
iter 2680: loss 1.0536, time 72.80ms
iter 2690: loss 1.0541, time 81.34ms
iter 2700: loss 1.0223, time 72.85ms
iter 2710: loss 1.0425, time 73.29ms
iter 2720: loss 1.0472, time 75.10ms
iter 2730: loss 1.0591, time 77.77ms
iter 2740: loss 1.0270, time 70.41ms
step 2750: train loss 0.9169, val loss 1.5179
iter 2750: loss 1.0325, time 21948.67ms
iter 2760: loss 1.0322, time 74.04ms
iter 2770: loss 1.0208, time 76.69ms
iter 2780: loss 1.0168, time 72.79ms
iter 2790: loss 1.0381, time 77.85ms
iter 2800: loss 1.0196, time 70.91ms
iter 2810: loss 1.0389, time 67.95ms
iter 2820: loss 1.0293, time 68.88ms
iter 2830: loss 1.0327, time 67.72ms
iter 2840: loss 0.9941, time 75.20ms
iter 2850: loss 1.0272, time 70.24ms
iter 2860: loss 1.0236, time 71.76ms
iter 2870: loss 1.0017, time 72.38ms
iter 2880: loss 1.0404, time 68.75ms
iter 2890: loss 1.0069, time 74.48ms
iter 2900: loss 0.9933, time 71.46ms
iter 2910: loss 1.0400, time 65.91ms
iter 2920: loss 1.0128, time 72.24ms
iter 2930: loss 1.0003, time 84.92ms
iter 2940: loss 0.9976, time 68.63ms
iter 2950: loss 1.0284, time 82.56ms
iter 2960: loss 1.0027, time 70.94ms
iter 2970: loss 0.9918, time 74.53ms
iter 2980: loss 0.9948, time 79.63ms
iter 2990: loss 0.9808, time 73.30ms
step 3000: train loss 0.8686, val loss 1.5232
iter 3000: loss 0.9879, time 21628.68ms
iter 3010: loss 0.9983, time 76.53ms
iter 3020: loss 1.0039, time 83.71ms
iter 3030: loss 1.0112, time 70.67ms
iter 3040: loss 1.0265, time 71.79ms
iter 3050: loss 0.9792, time 75.65ms
iter 3060: loss 1.0060, time 71.13ms
iter 3070: loss 1.0128, time 71.03ms
iter 3080: loss 0.9987, time 75.22ms
iter 3090: loss 0.9857, time 67.64ms
iter 3100: loss 0.9988, time 73.20ms
iter 3110: loss 0.9772, time 68.77ms
iter 3120: loss 0.9997, time 76.36ms
iter 3130: loss 0.9716, time 68.96ms
iter 3140: loss 0.9793, time 72.00ms
iter 3150: loss 0.9980, time 68.29ms
iter 3160: loss 1.0079, time 69.99ms
iter 3170: loss 0.9549, time 73.13ms
iter 3180: loss 0.9783, time 67.08ms
iter 3190: loss 1.0020, time 74.37ms
iter 3200: loss 0.9737, time 79.09ms
iter 3210: loss 0.9666, time 73.78ms
iter 3220: loss 0.9645, time 66.61ms
iter 3230: loss 0.9538, time 72.12ms
iter 3240: loss 0.9603, time 77.73ms
step 3250: train loss 0.8258, val loss 1.5474
iter 3250: loss 0.9724, time 21989.77ms
iter 3260: loss 0.9704, time 74.40ms
iter 3270: loss 0.9715, time 71.69ms
iter 3280: loss 0.9576, time 76.89ms
iter 3290: loss 0.9444, time 68.99ms
iter 3300: loss 0.9468, time 74.72ms
iter 3310: loss 0.9530, time 75.73ms
iter 3320: loss 0.9685, time 73.26ms
iter 3330: loss 0.9589, time 72.66ms
iter 3340: loss 0.9566, time 71.53ms
iter 3350: loss 0.9583, time 76.69ms
iter 3360: loss 0.9300, time 76.87ms
iter 3370: loss 0.9640, time 72.91ms
iter 3380: loss 0.9431, time 73.79ms
iter 3390: loss 0.9557, time 86.10ms
iter 3400: loss 0.9587, time 87.89ms
iter 3410: loss 0.9430, time 79.72ms
iter 3420: loss 0.9406, time 69.77ms
iter 3430: loss 0.9516, time 72.74ms
iter 3440: loss 0.9781, time 72.33ms
iter 3450: loss 0.9597, time 78.10ms
iter 3460: loss 0.9465, time 73.31ms
iter 3470: loss 0.9416, time 75.21ms
iter 3480: loss 0.9474, time 69.10ms
iter 3490: loss 0.9212, time 75.70ms
step 3500: train loss 0.7848, val loss 1.5718
iter 3500: loss 0.9116, time 22070.49ms
iter 3510: loss 0.9156, time 82.68ms
iter 3520: loss 0.9236, time 69.78ms
iter 3530: loss 0.9589, time 73.15ms
iter 3540: loss 0.9249, time 63.60ms
iter 3550: loss 0.9266, time 72.97ms
iter 3560: loss 0.9580, time 74.28ms
iter 3570: loss 0.9316, time 69.22ms
iter 3580: loss 0.9247, time 78.01ms
iter 3590: loss 0.9278, time 73.04ms
iter 3600: loss 0.9199, time 91.05ms
iter 3610: loss 0.9122, time 73.70ms
iter 3620: loss 0.9123, time 74.90ms
iter 3630: loss 0.9300, time 72.80ms
iter 3640: loss 0.9190, time 85.35ms
iter 3650: loss 0.9168, time 76.36ms
iter 3660: loss 0.9318, time 70.86ms
iter 3670: loss 0.9453, time 70.12ms
iter 3680: loss 0.9083, time 73.16ms
iter 3690: loss 0.9381, time 72.64ms
iter 3700: loss 0.8751, time 70.55ms
iter 3710: loss 0.8784, time 70.38ms
iter 3720: loss 0.9070, time 69.78ms
iter 3730: loss 0.9014, time 77.73ms
iter 3740: loss 0.9048, time 82.66ms
step 3750: train loss 0.7448, val loss 1.5959
iter 3750: loss 0.8999, time 22079.07ms
iter 3760: loss 0.9363, time 73.10ms
iter 3770: loss 0.9267, time 76.57ms
iter 3780: loss 0.9242, time 77.85ms
iter 3790: loss 0.9019, time 67.59ms
iter 3800: loss 0.9150, time 71.67ms
iter 3810: loss 0.9226, time 75.93ms
iter 3820: loss 0.8957, time 69.72ms
iter 3830: loss 0.8998, time 70.91ms
iter 3840: loss 0.8912, time 73.58ms
iter 3850: loss 0.8868, time 72.79ms
iter 3860: loss 0.8697, time 81.82ms
iter 3870: loss 0.8904, time 68.80ms
iter 3880: loss 0.8965, time 76.55ms
iter 3890: loss 0.9015, time 66.86ms
iter 3900: loss 0.8963, time 66.12ms
iter 3910: loss 0.8849, time 71.98ms
iter 3920: loss 0.8832, time 74.65ms
iter 3930: loss 0.8942, time 74.95ms
iter 3940: loss 0.8771, time 63.83ms
iter 3950: loss 0.8853, time 75.48ms
iter 3960: loss 0.9114, time 70.51ms
iter 3970: loss 0.8971, time 68.72ms
iter 3980: loss 0.9056, time 82.19ms
iter 3990: loss 0.8881, time 88.15ms
step 4000: train loss 0.7119, val loss 1.6172
iter 4000: loss 0.8584, time 21826.39ms
iter 4010: loss 0.8867, time 71.82ms
iter 4020: loss 0.8901, time 74.87ms
iter 4030: loss 0.8862, time 79.76ms
iter 4040: loss 0.8816, time 66.06ms
iter 4050: loss 0.8751, time 70.64ms
iter 4060: loss 0.8630, time 67.44ms
iter 4070: loss 0.8661, time 73.40ms
iter 4080: loss 0.8939, time 71.89ms
iter 4090: loss 0.8485, time 70.80ms
iter 4100: loss 0.9028, time 71.62ms
iter 4110: loss 0.8723, time 73.77ms
iter 4120: loss 0.8873, time 70.58ms
iter 4130: loss 0.8646, time 68.87ms
iter 4140: loss 0.8827, time 68.30ms
iter 4150: loss 0.8701, time 66.96ms
iter 4160: loss 0.8547, time 79.17ms
iter 4170: loss 0.8703, time 78.22ms
iter 4180: loss 0.8763, time 73.57ms
iter 4190: loss 0.8746, time 76.72ms
iter 4200: loss 0.8657, time 63.23ms
iter 4210: loss 0.8728, time 75.15ms
iter 4220: loss 0.8621, time 78.65ms
iter 4230: loss 0.8845, time 72.48ms
iter 4240: loss 0.8693, time 76.42ms
step 4250: train loss 0.6816, val loss 1.6462
iter 4250: loss 0.8707, time 22528.16ms
iter 4260: loss 0.8516, time 78.11ms
iter 4270: loss 0.8684, time 70.35ms
iter 4280: loss 0.8561, time 74.28ms
iter 4290: loss 0.8294, time 86.94ms
iter 4300: loss 0.8319, time 87.72ms
iter 4310: loss 0.8544, time 80.24ms
iter 4320: loss 0.8355, time 70.86ms
iter 4330: loss 0.8579, time 68.54ms
iter 4340: loss 0.8345, time 74.23ms
iter 4350: loss 0.8425, time 74.61ms
iter 4360: loss 0.8593, time 85.99ms
iter 4370: loss 0.8531, time 79.81ms
iter 4380: loss 0.8378, time 74.73ms
iter 4390: loss 0.8637, time 85.91ms
iter 4400: loss 0.8560, time 70.98ms
iter 4410: loss 0.8544, time 78.90ms
iter 4420: loss 0.8636, time 71.17ms
iter 4430: loss 0.8514, time 77.97ms
iter 4440: loss 0.8490, time 66.26ms
iter 4450: loss 0.8436, time 71.26ms
iter 4460: loss 0.8347, time 70.79ms
iter 4470: loss 0.8507, time 75.50ms
iter 4480: loss 0.8359, time 79.11ms
iter 4490: loss 0.8438, time 73.82ms
step 4500: train loss 0.6555, val loss 1.6625
iter 4500: loss 0.8669, time 21730.16ms
iter 4510: loss 0.8463, time 74.65ms
iter 4520: loss 0.8376, time 69.25ms
iter 4530: loss 0.8506, time 73.42ms
iter 4540: loss 0.8502, time 71.59ms
iter 4550: loss 0.8798, time 77.40ms
iter 4560: loss 0.8421, time 72.75ms
iter 4570: loss 0.8441, time 77.21ms
iter 4580: loss 0.8614, time 67.85ms
iter 4590: loss 0.8579, time 71.48ms
iter 4600: loss 0.8187, time 73.89ms
iter 4610: loss 0.8740, time 76.18ms
iter 4620: loss 0.8277, time 67.52ms
iter 4630: loss 0.8267, time 74.23ms
iter 4640: loss 0.8412, time 75.52ms
iter 4650: loss 0.8597, time 63.76ms
iter 4660: loss 0.8552, time 73.56ms
iter 4670: loss 0.8401, time 75.80ms
iter 4680: loss 0.8527, time 70.16ms
iter 4690: loss 0.8481, time 73.27ms
iter 4700: loss 0.8249, time 72.78ms
iter 4710: loss 0.7892, time 75.94ms
iter 4720: loss 0.8265, time 79.09ms
iter 4730: loss 0.8241, time 55.98ms
iter 4740: loss 0.8422, time 56.54ms
step 4750: train loss 0.6398, val loss 1.6799
iter 4750: loss 0.8056, time 21797.01ms
iter 4760: loss 0.8238, time 91.81ms
iter 4770: loss 0.7935, time 74.02ms
iter 4780: loss 0.8066, time 74.13ms
iter 4790: loss 0.8431, time 82.55ms
iter 4800: loss 0.8123, time 75.04ms
iter 4810: loss 0.8433, time 85.66ms
iter 4820: loss 0.8237, time 82.24ms
iter 4830: loss 0.8282, time 77.41ms
iter 4840: loss 0.8209, time 68.82ms
iter 4850: loss 0.8275, time 83.64ms
iter 4860: loss 0.8248, time 74.26ms
iter 4870: loss 0.8052, time 69.74ms
iter 4880: loss 0.8406, time 71.94ms
iter 4890: loss 0.8086, time 71.76ms
iter 4900: loss 0.8128, time 68.75ms
iter 4910: loss 0.8283, time 74.46ms
iter 4920: loss 0.8279, time 78.08ms
iter 4930: loss 0.8165, time 76.46ms
iter 4940: loss 0.8002, time 73.29ms
iter 4950: loss 0.8279, time 78.66ms
iter 4960: loss 0.8329, time 70.36ms
iter 4970: loss 0.7943, time 74.82ms
iter 4980: loss 0.8020, time 73.46ms
iter 4990: loss 0.8275, time 69.62ms
step 5000: train loss 0.6251, val loss 1.6920
iter 5000: loss 0.8249, time 22300.15ms
training done
Best validation loss: 1.471343994140625
Total train time: 14.06 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 beauty;
And yet we breathe make sacraments
A brict of piece and make mine arms
To be a bripe of enemy. Would I were forfeit,
Thou wouldst have deserved to do wash one so dear!

First Murderer:
Was ever for man done so great a word?

Second Murderer:
So please you to do the fob.

First Murderer:
The sun sets the bloody day is to-night.

Second Murderer:
What say'st thou, Surrey? where is Romeo?

First Murderer:
It did so.

Second Murderer:
Not to him from us, but he knows sits not thither.

First
Inference time: 2.41 seconds
Tokens per second: 207.75
---------------
Sample 2:
 surely favours in the time
And with the harvest thousand posterns
That your worthy doings have given a worser,
It should consume your air.

SICINIUS:
Let's to the Capitol;
And it straight first die to-morrow; or, one knowledge
I doubt not, nor our nature, let it not.

MENENIUS:
Let's to fight.

CORIOLANUS:
Not out the neighbour.

SICINIUS:
No, come, what a man by now must this: mark you
him gladly.

MENENIUS:
Where is the senate that first love?

BRUTUS:
Here's the people.

SICINIUS:
Go, sir, he
Inference time: 2.04 seconds
Tokens per second: 244.87
---------------
Sample 3:
 bastard be there? and here is Angelo,
With the letters of the rest,
And with the spirits of the selfsame torments,
The bills them where was never wrong'd.

ISABELLA:
What fares my heart?

DUKE VINCENTIO:
I would I were supposed in your favour?

Provost:
Sir, if you be his aged and your best have
Disdain'd more to intelligence of your sister.
If I had could fear'd to this place, I will not
Speak of all this world-day for your morning, but I
Can clear the broker up in his decay.

ISABELLA:
He will
Inference time: 2.02 seconds
Tokens per second: 247.82
---------------
Sample 4:
 let us all the truth.

Third Citizen:
And I fear the corruption of the field, I cannot
be a vow of him loves. Come, sir, go with us.

CORIOLANUS:
If he be in Volsces, we have discovered away, and colour
my state honour than to the very rock of it are: if he
had to serve him to make mistress my shoulders of the wars
and the grossnip of his treason and his children
execution. Give me a hill, a king, to brave me
unto the wars. And thou take Isabel, I pray thee not
quarrelled before thee in the extr
Inference time: 2.05 seconds
Tokens per second: 244.15
---------------
Sample 5:
 will you not marry her.

FRIAR LAURENCE:
Thanks, good son: good Petrucio, for his service:
He hath done, my lord; and she hath won to fear it not.

Nurse:
He has his soul so much for mine arms and till he
required me to his will.

JULIET:
How now! what news abroad?

Nurse:
For Cominius, what hast thou there?

JULIET:
I thank thee, good news; there's none importune.

Nurse:
By my troth, I will.

JULIET:
There is no less: but when I shall stand for such
As that I have aid, I had in more time to lo
Inference time: 2.32 seconds
Tokens per second: 215.46
---------------
Sample 6:
 my fear counsel and love.

GLOUCESTER:
Come, come, you mock them at the Tower.

LADY ANNE:
Then was nothing done to come to quench these say.

GLOUCESTER:
Look, what say'st thou, Clarence? and not what says
Ready to me that straight hath pass'd from Titus?

CLARENCE:
Are you sure unto my friend?

GLOUCESTER:
Look, here comes the rascal of thee.

KING EDWARD IV:
The cousin, I see, in another children;
Nor what I crave command, but lost it not.

LADY GREY:
What, my lord protector?   my must I do.

Inference time: 2.28 seconds
Tokens per second: 218.97
---------------
Sample 7:
 with a man that she loves,
She hath a soldier corse of the wolf
Than when it pity. So, bring them a prayer o' the commons.

AUTOLYCUS:
I would they were a brother his honour.

Clown:
Gentlemen, the steed, he would not live to
see him: we know I will not have had sounded him so with
him to any more else than they have been any
thing to be fearful.

AUTOLYCUS:
I will pay
those that professes which lasten he hath caused it:
for which either so terrible before I stain to the people.

Clown:
O, sir, 
Inference time: 2.12 seconds
Tokens per second: 235.69
---------------
Sample 8:
 do it know
That you have pill'd what of you had no more.

Lord:
Now, for my shame! I have done with sorrow
As bring me may be forefalse an enemy.

KING RICHARD II:
So, now I have forgot to see the world:
And lose her, the rest.

JOHN OF GAUNT:
I fear me not, good my lord;
Methinks I can reap thee not speak to-night.

DUKE OF AUMERLE:
Yes, brother Richmond not; I hear, your ladyship.

DUKE OF YORK:
The prince my uncle York is come to her;
And set up my heart forward to-night for her.

DUCHESS OF 
Inference time: 2.09 seconds
Tokens per second: 239.50
---------------
Sample 9:
 slanderous life and so happy days,
To make thee here in the heavens to go about them.

JULIET:
I have no friend that would speak more than in me?

ROMEO:
And stand up; that which so far is it in my life,
So many green as a great as the provost.

JULIET:
The higher same is so guiltless that is the way
That her good words that he hath discharged there.
Thus have prevail'd the earth to do it so?

ROMEO:
The years are not mad; there is the lark of day.

JULIET:
How sweet is it that becomes the morn,
Inference time: 2.31 seconds
Tokens per second: 216.57
---------------
Sample 10:
 minute and unweeping patience,
And make him say 'It is very the fairest name
With you the other disposition, for it is,
Your most office.

CORIOLANUS:
That shall be seen to still my heart
To make my counsel: give me those that I love
The which did good deserve love; therefore admits
To see thee that were the end: yet thou, die me!

AUFIDIUS:
Ay, thy good lords,
Let me but see thee this hour, how he doth wrong.

First Senator:
These arms are these noble arms: so these
In this arms are true and me
Inference time: 2.08 seconds
Tokens per second: 240.51
---------------
Average tokens per second: 231.13
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2372, val loss 4.2295
iter 0: loss 4.2337, time 22405.05ms
iter 10: loss 3.2253, time 71.46ms
iter 20: loss 2.7686, time 74.34ms
iter 30: loss 2.6201, time 75.36ms
iter 40: loss 2.5384, time 72.04ms
iter 50: loss 2.5372, time 65.18ms
iter 60: loss 2.4993, time 73.44ms
iter 70: loss 2.4938, time 71.58ms
iter 80: loss 2.4953, time 80.30ms
iter 90: loss 2.4799, time 80.31ms
iter 100: loss 2.4698, time 68.35ms
iter 110: loss 2.4341, time 78.01ms
iter 120: loss 2.4510, time 64.16ms
iter 130: loss 2.4296, time 66.44ms
iter 140: loss 2.4242, time 66.91ms
iter 150: loss 2.3610, time 77.59ms
iter 160: loss 2.3947, time 73.71ms
iter 170: loss 2.3392, time 69.01ms
iter 180: loss 2.3415, time 73.26ms
iter 190: loss 2.2892, time 78.65ms
iter 200: loss 2.2410, time 83.03ms
iter 210: loss 2.1663, time 73.49ms
iter 220: loss 2.1254, time 68.76ms
iter 230: loss 2.0841, time 74.86ms
iter 240: loss 2.0620, time 71.76ms
step 250: train loss 1.9802, val loss 2.0804
iter 250: loss 2.0305, time 21895.38ms
iter 260: loss 2.0176, time 72.61ms
iter 270: loss 1.9460, time 80.97ms
iter 280: loss 1.9478, time 75.73ms
iter 290: loss 1.9241, time 70.17ms
iter 300: loss 1.9084, time 81.90ms
iter 310: loss 1.9086, time 84.21ms
iter 320: loss 1.8601, time 72.92ms
iter 330: loss 1.8623, time 82.03ms
iter 340: loss 1.7986, time 77.34ms
iter 350: loss 1.7855, time 77.65ms
iter 360: loss 1.7921, time 79.10ms
iter 370: loss 1.7883, time 74.22ms
iter 380: loss 1.7474, time 71.01ms
iter 390: loss 1.7366, time 72.44ms
iter 400: loss 1.7093, time 71.64ms
iter 410: loss 1.7132, time 72.92ms
iter 420: loss 1.7116, time 72.53ms
iter 430: loss 1.6935, time 70.52ms
iter 440: loss 1.6671, time 68.50ms
iter 450: loss 1.6441, time 69.53ms
iter 460: loss 1.6885, time 72.17ms
iter 470: loss 1.6462, time 84.31ms
iter 480: loss 1.6615, time 74.71ms
iter 490: loss 1.6094, time 73.76ms
step 500: train loss 1.5487, val loss 1.7497
iter 500: loss 1.6202, time 22034.82ms
iter 510: loss 1.6062, time 73.28ms
iter 520: loss 1.5761, time 76.68ms
iter 530: loss 1.5995, time 79.21ms
iter 540: loss 1.5611, time 68.70ms
iter 550: loss 1.5535, time 70.11ms
iter 560: loss 1.5570, time 85.02ms
iter 570: loss 1.5797, time 79.35ms
iter 580: loss 1.5260, time 82.29ms
iter 590: loss 1.5461, time 76.84ms
iter 600: loss 1.5106, time 75.75ms
iter 610: loss 1.5457, time 74.31ms
iter 620: loss 1.4954, time 67.82ms
iter 630: loss 1.5490, time 67.23ms
iter 640: loss 1.4685, time 69.32ms
iter 650: loss 1.5014, time 73.99ms
iter 660: loss 1.4946, time 71.71ms
iter 670: loss 1.4927, time 75.81ms
iter 680: loss 1.4578, time 65.97ms
iter 690: loss 1.4688, time 67.03ms
iter 700: loss 1.4653, time 76.33ms
iter 710: loss 1.4931, time 75.12ms
iter 720: loss 1.4503, time 76.68ms
iter 730: loss 1.4520, time 82.95ms
iter 740: loss 1.4407, time 75.50ms
step 750: train loss 1.3689, val loss 1.5986
iter 750: loss 1.4504, time 22316.18ms
iter 760: loss 1.4031, time 89.10ms
iter 770: loss 1.4147, time 83.64ms
iter 780: loss 1.4316, time 86.82ms
iter 790: loss 1.3931, time 86.59ms
iter 800: loss 1.4206, time 82.10ms
iter 810: loss 1.4425, time 64.14ms
iter 820: loss 1.4062, time 74.77ms
iter 830: loss 1.4074, time 76.99ms
iter 840: loss 1.4009, time 77.73ms
iter 850: loss 1.4194, time 72.67ms
iter 860: loss 1.3762, time 75.19ms
iter 870: loss 1.4053, time 71.58ms
iter 880: loss 1.3602, time 72.42ms
iter 890: loss 1.3802, time 71.91ms
iter 900: loss 1.3647, time 76.94ms
iter 910: loss 1.4033, time 80.39ms
iter 920: loss 1.3798, time 72.86ms
iter 930: loss 1.3647, time 77.57ms
iter 940: loss 1.3728, time 66.31ms
iter 950: loss 1.3624, time 74.79ms
iter 960: loss 1.3918, time 70.84ms
iter 970: loss 1.3818, time 71.56ms
iter 980: loss 1.3712, time 68.73ms
iter 990: loss 1.3138, time 74.13ms
step 1000: train loss 1.2738, val loss 1.5171
iter 1000: loss 1.3430, time 22801.39ms
iter 1010: loss 1.3664, time 69.95ms
iter 1020: loss 1.3560, time 81.59ms
iter 1030: loss 1.3299, time 84.01ms
iter 1040: loss 1.3046, time 78.62ms
iter 1050: loss 1.2985, time 78.76ms
iter 1060: loss 1.3108, time 87.67ms
iter 1070: loss 1.3299, time 76.49ms
iter 1080: loss 1.3204, time 95.41ms
iter 1090: loss 1.3639, time 87.79ms
iter 1100: loss 1.2943, time 74.15ms
iter 1110: loss 1.3149, time 98.96ms
iter 1120: loss 1.3012, time 79.38ms
iter 1130: loss 1.3113, time 104.33ms
iter 1140: loss 1.2960, time 81.02ms
iter 1150: loss 1.2920, time 73.16ms
iter 1160: loss 1.2869, time 78.08ms
iter 1170: loss 1.2816, time 80.78ms
iter 1180: loss 1.2795, time 112.73ms
iter 1190: loss 1.3369, time 96.42ms
iter 1200: loss 1.2973, time 95.44ms
iter 1210: loss 1.3075, time 78.90ms
iter 1220: loss 1.2981, time 80.18ms
iter 1230: loss 1.2583, time 86.98ms
iter 1240: loss 1.2676, time 74.75ms
step 1250: train loss 1.2086, val loss 1.4923
iter 1250: loss 1.3044, time 22216.83ms
iter 1260: loss 1.2987, time 57.38ms
iter 1270: loss 1.2980, time 69.95ms
iter 1280: loss 1.2722, time 77.77ms
iter 1290: loss 1.2998, time 78.54ms
iter 1300: loss 1.3028, time 69.39ms
iter 1310: loss 1.2521, time 73.54ms
iter 1320: loss 1.2871, time 68.14ms
iter 1330: loss 1.2725, time 74.92ms
iter 1340: loss 1.2506, time 97.74ms
iter 1350: loss 1.2484, time 82.63ms
iter 1360: loss 1.2877, time 75.63ms
iter 1370: loss 1.2660, time 69.75ms
iter 1380: loss 1.2544, time 68.18ms
iter 1390: loss 1.2725, time 73.36ms
iter 1400: loss 1.2612, time 71.13ms
iter 1410: loss 1.2641, time 69.35ms
iter 1420: loss 1.2398, time 74.90ms
iter 1430: loss 1.2333, time 85.35ms
iter 1440: loss 1.2231, time 76.21ms
iter 1450: loss 1.2833, time 89.61ms
iter 1460: loss 1.2377, time 80.70ms
iter 1470: loss 1.2191, time 76.33ms
iter 1480: loss 1.2178, time 90.74ms
iter 1490: loss 1.2534, time 87.67ms
step 1500: train loss 1.1533, val loss 1.4802
iter 1500: loss 1.2479, time 23742.98ms
iter 1510: loss 1.2302, time 80.75ms
iter 1520: loss 1.2414, time 80.64ms
iter 1530: loss 1.2185, time 87.95ms
iter 1540: loss 1.2526, time 81.34ms
iter 1550: loss 1.2212, time 82.43ms
iter 1560: loss 1.2569, time 80.17ms
iter 1570: loss 1.2107, time 85.22ms
iter 1580: loss 1.1989, time 73.95ms
iter 1590: loss 1.2002, time 84.32ms
iter 1600: loss 1.2170, time 80.56ms
iter 1610: loss 1.1826, time 79.84ms
iter 1620: loss 1.2173, time 86.30ms
iter 1630: loss 1.2379, time 86.09ms
iter 1640: loss 1.2328, time 85.45ms
iter 1650: loss 1.1915, time 91.90ms
iter 1660: loss 1.1940, time 82.32ms
iter 1670: loss 1.2266, time 80.95ms
iter 1680: loss 1.1889, time 89.73ms
iter 1690: loss 1.1958, time 82.71ms
iter 1700: loss 1.1809, time 95.75ms
iter 1710: loss 1.1515, time 90.40ms
iter 1720: loss 1.1732, time 79.21ms
iter 1730: loss 1.1975, time 78.52ms
iter 1740: loss 1.1949, time 88.39ms
step 1750: train loss 1.1037, val loss 1.4695
iter 1750: loss 1.1855, time 22369.50ms
iter 1760: loss 1.2001, time 96.69ms
iter 1770: loss 1.1945, time 83.55ms
iter 1780: loss 1.1483, time 85.45ms
iter 1790: loss 1.1828, time 91.77ms
iter 1800: loss 1.1462, time 76.38ms
iter 1810: loss 1.1775, time 76.93ms
iter 1820: loss 1.1775, time 71.56ms
iter 1830: loss 1.1977, time 72.86ms
iter 1840: loss 1.1655, time 76.66ms
iter 1850: loss 1.1862, time 69.72ms
iter 1860: loss 1.2302, time 77.09ms
iter 1870: loss 1.1841, time 82.01ms
iter 1880: loss 1.1557, time 84.79ms
iter 1890: loss 1.1646, time 83.49ms
iter 1900: loss 1.1606, time 91.36ms
iter 1910: loss 1.1678, time 85.67ms
iter 1920: loss 1.1539, time 73.47ms
iter 1930: loss 1.1778, time 74.43ms
iter 1940: loss 1.1503, time 72.43ms
iter 1950: loss 1.1319, time 84.56ms
iter 1960: loss 1.1556, time 90.55ms
iter 1970: loss 1.1377, time 79.87ms
iter 1980: loss 1.1976, time 73.90ms
iter 1990: loss 1.1383, time 83.28ms
step 2000: train loss 1.0593, val loss 1.4782
iter 2000: loss 1.1296, time 22417.07ms
iter 2010: loss 1.1495, time 74.18ms
iter 2020: loss 1.1252, time 74.61ms
iter 2030: loss 1.1453, time 77.73ms
iter 2040: loss 1.1386, time 73.18ms
iter 2050: loss 1.1216, time 71.00ms
iter 2060: loss 1.1134, time 72.28ms
iter 2070: loss 1.1401, time 80.19ms
iter 2080: loss 1.1181, time 76.64ms
iter 2090: loss 1.1291, time 67.95ms
iter 2100: loss 1.1325, time 74.71ms
iter 2110: loss 1.1462, time 67.73ms
iter 2120: loss 1.1401, time 69.48ms
iter 2130: loss 1.1433, time 70.89ms
iter 2140: loss 1.1025, time 73.36ms
iter 2150: loss 1.1174, time 71.75ms
iter 2160: loss 1.0962, time 79.00ms
iter 2170: loss 1.1220, time 68.41ms
iter 2180: loss 1.1167, time 83.56ms
iter 2190: loss 1.1591, time 84.52ms
iter 2200: loss 1.1379, time 92.82ms
iter 2210: loss 1.1256, time 74.71ms
iter 2220: loss 1.1270, time 84.50ms
iter 2230: loss 1.1023, time 82.96ms
iter 2240: loss 1.1225, time 78.56ms
step 2250: train loss 1.0103, val loss 1.4946
iter 2250: loss 1.1308, time 23551.36ms
iter 2260: loss 1.1090, time 89.57ms
iter 2270: loss 1.1027, time 80.29ms
iter 2280: loss 1.0877, time 72.64ms
iter 2290: loss 1.0848, time 70.86ms
iter 2300: loss 1.1273, time 91.19ms
iter 2310: loss 1.0919, time 93.50ms
iter 2320: loss 1.0887, time 80.49ms
iter 2330: loss 1.0909, time 83.21ms
iter 2340: loss 1.0930, time 91.61ms
iter 2350: loss 1.0766, time 91.36ms
iter 2360: loss 1.0909, time 90.23ms
iter 2370: loss 1.1004, time 86.73ms
iter 2380: loss 1.0920, time 91.52ms
iter 2390: loss 1.0857, time 85.41ms
iter 2400: loss 1.0457, time 78.22ms
iter 2410: loss 1.0879, time 90.47ms
iter 2420: loss 1.0677, time 81.62ms
iter 2430: loss 1.0565, time 88.43ms
iter 2440: loss 1.0495, time 73.13ms
iter 2450: loss 1.0790, time 79.02ms
iter 2460: loss 1.0912, time 91.26ms
iter 2470: loss 1.0786, time 94.68ms
iter 2480: loss 1.0583, time 88.23ms
iter 2490: loss 1.0662, time 82.68ms
step 2500: train loss 0.9591, val loss 1.4932
iter 2500: loss 1.0795, time 23890.81ms
iter 2510: loss 1.0501, time 78.44ms
iter 2520: loss 1.0644, time 71.94ms
iter 2530: loss 1.0741, time 86.50ms
iter 2540: loss 1.0431, time 87.52ms
iter 2550: loss 1.0724, time 78.71ms
iter 2560: loss 1.0574, time 84.83ms
iter 2570: loss 1.0554, time 86.03ms
iter 2580: loss 1.0518, time 78.19ms
iter 2590: loss 1.0569, time 70.13ms
iter 2600: loss 1.0712, time 68.07ms
iter 2610: loss 1.0367, time 75.37ms
iter 2620: loss 1.0536, time 72.00ms
iter 2630: loss 1.0666, time 74.30ms
iter 2640: loss 1.0173, time 72.03ms
iter 2650: loss 1.0349, time 70.61ms
iter 2660: loss 1.0514, time 76.20ms
iter 2670: loss 1.0468, time 67.10ms
iter 2680: loss 1.0422, time 79.48ms
iter 2690: loss 1.0570, time 90.75ms
iter 2700: loss 1.0112, time 83.48ms
iter 2710: loss 1.0370, time 81.65ms
iter 2720: loss 0.9983, time 76.28ms
iter 2730: loss 1.0383, time 74.27ms
iter 2740: loss 1.0320, time 83.90ms
step 2750: train loss 0.9125, val loss 1.5133
iter 2750: loss 1.0496, time 22113.50ms
iter 2760: loss 1.0151, time 82.91ms
iter 2770: loss 1.0526, time 74.57ms
iter 2780: loss 1.0380, time 69.29ms
iter 2790: loss 1.0174, time 79.22ms
iter 2800: loss 1.0151, time 74.75ms
iter 2810: loss 1.0135, time 77.63ms
iter 2820: loss 1.0338, time 81.70ms
iter 2830: loss 0.9881, time 79.23ms
iter 2840: loss 1.0091, time 83.62ms
iter 2850: loss 1.0172, time 74.65ms
iter 2860: loss 1.0021, time 78.74ms
iter 2870: loss 1.0254, time 77.07ms
iter 2880: loss 1.0275, time 71.61ms
iter 2890: loss 1.0121, time 79.79ms
iter 2900: loss 1.0329, time 82.16ms
iter 2910: loss 1.0127, time 74.68ms
iter 2920: loss 0.9806, time 70.23ms
iter 2930: loss 0.9998, time 76.92ms
iter 2940: loss 0.9968, time 77.55ms
iter 2950: loss 0.9966, time 79.50ms
iter 2960: loss 1.0084, time 75.63ms
iter 2970: loss 0.9960, time 86.88ms
iter 2980: loss 1.0207, time 74.01ms
iter 2990: loss 0.9978, time 66.07ms
step 3000: train loss 0.8658, val loss 1.5440
iter 3000: loss 1.0015, time 22104.89ms
iter 3010: loss 1.0184, time 71.40ms
iter 3020: loss 1.0024, time 94.77ms
iter 3030: loss 1.0126, time 78.26ms
iter 3040: loss 0.9884, time 92.60ms
iter 3050: loss 0.9930, time 79.02ms
iter 3060: loss 1.0041, time 75.55ms
iter 3070: loss 0.9928, time 93.92ms
iter 3080: loss 0.9985, time 75.68ms
iter 3090: loss 0.9946, time 68.06ms
iter 3100: loss 0.9803, time 87.92ms
iter 3110: loss 0.9923, time 72.25ms
iter 3120: loss 0.9928, time 83.28ms
iter 3130: loss 0.9933, time 73.81ms
iter 3140: loss 0.9922, time 70.64ms
iter 3150: loss 0.9701, time 82.04ms
iter 3160: loss 0.9808, time 80.00ms
iter 3170: loss 0.9642, time 92.25ms
iter 3180: loss 0.9570, time 71.47ms
iter 3190: loss 0.9793, time 78.50ms
iter 3200: loss 0.9846, time 76.95ms
iter 3210: loss 1.0021, time 73.23ms
iter 3220: loss 0.9852, time 77.42ms
iter 3230: loss 0.9857, time 72.26ms
iter 3240: loss 0.9458, time 78.83ms
step 3250: train loss 0.8190, val loss 1.5707
iter 3250: loss 0.9940, time 22288.15ms
iter 3260: loss 0.9815, time 83.96ms
iter 3270: loss 0.9718, time 77.21ms
iter 3280: loss 0.9715, time 74.30ms
iter 3290: loss 0.9619, time 76.86ms
iter 3300: loss 0.9536, time 80.74ms
iter 3310: loss 0.9662, time 73.93ms
iter 3320: loss 0.9484, time 82.60ms
iter 3330: loss 0.9531, time 73.83ms
iter 3340: loss 0.9631, time 72.98ms
iter 3350: loss 0.9757, time 73.18ms
iter 3360: loss 0.9371, time 71.99ms
iter 3370: loss 0.9355, time 72.79ms
iter 3380: loss 0.9502, time 81.46ms
iter 3390: loss 0.9392, time 80.22ms
iter 3400: loss 0.9476, time 72.11ms
iter 3410: loss 0.9492, time 73.31ms
iter 3420: loss 0.9460, time 71.78ms
iter 3430: loss 0.9399, time 78.54ms
iter 3440: loss 0.9305, time 77.32ms
iter 3450: loss 0.9274, time 74.31ms
iter 3460: loss 0.9818, time 76.13ms
iter 3470: loss 0.9291, time 78.19ms
iter 3480: loss 0.9353, time 75.96ms
iter 3490: loss 0.9362, time 79.57ms
step 3500: train loss 0.7799, val loss 1.5894
iter 3500: loss 0.9254, time 22113.48ms
iter 3510: loss 0.9596, time 78.19ms
iter 3520: loss 0.9075, time 77.49ms
iter 3530: loss 0.9266, time 77.12ms
iter 3540: loss 0.9516, time 83.72ms
iter 3550: loss 0.9264, time 78.26ms
iter 3560: loss 0.9583, time 65.64ms
iter 3570: loss 0.9264, time 69.44ms
iter 3580: loss 0.9305, time 72.68ms
iter 3590: loss 0.9494, time 71.10ms
iter 3600: loss 0.9081, time 73.46ms
iter 3610: loss 0.9281, time 80.92ms
iter 3620: loss 0.9309, time 71.41ms
iter 3630: loss 0.9103, time 73.10ms
iter 3640: loss 0.9200, time 68.36ms
iter 3650: loss 0.9316, time 77.29ms
iter 3660: loss 0.9178, time 75.74ms
iter 3670: loss 0.9104, time 72.55ms
iter 3680: loss 0.8976, time 75.41ms
iter 3690: loss 0.9060, time 66.67ms
iter 3700: loss 0.9075, time 72.37ms
iter 3710: loss 0.8819, time 78.60ms
iter 3720: loss 0.9121, time 85.58ms
iter 3730: loss 0.9190, time 86.76ms
iter 3740: loss 0.8896, time 72.56ms
step 3750: train loss 0.7369, val loss 1.6134
iter 3750: loss 0.9303, time 21936.36ms
iter 3760: loss 0.8824, time 76.14ms
iter 3770: loss 0.8908, time 72.73ms
iter 3780: loss 0.9101, time 82.03ms
iter 3790: loss 0.8661, time 67.54ms
iter 3800: loss 0.8963, time 76.90ms
iter 3810: loss 0.9293, time 73.84ms
iter 3820: loss 0.9087, time 74.05ms
iter 3830: loss 0.8946, time 83.26ms
iter 3840: loss 0.8960, time 76.66ms
iter 3850: loss 0.9006, time 72.43ms
iter 3860: loss 0.8756, time 79.82ms
iter 3870: loss 0.8720, time 71.57ms
iter 3880: loss 0.8794, time 71.82ms
iter 3890: loss 0.9009, time 69.48ms
iter 3900: loss 0.9239, time 65.78ms
iter 3910: loss 0.8912, time 71.15ms
iter 3920: loss 0.8967, time 76.40ms
iter 3930: loss 0.8973, time 84.29ms
iter 3940: loss 0.8920, time 88.86ms
iter 3950: loss 0.8885, time 66.77ms
iter 3960: loss 0.8830, time 70.62ms
iter 3970: loss 0.8582, time 74.32ms
iter 3980: loss 0.8932, time 77.72ms
iter 3990: loss 0.8871, time 76.08ms
step 4000: train loss 0.7049, val loss 1.6355
iter 4000: loss 0.8921, time 21719.85ms
iter 4010: loss 0.8952, time 75.28ms
iter 4020: loss 0.8451, time 77.10ms
iter 4030: loss 0.8807, time 71.30ms
iter 4040: loss 0.8792, time 77.29ms
iter 4050: loss 0.8553, time 70.18ms
iter 4060: loss 0.8771, time 72.11ms
iter 4070: loss 0.8940, time 70.52ms
iter 4080: loss 0.8907, time 76.69ms
iter 4090: loss 0.8424, time 85.59ms
iter 4100: loss 0.8731, time 80.79ms
iter 4110: loss 0.8842, time 74.25ms
iter 4120: loss 0.8388, time 72.61ms
iter 4130: loss 0.8511, time 78.59ms
iter 4140: loss 0.8625, time 68.02ms
iter 4150: loss 0.8556, time 70.69ms
iter 4160: loss 0.8536, time 70.76ms
iter 4170: loss 0.8658, time 75.04ms
iter 4180: loss 0.8787, time 72.05ms
iter 4190: loss 0.8583, time 73.74ms
iter 4200: loss 0.8529, time 82.31ms
iter 4210: loss 0.8864, time 77.32ms
iter 4220: loss 0.8717, time 69.60ms
iter 4230: loss 0.8670, time 73.23ms
iter 4240: loss 0.8602, time 75.53ms
step 4250: train loss 0.6737, val loss 1.6570
iter 4250: loss 0.8581, time 21812.44ms
iter 4260: loss 0.8667, time 72.07ms
iter 4270: loss 0.8414, time 72.51ms
iter 4280: loss 0.8685, time 72.53ms
iter 4290: loss 0.8398, time 78.19ms
iter 4300: loss 0.8491, time 77.98ms
iter 4310: loss 0.8508, time 72.38ms
iter 4320: loss 0.8603, time 71.43ms
iter 4330: loss 0.8672, time 77.50ms
iter 4340: loss 0.8792, time 74.19ms
iter 4350: loss 0.8489, time 83.67ms
iter 4360: loss 0.8388, time 68.60ms
iter 4370: loss 0.8232, time 72.54ms
iter 4380: loss 0.8589, time 75.97ms
iter 4390: loss 0.8359, time 73.68ms
iter 4400: loss 0.8340, time 76.31ms
iter 4410: loss 0.8395, time 70.95ms
iter 4420: loss 0.8387, time 75.15ms
iter 4430: loss 0.8419, time 71.43ms
iter 4440: loss 0.8402, time 66.42ms
iter 4450: loss 0.8584, time 71.47ms
iter 4460: loss 0.8412, time 71.64ms
iter 4470: loss 0.8504, time 78.02ms
iter 4480: loss 0.8461, time 75.09ms
iter 4490: loss 0.8194, time 73.04ms
step 4500: train loss 0.6471, val loss 1.6764
iter 4500: loss 0.8517, time 21914.34ms
iter 4510: loss 0.8359, time 78.05ms
iter 4520: loss 0.8456, time 77.99ms
iter 4530: loss 0.8354, time 75.10ms
iter 4540: loss 0.8182, time 71.51ms
iter 4550: loss 0.8245, time 75.45ms
iter 4560: loss 0.8340, time 75.52ms
iter 4570: loss 0.8269, time 68.69ms
iter 4580: loss 0.8132, time 74.12ms
iter 4590: loss 0.8182, time 77.51ms
iter 4600: loss 0.8439, time 68.74ms
iter 4610: loss 0.8257, time 70.68ms
iter 4620: loss 0.8131, time 71.32ms
iter 4630: loss 0.8303, time 81.71ms
iter 4640: loss 0.8408, time 71.85ms
iter 4650: loss 0.8299, time 72.70ms
iter 4660: loss 0.8462, time 78.38ms
iter 4670: loss 0.8616, time 73.51ms
iter 4680: loss 0.8607, time 67.24ms
iter 4690: loss 0.8383, time 74.32ms
iter 4700: loss 0.8270, time 72.72ms
iter 4710: loss 0.8407, time 81.93ms
iter 4720: loss 0.8253, time 74.24ms
iter 4730: loss 0.8322, time 79.43ms
iter 4740: loss 0.8211, time 70.06ms
step 4750: train loss 0.6318, val loss 1.6894
iter 4750: loss 0.8331, time 21922.45ms
iter 4760: loss 0.8423, time 73.80ms
iter 4770: loss 0.8339, time 80.12ms
iter 4780: loss 0.8238, time 78.94ms
iter 4790: loss 0.8146, time 73.53ms
iter 4800: loss 0.8056, time 76.55ms
iter 4810: loss 0.8387, time 74.29ms
iter 4820: loss 0.8523, time 76.13ms
iter 4830: loss 0.8263, time 80.39ms
iter 4840: loss 0.8160, time 77.81ms
iter 4850: loss 0.7944, time 77.89ms
iter 4860: loss 0.8151, time 76.98ms
iter 4870: loss 0.8156, time 72.09ms
iter 4880: loss 0.8127, time 71.20ms
iter 4890: loss 0.8224, time 67.67ms
iter 4900: loss 0.8048, time 77.66ms
iter 4910: loss 0.8201, time 79.70ms
iter 4920: loss 0.8111, time 79.52ms
iter 4930: loss 0.8216, time 76.69ms
iter 4940: loss 0.8338, time 73.82ms
iter 4950: loss 0.8333, time 74.48ms
iter 4960: loss 0.8510, time 73.83ms
iter 4970: loss 0.8035, time 68.85ms
iter 4980: loss 0.7889, time 80.95ms
iter 4990: loss 0.7988, time 73.09ms
step 5000: train loss 0.6141, val loss 1.7036
iter 5000: loss 0.7877, time 21635.71ms
training done
Best validation loss: 1.4694905281066895
Total train time: 14.34 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 and thought oft to do it.

PETRUCHIO:
I would I had found my hands for I should have.

GRUMIO:
Would I had he mine, that I would have her so far
and so well!

HORTENSIO:
My mistress ' Grumio, sir, you say your love
As much before you do not know that is safe.

GREMIO:
Not till show them such a bastard to take her hands.

GREMIO:
I would thou wert in a shoulders her good,
As if he were a sin to please her face.
Thou speak'st unrise thy followers.

PETRUCHIO:
Signior Hortensio, for thou know'st no
Inference time: 2.03 seconds
Tokens per second: 246.83
---------------
Sample 2:
 but that she did is not force to hear him as
living to be upon the day, he is head in manner.

ANGELO:
He would the skies of the morning.

ISABELLA:
This is a woman: if all prove is true.

ANGELO:
I am a very well, thou shalt content thyself.

ISABELLA:
And when she live with a thousand deeds,
That in the bosom of the wise climb'd spirit
Where we dare grace then to the base earth.

ANGELO:
What do you call? which resummer'd by this?

ISABELLA:
Sir, I shall know him when you shall ask, if any fin
Inference time: 1.96 seconds
Tokens per second: 255.44
---------------
Sample 3:
 his eye, my lord and the beg was their heads?

KING RICHARD III:
A thousand times afflicts no further than he.

QUEEN ELIZABETH:
By Him that hast thou been lost for thy death:
This is the traitor, the honour of King Richard;
And now thy king, the royal king is left.

KING RICHARD III:
Come, charitable woman, loving charms,
Which God do in fence and safety of me.

QUEEN ELIZABETH:
Who sets me from me? and how came I
A medle crown in a kingdom?

KING RICHARD III:
Give me another house, be content.
Inference time: 1.94 seconds
Tokens per second: 257.70
---------------
Sample 4:
 gentleman what we may make give us like good
for the bridal benefit of a man's kindness.

Second Gentleman:
How now! what news? what news? has the duke is this?

LUCIO:
Not stink down.

First Gentleman:
I think there was a prisoner man would be
crushes all the choice.

Third Gentleman:
So say the truth, and a maid to all the prince: we have
shown them both. Indeed I have said with the very
good deliverance; not the which they have had loved him
undiscovered. Therefore, go with us; and the
more e
Inference time: 2.13 seconds
Tokens per second: 234.51
---------------
Sample 5:
 and tell the one
Wherein this lady's love is in the which is nothing;
And therein we have sometime in our success.

Second Murderer:
This is the city to keep of his curse.

First Murderer:
Now the palace we must suffer it.

Second Murderer:
But when she saw with him?

Second Murderer:
I shall kill him in arms.

First Murderer:
Relent! will you go with me?

CLARENCE:
By heaven, I did not kill him he is dead.

KING HENRY VI:
O Clarence, dear friend thou, and tell me,
He shall bring me once, while 
Inference time: 2.13 seconds
Tokens per second: 235.28
---------------
Sample 6:
 death, that the gentler honest between
A true glass, and the theme will stand upon our time
That both groan had painted with their grieves.
And thou, by thy troth, I know thou well, nor wilt be none.
The san pity that's more made thee joy
Than a weary eye of thee for thy bridal back.
Welcome to the harvest heart says thou this?

BALTHASAR:
I will to the edge of thy means,
And yet I will pawn the first coast to die.

KING RICHARD III:
Yea, brother Richmond: yet, if he be not true,
'Tis drunk the 
Inference time: 2.11 seconds
Tokens per second: 237.13
---------------
Sample 7:
 desire.

Second Lady:
Stay, there's no remedy.

First Lord:
What, what! let's hear your grace.

Second Lord:
What's here!

Shepherd:
Here is a fellow of pines, would fall in
false friends: and there is no less for that
A poor gentleman. I do believe the provost. If her
Behold the master of Lord Angelo, her bedeced it,
In choosing particular of the deed, you must
climb change them, and he cannot on it
Be not the plainer below of your son: your affairs
Are soldiers as those free your good will cal
Inference time: 2.08 seconds
Tokens per second: 240.25
---------------
Sample 8:
 chamber-shoot on me
That have to thee sender to thy succession;
Had I not thy body to catch the devil,
That love so much dear stands and deposed,
To lose thee this deputy would be done:
Though I had compared thee, and villain, provided
That thou hast sentenced to stay, thou hast any thing,
My brother Gloucester's death in my cheeks,
And mock'd my just and my pernicious soul,
Why I would seek to the world of mine eyes,
And Bohemia lives my son with such a deep-
This alliance from my knee hath cal
Inference time: 2.14 seconds
Tokens per second: 233.94
---------------
Sample 9:
 Crosby:
I have fought it out.

GLOUCESTER:
Then be it as my untrust for that.

WARWICK:
The soul was my father Henry's head.

YORK:
What should you so too?

EXETER:
The crown we have shed for your cousin's son,
Were but a virtuous prince and ghost
Is it not set down your brother; but justice
Hath deserved of this harlous eye of your daughter.

YORK:
If you head to see your crown, you shall ne'er see
Mistake it off again.

WARWICK:
Once more, fair lord,
Than cannot a pitch again of your shining c
Inference time: 2.08 seconds
Tokens per second: 240.43
---------------
Sample 10:
 the never state of birth,
In any place were many food of despairs;
For sorrow I may not be an England's queen.

QUEEN ELIZABETH:
So long as I respect him to know.

KING RICHARD III:
Here comes the king cold corn and safe
The one hath made his power.

QUEEN ELIZABETH:
So long hast thou fall'n out away:
Revolt, gentle soul, to make him speak.

KING RICHARD III:
Stay, England's royal cousin!

QUEEN ELIZABETH:
Oh, that same hands that he hath heard of that?

KING RICHARD III:
Rode he is ten times tr
Inference time: 2.18 seconds
Tokens per second: 228.88
---------------
Average tokens per second: 241.04
tokens per iteration will be: 16,384
found vocab_size = 65 (inside ../../../data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 10.65M
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 4.2469, val loss 4.2417
iter 0: loss 4.2478, time 22910.42ms
iter 10: loss 3.2118, time 81.60ms
iter 20: loss 2.7846, time 82.63ms
iter 30: loss 2.6129, time 73.83ms
iter 40: loss 2.5605, time 69.95ms
iter 50: loss 2.5318, time 65.65ms
iter 60: loss 2.4924, time 74.79ms
iter 70: loss 2.4931, time 72.51ms
iter 80: loss 2.5013, time 78.79ms
iter 90: loss 2.4811, time 79.60ms
iter 100: loss 2.4761, time 72.99ms
iter 110: loss 2.4399, time 80.87ms
iter 120: loss 2.4302, time 76.36ms
iter 130: loss 2.4319, time 82.52ms
iter 140: loss 2.3936, time 74.66ms
iter 150: loss 2.4041, time 79.40ms
iter 160: loss 2.3639, time 74.01ms
iter 170: loss 2.3612, time 74.13ms
iter 180: loss 2.3010, time 79.74ms
iter 190: loss 2.2873, time 83.14ms
iter 200: loss 2.2286, time 71.92ms
iter 210: loss 2.1748, time 77.94ms
iter 220: loss 2.1439, time 76.56ms
iter 230: loss 2.0910, time 68.52ms
iter 240: loss 2.0479, time 70.29ms
step 250: train loss 1.9725, val loss 2.0781
iter 250: loss 2.0430, time 22128.52ms
iter 260: loss 2.0019, time 69.99ms
iter 270: loss 1.9525, time 76.44ms
iter 280: loss 1.9480, time 71.86ms
iter 290: loss 1.9073, time 84.24ms
iter 300: loss 1.9100, time 78.22ms
iter 310: loss 1.8621, time 73.26ms
iter 320: loss 1.8613, time 68.54ms
iter 330: loss 1.8148, time 81.63ms
iter 340: loss 1.8209, time 73.56ms
iter 350: loss 1.8072, time 74.88ms
iter 360: loss 1.7856, time 73.23ms
iter 370: loss 1.7480, time 74.56ms
iter 380: loss 1.7515, time 70.00ms
iter 390: loss 1.7261, time 67.08ms
iter 400: loss 1.7313, time 69.49ms
iter 410: loss 1.7529, time 70.20ms
iter 420: loss 1.6875, time 70.27ms
iter 430: loss 1.6439, time 74.91ms
iter 440: loss 1.6996, time 77.08ms
iter 450: loss 1.6379, time 79.88ms
iter 460: loss 1.6472, time 78.04ms
iter 470: loss 1.6323, time 73.17ms
iter 480: loss 1.6486, time 76.35ms
iter 490: loss 1.6100, time 69.18ms
step 500: train loss 1.5256, val loss 1.7165
iter 500: loss 1.5565, time 22241.70ms
iter 510: loss 1.6211, time 74.10ms
iter 520: loss 1.5649, time 74.18ms
iter 530: loss 1.5721, time 77.72ms
iter 540: loss 1.5889, time 72.53ms
iter 550: loss 1.5482, time 75.14ms
iter 560: loss 1.5522, time 69.88ms
iter 570: loss 1.5602, time 70.70ms
iter 580: loss 1.5350, time 79.64ms
iter 590: loss 1.5229, time 73.22ms
iter 600: loss 1.5110, time 78.62ms
iter 610: loss 1.5114, time 70.78ms
iter 620: loss 1.5180, time 82.85ms
iter 630: loss 1.5189, time 72.79ms
iter 640: loss 1.4931, time 67.97ms
iter 650: loss 1.5054, time 76.42ms
iter 660: loss 1.4788, time 72.57ms
iter 670: loss 1.4726, time 81.29ms
iter 680: loss 1.4618, time 74.20ms
iter 690: loss 1.4645, time 73.58ms
iter 700: loss 1.4703, time 72.30ms
iter 710: loss 1.4822, time 74.39ms
iter 720: loss 1.4705, time 76.50ms
iter 730: loss 1.4626, time 68.41ms
iter 740: loss 1.4300, time 79.63ms
step 750: train loss 1.3586, val loss 1.5768
iter 750: loss 1.4306, time 21975.63ms
iter 760: loss 1.4412, time 78.46ms
iter 770: loss 1.4526, time 77.94ms
iter 780: loss 1.4384, time 78.66ms
iter 790: loss 1.4145, time 74.58ms
iter 800: loss 1.4270, time 72.19ms
iter 810: loss 1.4240, time 82.73ms
iter 820: loss 1.3824, time 82.13ms
iter 830: loss 1.3718, time 84.27ms
iter 840: loss 1.4075, time 93.77ms
iter 850: loss 1.3720, time 76.80ms
iter 860: loss 1.3885, time 81.90ms
iter 870: loss 1.3910, time 77.38ms
iter 880: loss 1.3656, time 81.40ms
iter 890: loss 1.3752, time 78.36ms
iter 900: loss 1.3494, time 74.02ms
iter 910: loss 1.3887, time 71.22ms
iter 920: loss 1.3728, time 75.65ms
iter 930: loss 1.3668, time 75.51ms
iter 940: loss 1.3474, time 75.82ms
iter 950: loss 1.3578, time 81.10ms
iter 960: loss 1.3794, time 75.93ms
iter 970: loss 1.3673, time 73.60ms
iter 980: loss 1.3591, time 70.38ms
iter 990: loss 1.3454, time 66.23ms
step 1000: train loss 1.2700, val loss 1.5256
iter 1000: loss 1.3260, time 21954.85ms
iter 1010: loss 1.3529, time 71.72ms
iter 1020: loss 1.3253, time 72.08ms
iter 1030: loss 1.3511, time 78.16ms
iter 1040: loss 1.3278, time 74.73ms
iter 1050: loss 1.3372, time 75.04ms
iter 1060: loss 1.3362, time 70.72ms
iter 1070: loss 1.3476, time 69.97ms
iter 1080: loss 1.3231, time 77.56ms
iter 1090: loss 1.2752, time 77.65ms
iter 1100: loss 1.3193, time 63.96ms
iter 1110: loss 1.3162, time 73.26ms
iter 1120: loss 1.2978, time 71.12ms
iter 1130: loss 1.3002, time 72.27ms
iter 1140: loss 1.3314, time 73.03ms
iter 1150: loss 1.2673, time 79.78ms
iter 1160: loss 1.3163, time 77.88ms
iter 1170: loss 1.2838, time 78.56ms
iter 1180: loss 1.2668, time 81.12ms
iter 1190: loss 1.3173, time 76.24ms
iter 1200: loss 1.2908, time 69.09ms
iter 1210: loss 1.2837, time 72.01ms
iter 1220: loss 1.3022, time 84.20ms
iter 1230: loss 1.2931, time 84.34ms
iter 1240: loss 1.3051, time 73.46ms
step 1250: train loss 1.2031, val loss 1.4871
iter 1250: loss 1.3020, time 22083.28ms
iter 1260: loss 1.2978, time 80.05ms
iter 1270: loss 1.2943, time 71.84ms
iter 1280: loss 1.2721, time 78.24ms
iter 1290: loss 1.2619, time 76.20ms
iter 1300: loss 1.2979, time 75.71ms
iter 1310: loss 1.2621, time 73.26ms
iter 1320: loss 1.2645, time 78.84ms
iter 1330: loss 1.2520, time 67.43ms
iter 1340: loss 1.2783, time 71.95ms
iter 1350: loss 1.2357, time 81.02ms
iter 1360: loss 1.2408, time 71.61ms
iter 1370: loss 1.2692, time 79.31ms
iter 1380: loss 1.2377, time 74.10ms
iter 1390: loss 1.2449, time 71.70ms
iter 1400: loss 1.2313, time 71.84ms
iter 1410: loss 1.2083, time 66.72ms
iter 1420: loss 1.2610, time 74.61ms
iter 1430: loss 1.2651, time 76.45ms
iter 1440: loss 1.2141, time 79.50ms
iter 1450: loss 1.2672, time 79.44ms
iter 1460: loss 1.2487, time 75.96ms
iter 1470: loss 1.2191, time 74.16ms
iter 1480: loss 1.2179, time 74.50ms
iter 1490: loss 1.2432, time 71.56ms
step 1500: train loss 1.1458, val loss 1.4609
iter 1500: loss 1.1993, time 21769.10ms
iter 1510: loss 1.2362, time 73.97ms
iter 1520: loss 1.2131, time 67.04ms
iter 1530: loss 1.2350, time 75.20ms
iter 1540: loss 1.2141, time 79.30ms
iter 1550: loss 1.1704, time 81.40ms
iter 1560: loss 1.2245, time 72.46ms
iter 1570: loss 1.1953, time 66.65ms
iter 1580: loss 1.2097, time 63.49ms
iter 1590: loss 1.2410, time 61.58ms
iter 1600: loss 1.2101, time 63.19ms
iter 1610: loss 1.2033, time 63.58ms
iter 1620: loss 1.2040, time 57.27ms
iter 1630: loss 1.1987, time 74.15ms
iter 1640: loss 1.2145, time 78.87ms
iter 1650: loss 1.1867, time 64.42ms
iter 1660: loss 1.2172, time 82.31ms
iter 1670: loss 1.2149, time 74.69ms
iter 1680: loss 1.2234, time 74.76ms
iter 1690: loss 1.2225, time 71.75ms
iter 1700: loss 1.2185, time 73.79ms
iter 1710: loss 1.1917, time 92.62ms
iter 1720: loss 1.1864, time 76.19ms
iter 1730: loss 1.1933, time 82.22ms
iter 1740: loss 1.1874, time 73.79ms
step 1750: train loss 1.0999, val loss 1.4719
iter 1750: loss 1.1882, time 21939.41ms
iter 1760: loss 1.1681, time 68.94ms
iter 1770: loss 1.1628, time 75.32ms
iter 1780: loss 1.1615, time 74.65ms
iter 1790: loss 1.1834, time 68.76ms
iter 1800: loss 1.1680, time 75.06ms
iter 1810: loss 1.1879, time 71.97ms
iter 1820: loss 1.1638, time 77.56ms
iter 1830: loss 1.1224, time 84.59ms
iter 1840: loss 1.1718, time 76.52ms
iter 1850: loss 1.1880, time 74.23ms
iter 1860: loss 1.1937, time 72.42ms
iter 1870: loss 1.1508, time 75.77ms
iter 1880: loss 1.2031, time 74.15ms
iter 1890: loss 1.1440, time 75.93ms
iter 1900: loss 1.1854, time 82.64ms
iter 1910: loss 1.1362, time 75.59ms
iter 1920: loss 1.1410, time 75.36ms
iter 1930: loss 1.1441, time 73.96ms
iter 1940: loss 1.1416, time 73.91ms
iter 1950: loss 1.1637, time 78.67ms
iter 1960: loss 1.1399, time 77.28ms
iter 1970: loss 1.1294, time 80.68ms
iter 1980: loss 1.1415, time 86.70ms
iter 1990: loss 1.1461, time 81.66ms
step 2000: train loss 1.0555, val loss 1.4784
iter 2000: loss 1.1536, time 21934.97ms
iter 2010: loss 1.1380, time 75.55ms
iter 2020: loss 1.1476, time 78.66ms
iter 2030: loss 1.1217, time 69.71ms
iter 2040: loss 1.1348, time 69.96ms
iter 2050: loss 1.1461, time 69.32ms
iter 2060: loss 1.1632, time 76.85ms
iter 2070: loss 1.1531, time 68.86ms
iter 2080: loss 1.1300, time 76.71ms
iter 2090: loss 1.1130, time 77.87ms
iter 2100: loss 1.1149, time 74.99ms
iter 2110: loss 1.1087, time 79.17ms
iter 2120: loss 1.0774, time 69.94ms
iter 2130: loss 1.1181, time 77.22ms
iter 2140: loss 1.1139, time 80.99ms
iter 2150: loss 1.1325, time 81.08ms
iter 2160: loss 1.1404, time 86.00ms
iter 2170: loss 1.1454, time 82.38ms
iter 2180: loss 1.1428, time 75.46ms
iter 2190: loss 1.1240, time 77.35ms
iter 2200: loss 1.1035, time 73.22ms
iter 2210: loss 1.1057, time 84.85ms
iter 2220: loss 1.1019, time 81.85ms
iter 2230: loss 1.1101, time 78.53ms
iter 2240: loss 1.1126, time 72.06ms
step 2250: train loss 1.0046, val loss 1.4841
iter 2250: loss 1.0934, time 21310.86ms
iter 2260: loss 1.1038, time 72.02ms
iter 2270: loss 1.1243, time 78.03ms
iter 2280: loss 1.1015, time 79.54ms
iter 2290: loss 1.0734, time 75.92ms
iter 2300: loss 1.0657, time 73.97ms
iter 2310: loss 1.0958, time 76.19ms
iter 2320: loss 1.0878, time 80.68ms
iter 2330: loss 1.1208, time 73.27ms
iter 2340: loss 1.0833, time 78.09ms
iter 2350: loss 1.1137, time 76.33ms
iter 2360: loss 1.0726, time 78.78ms
iter 2370: loss 1.0759, time 87.53ms
iter 2380: loss 1.0901, time 81.28ms
iter 2390: loss 1.0812, time 76.01ms
iter 2400: loss 1.0966, time 74.20ms
iter 2410: loss 1.0736, time 78.28ms
iter 2420: loss 1.0554, time 76.35ms
iter 2430: loss 1.0912, time 84.25ms
iter 2440: loss 1.0923, time 85.47ms
iter 2450: loss 1.0787, time 74.84ms
iter 2460: loss 1.0368, time 75.88ms
iter 2470: loss 1.0866, time 74.31ms
iter 2480: loss 1.0615, time 82.69ms
iter 2490: loss 1.0769, time 79.34ms
step 2500: train loss 0.9598, val loss 1.5032
iter 2500: loss 1.0471, time 21109.98ms
iter 2510: loss 1.0749, time 64.61ms
iter 2520: loss 1.0681, time 79.97ms
iter 2530: loss 1.0692, time 72.05ms
iter 2540: loss 1.0590, time 75.03ms
iter 2550: loss 1.0638, time 75.23ms
iter 2560: loss 1.0455, time 79.25ms
iter 2570: loss 1.0751, time 71.23ms
iter 2580: loss 1.0434, time 75.96ms
iter 2590: loss 1.0749, time 73.62ms
iter 2600: loss 1.0829, time 75.31ms
iter 2610: loss 1.0716, time 76.87ms
iter 2620: loss 1.0543, time 72.50ms
iter 2630: loss 1.0610, time 70.52ms
iter 2640: loss 1.0525, time 80.78ms
iter 2650: loss 1.0489, time 73.11ms
iter 2660: loss 1.0495, time 72.95ms
iter 2670: loss 1.0248, time 71.20ms
iter 2680: loss 1.0451, time 72.91ms
iter 2690: loss 1.0642, time 70.09ms
iter 2700: loss 1.0703, time 77.95ms
iter 2710: loss 1.0353, time 87.18ms
iter 2720: loss 1.0278, time 75.50ms
iter 2730: loss 1.0340, time 82.99ms
iter 2740: loss 1.0435, time 74.40ms
step 2750: train loss 0.9098, val loss 1.5267
iter 2750: loss 1.0362, time 20291.64ms
iter 2760: loss 1.0101, time 69.61ms
iter 2770: loss 1.0076, time 73.09ms
iter 2780: loss 1.0157, time 73.02ms
iter 2790: loss 1.0446, time 76.06ms
iter 2800: loss 1.0196, time 78.72ms
iter 2810: loss 0.9839, time 81.33ms
iter 2820: loss 1.0210, time 80.65ms
iter 2830: loss 0.9787, time 74.75ms
iter 2840: loss 1.0302, time 85.22ms
iter 2850: loss 1.0144, time 66.34ms
iter 2860: loss 1.0197, time 73.82ms
iter 2870: loss 1.0268, time 79.76ms
iter 2880: loss 1.0101, time 73.33ms
iter 2890: loss 0.9882, time 64.75ms
iter 2900: loss 1.0023, time 73.78ms
iter 2910: loss 1.0259, time 68.47ms
iter 2920: loss 1.0176, time 79.42ms
iter 2930: loss 0.9959, time 74.89ms
iter 2940: loss 0.9906, time 78.27ms
iter 2950: loss 0.9858, time 74.08ms
iter 2960: loss 1.0251, time 74.82ms
iter 2970: loss 0.9835, time 80.75ms
iter 2980: loss 0.9860, time 78.25ms
iter 2990: loss 0.9855, time 70.47ms
step 3000: train loss 0.8642, val loss 1.5434
iter 3000: loss 0.9954, time 20684.56ms
iter 3010: loss 0.9728, time 70.60ms
iter 3020: loss 1.0229, time 68.46ms
iter 3030: loss 0.9973, time 77.39ms
iter 3040: loss 0.9641, time 69.96ms
iter 3050: loss 0.9832, time 75.80ms
iter 3060: loss 1.0075, time 72.32ms
iter 3070: loss 0.9950, time 78.80ms
iter 3080: loss 0.9783, time 77.99ms
iter 3090: loss 1.0016, time 71.45ms
iter 3100: loss 0.9909, time 80.42ms
iter 3110: loss 1.0120, time 73.70ms
iter 3120: loss 0.9745, time 78.09ms
iter 3130: loss 0.9602, time 71.00ms
iter 3140: loss 0.9826, time 73.93ms
iter 3150: loss 0.9934, time 74.47ms
iter 3160: loss 0.9604, time 61.41ms
iter 3170: loss 0.9590, time 81.97ms
iter 3180: loss 0.9853, time 64.23ms
iter 3190: loss 0.9547, time 71.09ms
iter 3200: loss 0.9524, time 62.02ms
iter 3210: loss 0.9562, time 70.62ms
iter 3220: loss 0.9421, time 86.68ms
iter 3230: loss 0.9551, time 70.73ms
iter 3240: loss 0.9597, time 80.87ms
step 3250: train loss 0.8195, val loss 1.5520
iter 3250: loss 0.9394, time 20910.54ms
iter 3260: loss 0.9452, time 80.60ms
iter 3270: loss 0.9382, time 70.48ms
iter 3280: loss 0.9318, time 72.83ms
iter 3290: loss 0.9720, time 81.63ms
iter 3300: loss 0.9587, time 79.12ms
iter 3310: loss 0.9658, time 73.11ms
iter 3320: loss 0.9189, time 72.68ms
iter 3330: loss 0.9567, time 75.51ms
iter 3340: loss 0.9668, time 77.15ms
iter 3350: loss 0.9457, time 71.46ms
iter 3360: loss 0.9614, time 73.72ms
iter 3370: loss 0.9443, time 78.78ms
iter 3380: loss 0.9290, time 80.92ms
iter 3390: loss 0.9227, time 75.56ms
iter 3400: loss 0.9715, time 72.73ms
iter 3410: loss 0.9769, time 73.99ms
iter 3420: loss 0.9389, time 79.65ms
iter 3430: loss 0.9157, time 83.31ms
iter 3440: loss 0.9456, time 75.23ms
iter 3450: loss 0.9501, time 73.74ms
iter 3460: loss 0.9271, time 72.94ms
iter 3470: loss 0.9354, time 77.20ms
iter 3480: loss 0.9103, time 77.01ms
iter 3490: loss 0.9437, time 84.24ms
step 3500: train loss 0.7777, val loss 1.5755
iter 3500: loss 0.9123, time 20832.96ms
iter 3510: loss 0.9212, time 70.68ms
iter 3520: loss 0.9390, time 78.06ms
iter 3530: loss 0.9306, time 79.92ms
iter 3540: loss 0.9295, time 76.12ms
iter 3550: loss 0.9388, time 80.44ms
iter 3560: loss 0.9375, time 69.58ms
iter 3570: loss 0.9341, time 76.92ms
iter 3580: loss 0.9315, time 73.32ms
iter 3590: loss 0.9129, time 75.86ms
iter 3600: loss 0.9293, time 81.70ms
iter 3610: loss 0.9090, time 77.45ms
iter 3620: loss 0.9107, time 76.69ms
iter 3630: loss 0.9200, time 81.31ms
iter 3640: loss 0.9307, time 76.28ms
iter 3650: loss 0.8950, time 73.49ms
iter 3660: loss 0.9131, time 71.78ms
iter 3670: loss 0.9210, time 58.97ms
iter 3680: loss 0.9028, time 87.13ms
iter 3690: loss 0.9273, time 76.81ms
iter 3700: loss 0.9322, time 73.94ms
iter 3710: loss 0.9171, time 74.93ms
iter 3720: loss 0.8937, time 76.91ms
iter 3730: loss 0.9179, time 67.66ms
iter 3740: loss 0.8863, time 77.72ms
step 3750: train loss 0.7398, val loss 1.6032
iter 3750: loss 0.9398, time 20659.33ms
iter 3760: loss 0.9003, time 81.08ms
iter 3770: loss 0.9010, time 70.18ms
iter 3780: loss 0.9094, time 72.35ms
iter 3790: loss 0.9250, time 75.73ms
iter 3800: loss 0.9123, time 71.64ms
iter 3810: loss 0.8893, time 69.45ms
iter 3820: loss 0.8726, time 77.11ms
iter 3830: loss 0.8820, time 74.10ms
iter 3840: loss 0.9080, time 75.77ms
iter 3850: loss 0.8614, time 67.56ms
iter 3860: loss 0.9091, time 71.83ms
iter 3870: loss 0.8744, time 72.99ms
iter 3880: loss 0.8817, time 82.44ms
iter 3890: loss 0.9049, time 71.85ms
iter 3900: loss 0.8704, time 67.56ms
iter 3910: loss 0.8986, time 72.00ms
iter 3920: loss 0.8836, time 69.16ms
iter 3930: loss 0.8629, time 69.47ms
iter 3940: loss 0.8761, time 69.16ms
iter 3950: loss 0.9196, time 83.30ms
iter 3960: loss 0.8810, time 79.41ms
iter 3970: loss 0.8832, time 72.55ms
iter 3980: loss 0.8794, time 69.54ms
iter 3990: loss 0.8845, time 72.14ms
step 4000: train loss 0.7040, val loss 1.6368
iter 4000: loss 0.8678, time 20640.46ms
iter 4010: loss 0.8619, time 72.65ms
iter 4020: loss 0.8808, time 77.52ms
iter 4030: loss 0.9099, time 76.02ms
iter 4040: loss 0.8840, time 70.49ms
iter 4050: loss 0.8837, time 72.54ms
iter 4060: loss 0.8654, time 77.46ms
iter 4070: loss 0.8544, time 81.79ms
iter 4080: loss 0.8607, time 78.80ms
iter 4090: loss 0.8682, time 80.02ms
iter 4100: loss 0.8468, time 72.08ms
iter 4110: loss 0.8732, time 79.55ms
iter 4120: loss 0.8807, time 76.53ms
iter 4130: loss 0.8630, time 77.58ms
iter 4140: loss 0.8648, time 72.97ms
iter 4150: loss 0.8761, time 74.87ms
iter 4160: loss 0.8428, time 75.15ms
iter 4170: loss 0.8716, time 73.81ms
iter 4180: loss 0.8662, time 70.76ms
iter 4190: loss 0.8595, time 77.70ms
iter 4200: loss 0.8484, time 74.30ms
iter 4210: loss 0.8514, time 78.54ms
iter 4220: loss 0.8687, time 80.27ms
iter 4230: loss 0.8615, time 78.73ms
iter 4240: loss 0.8580, time 70.44ms
step 4250: train loss 0.6768, val loss 1.6504
iter 4250: loss 0.8679, time 20728.05ms
iter 4260: loss 0.8795, time 77.56ms
iter 4270: loss 0.8628, time 73.67ms
iter 4280: loss 0.8605, time 81.20ms
iter 4290: loss 0.8388, time 86.23ms
iter 4300: loss 0.8560, time 83.64ms
iter 4310: loss 0.8358, time 77.12ms
iter 4320: loss 0.8769, time 82.87ms
iter 4330: loss 0.8595, time 74.85ms
iter 4340: loss 0.8286, time 78.90ms
iter 4350: loss 0.8438, time 84.34ms
iter 4360: loss 0.8358, time 68.59ms
iter 4370: loss 0.8401, time 71.85ms
iter 4380: loss 0.8290, time 68.22ms
iter 4390: loss 0.8654, time 81.34ms
iter 4400: loss 0.8514, time 78.85ms
iter 4410: loss 0.8512, time 77.71ms
iter 4420: loss 0.8471, time 87.33ms
iter 4430: loss 0.8355, time 71.11ms
iter 4440: loss 0.8631, time 73.81ms
iter 4450: loss 0.8461, time 79.83ms
iter 4460: loss 0.8442, time 73.27ms
iter 4470: loss 0.8602, time 78.17ms
iter 4480: loss 0.8219, time 74.79ms
iter 4490: loss 0.8550, time 86.28ms
step 4500: train loss 0.6505, val loss 1.6759
iter 4500: loss 0.8351, time 20716.04ms
iter 4510: loss 0.8498, time 80.30ms
iter 4520: loss 0.8242, time 60.60ms
iter 4530: loss 0.8729, time 79.35ms
iter 4540: loss 0.8284, time 66.91ms
iter 4550: loss 0.8674, time 74.60ms
iter 4560: loss 0.8525, time 72.41ms
iter 4570: loss 0.8498, time 74.18ms
iter 4580: loss 0.8167, time 70.43ms
iter 4590: loss 0.8303, time 78.10ms
iter 4600: loss 0.8335, time 80.45ms
iter 4610: loss 0.8142, time 71.39ms
iter 4620: loss 0.8172, time 73.60ms
iter 4630: loss 0.8389, time 78.25ms
iter 4640: loss 0.8558, time 71.80ms
iter 4650: loss 0.8218, time 78.33ms
iter 4660: loss 0.8366, time 74.87ms
iter 4670: loss 0.8348, time 71.29ms
iter 4680: loss 0.8488, time 76.95ms
iter 4690: loss 0.8343, time 69.98ms
iter 4700: loss 0.8299, time 77.37ms
iter 4710: loss 0.8414, time 72.90ms
iter 4720: loss 0.8349, time 72.80ms
iter 4730: loss 0.8369, time 79.54ms
iter 4740: loss 0.8135, time 77.63ms
step 4750: train loss 0.6332, val loss 1.6881
iter 4750: loss 0.8404, time 20963.05ms
iter 4760: loss 0.8158, time 75.22ms
iter 4770: loss 0.8201, time 75.04ms
iter 4780: loss 0.8410, time 79.15ms
iter 4790: loss 0.8256, time 75.20ms
iter 4800: loss 0.8411, time 80.93ms
iter 4810: loss 0.8223, time 83.80ms
iter 4820: loss 0.8172, time 76.10ms
iter 4830: loss 0.8265, time 80.42ms
iter 4840: loss 0.8229, time 76.24ms
iter 4850: loss 0.8372, time 85.13ms
iter 4860: loss 0.8182, time 82.07ms
iter 4870: loss 0.8117, time 76.95ms
iter 4880: loss 0.8113, time 83.58ms
iter 4890: loss 0.7996, time 77.76ms
iter 4900: loss 0.8262, time 78.30ms
iter 4910: loss 0.8316, time 72.08ms
iter 4920: loss 0.8286, time 76.94ms
iter 4930: loss 0.8252, time 74.92ms
iter 4940: loss 0.8202, time 71.62ms
iter 4950: loss 0.8325, time 79.56ms
iter 4960: loss 0.8088, time 74.77ms
iter 4970: loss 0.8259, time 72.14ms
iter 4980: loss 0.8298, time 73.05ms
iter 4990: loss 0.7941, time 78.87ms
step 5000: train loss 0.6167, val loss 1.7040
iter 5000: loss 0.8069, time 20733.72ms
training done
Best validation loss: 1.4608863592147827
Total train time: 13.91 mins
Loading meta from ../../../data/shakespeare_char/meta.pkl...
Sample 1:
 the conversation of the north,
To the bleeders of the eastern baseness
Of his presence and treason with royal flood.
And for this time, to make thee within me,
Fear to thy life, and be heap'd we from thee
And never learn to my grave to the fight
The common people worship of thy mother's life.

WARWICK:
As I reconcile thee, the king, you must confess
The corruption of thy compliment; stand all faced
Till we have great thought it.

KING HENRY VI:
As partly I protest, Edward, my lord,
The proudest 
Inference time: 2.09 seconds
Tokens per second: 238.96
---------------
Sample 2:
 away.

DUKE OF YORK:
Why then he hath been long so tender than forty you?

DUCHESS OF YORK:
Art thou gone? is there no reason?

DUCHESS OF YORK:
It is no grief, not but the king perform'd
The murderers of my sons, whose will be dead?

QUEEN ELIZABETH:
Shall we be confirm'd of this suburity?

DUCHESS OF YORK:
I will bite my tongue back again.

QUEEN ELIZABETH:
I go, and suspect thee for that tell me.

Messenger:
The more that I have show'd thee here in my true death.

QUEEN ELIZABETH:
As much to 
Inference time: 2.23 seconds
Tokens per second: 224.62
---------------
Sample 3:
 of late,
The common being affections, the queen
Of the secrets and the princess, the king's process
Of Europe hath the prisoner.

Provost:
I'll play the king.

DUKE VINCENTIO:
Sir, I pray you, leave till the law provincipal
That hath more less importance: nay, he shall bear
The charges of this one half-will have answer'd
The white-uperped swimmathy beauteous trees!
For news of winter thousand here, to the fight
Is bold as the and taste all as the winter
As the which is the fame of his sovereign'
Inference time: 2.08 seconds
Tokens per second: 240.37
---------------
Sample 4:
 not be so.

CLAUDIO:
Not so, marry, but not my leisure is but
confident and make peace with my hands of life. I
prithee, to fetch thee from thy lips, thou mistakest.

LUCIO:
Thou art term to do any great day's work: I have
not thine own again; let them not be good unto
us. I would speak to thee were an old tender things for
performed, having thy news to find thee shall
not hear: he hath made thee writing after his
accomples. If thou art be not of, sir, I have it is a son, a
man to be known, and 